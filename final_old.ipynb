{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from ruptures import Pelt\n",
    "from ruptures.metrics import hausdorff\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_kurento_names = [\n",
    "    \"loadtest-webrtc-final-kurento-2p-t3medium\",\n",
    "    \"loadtest-webrtc-final-kurento-2p-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-kurento-2p-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-kurento-2p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-kurento-5p-t3medium\",\n",
    "    \"loadtest-webrtc-final-kurento-5p-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-kurento-5p-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-kurento-5p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-kurento-8p-t3medium\",\n",
    "    \"loadtest-webrtc-final-kurento-8p-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-kurento-8p-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-kurento-8p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-10s-t3medium\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-10s-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-10s-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-10s-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-20s-t3medium\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-20s-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-20s-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-20s-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-40s-t3medium\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-40s-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-40s-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-kurento-3p-40s-t3medium-retry\",\n",
    "]\n",
    "\n",
    "index_mediasoup_names = [\n",
    "    \"loadtest-webrtc-final-mediasoup-2p-t3medium\",\n",
    "    \"loadtest-webrtc-final-mediasoup-2p-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-mediasoup-2p-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-mediasoup-2p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-mediasoup-5p-t3medium\",\n",
    "    \"loadtest-webrtc-final-mediasoup-5p-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-mediasoup-5p-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-mediasoup-5p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-mediasoup-8p-t3medium\",\n",
    "    \"loadtest-webrtc-final-mediasoup-8p-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-mediasoup-8p-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-mediasoup-8p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium-2\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium-wait\",\n",
    "    \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium-retry\",\n",
    "]\n",
    "\n",
    "index_livekit_names = [\n",
    "    \"loadtest-webrtc-final-livekit-2p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-livekit-8p-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-livekit-3p-10s-t3medium-retry\",\n",
    "    \"loadtest-webrtc-final-livekit-3p-40s-t3medium-retry\",\n",
    "]\n",
    "\n",
    "index_list_names = index_kurento_names + index_mediasoup_names + index_livekit_names\n",
    "\n",
    "node_types = [\"browseremulator\", \"masternode\", \"medianode\"]\n",
    "packet_types = [\"inbound\", \"outbound\"]\n",
    "\n",
    "seconds_per_fragment = 17\n",
    "\n",
    "start_end_times = pd.read_json(\"dfs_final/start-end-times.json\", orient=\"index\")\n",
    "start_end_times[\"from\"] = pd.to_datetime(\n",
    "    start_end_times[\"from\"], format=\"ISO8601\", utc=True\n",
    ").dt.tz_convert(\"UTC\")\n",
    "start_end_times[\"to\"] = pd.to_datetime(\n",
    "    start_end_times[\"to\"], format=\"ISO8601\", utc=True\n",
    ").dt.tz_convert(\"UTC\")\n",
    "\n",
    "\n",
    "def timestamp_to_secs(df_node, index, cpu_times=True):\n",
    "    df_tmp = df_node.copy()\n",
    "    df_tmp[\"@timestamp\"] = pd.to_datetime(df_tmp[\"@timestamp\"], format=\"ISO8601\")\n",
    "    tmp_serie = pd.Series(\n",
    "        [df_tmp[\"@timestamp\"].max(), start_end_times.loc[index, \"to\"]]\n",
    "    )\n",
    "    end_time = tmp_serie.min() if cpu_times else tmp_serie.max()\n",
    "    df_tmp = df_tmp[df_tmp[\"@timestamp\"] < end_time]\n",
    "    tmp_serie = pd.Series(\n",
    "        [df_tmp[\"@timestamp\"].min(), start_end_times.loc[index, \"from\"]]\n",
    "    )\n",
    "    start_time = tmp_serie.max() if cpu_times else tmp_serie.min()\n",
    "    df_tmp[\"timestamp_secs\"] = (df_tmp[\"@timestamp\"] - start_time).dt.total_seconds()\n",
    "    df_tmp = df_tmp[df_tmp[\"timestamp_secs\"] >= 0]\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [\n",
    "    pd.read_csv(f\"dfs_final/{x}.csv\")\n",
    "    if os.path.isfile(f\"dfs_final/{x}.csv\")\n",
    "    else pd.DataFrame()\n",
    "    for x in index_list_names\n",
    "]\n",
    "\n",
    "df_user_join_list = [\n",
    "    pd.read_csv(f\"dfs_final/{x}-user-join.csv\")\n",
    "    if os.path.isfile(f\"dfs_final/{x}-user-join.csv\")\n",
    "    else pd.DataFrame()\n",
    "    for x in index_list_names\n",
    "]\n",
    "\n",
    "for i, df_user in enumerate(df_list):\n",
    "    if not df_user.empty:\n",
    "        index = index_list_names[i]\n",
    "        user_join = df_user_join_list[i]\n",
    "        start_test_time = start_end_times.loc[index, \"from\"]\n",
    "\n",
    "        df_user[\"@timestamp\"] = pd.to_datetime(df_user[\"@timestamp\"], format=\"ISO8601\")\n",
    "        user_join[\"@timestamp\"] = pd.to_datetime(user_join[\"@timestamp\"], format=\"ISO8601\")\n",
    "\n",
    "        df_user[\"cut_index\"] = df_user[\"cut_index\"].astype(float)\n",
    "\n",
    "        # cut index starts at 0 for each video\n",
    "        # we asume that video starts on the timestamp that the user joins\n",
    "        # each fragment lasts *seconds_per_fragment* seconds\n",
    "        # for each row in user_join, change all entries with the same\n",
    "        # user_join[\"participant\"] and df_user[\"userFrom\"]; and user_join[\"session\"] and df_user[\"session\"],\n",
    "        # calculating its cut_index as\n",
    "        # (user_join[\"@timestamp\"] - start_test_time) + df_user[\"cut_index\"] * seconds_per_fragment\n",
    "        for _, row in user_join.iterrows():\n",
    "            user_joined_at_sec = (row[\"@timestamp\"] - start_test_time).total_seconds()\n",
    "            data = df_user.loc[\n",
    "                (df_user[\"userFrom\"] == row[\"participant\"])\n",
    "                & (df_user[\"session\"] == row[\"session\"]),\n",
    "                \"cut_index\"\n",
    "            ] = df_user.loc[\n",
    "                (df_user[\"userFrom\"] == row[\"participant\"])\n",
    "                & (df_user[\"session\"] == row[\"session\"]),\n",
    "                \"cut_index\"\n",
    "            ] * seconds_per_fragment + user_joined_at_sec\n",
    "\n",
    "index_data = []\n",
    "\n",
    "data_types = node_types + packet_types\n",
    "\n",
    "\n",
    "for i, index in enumerate(index_list_names):\n",
    "    splitted = index.split(\"-\")\n",
    "    media_server = splitted[3]\n",
    "    publishers = int(splitted[4][:-1])\n",
    "    subscribers = int(splitted[5][:-1]) if splitted[5][-1] == \"s\" else 0\n",
    "    users = publishers + subscribers\n",
    "    repeat = \"1\" if splitted[-1] == \"t3medium\" else splitted[-1]\n",
    "\n",
    "    index_data.append({\n",
    "        \"index\": index,\n",
    "        \"media_server\": media_server,\n",
    "        \"type\": repeat,\n",
    "        \"publishers\": publishers,\n",
    "        \"subscribers\": subscribers,\n",
    "        \"users\": users,\n",
    "    })\n",
    "\n",
    "index_data = pd.DataFrame(index_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "metric = \"pesq\"\n",
    "for i, df_tmp in enumerate(df_list):\n",
    "    if not df_tmp.empty:\n",
    "        average = df_tmp[metric].mean()\n",
    "        median = df_tmp[metric].median()\n",
    "        min = df_tmp[metric].min()\n",
    "        max = df_tmp[metric].max()\n",
    "        std = df_tmp[metric].std()\n",
    "        data.append([index_list_names[i], average, median, std, min, max])\n",
    "\n",
    "pd.DataFrame(data, columns=[\"index_type\", \"average\", \"median\", \"std\", \"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDIVIDUAL PLOTS PARAMETERS\n",
    "\n",
    "# Index of index_list to use\n",
    "index = 45\n",
    "# If there is only one metric in display, choose which one\n",
    "metric = \"vmaf\"\n",
    "\n",
    "show_full_range = True\n",
    "\n",
    "# Calculations\n",
    "index_name = index_list_names[index]\n",
    "metric_label = metric.upper()\n",
    "qoe_metrics_normalized = [\"vmaf\", \"msssim\", \"ssim\", \"vifp\", \"pesq\", \"visqol\"]\n",
    "qoe_metrics_not_normalized = [\"psnr\", \"psnrhvs\", \"psnrhvsm\"]\n",
    "metrics = qoe_metrics_normalized + qoe_metrics_not_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "colors = [\"blue\", \"orange\", \"green\", \"red\"]\n",
    "k = 0\n",
    "\n",
    "for metric in qoe_metrics_normalized:\n",
    "    for i, df_tmp in enumerate(df_list):\n",
    "        if not df_tmp.empty:\n",
    "            if k == 0:\n",
    "                fig, ax = plt.subplots()\n",
    "            df_mean = df_tmp.groupby([\"cut_index\"]).mean(numeric_only=True)\n",
    "            plot_name = index_list_names[i]\n",
    "            if not df_mean.empty:\n",
    "                ax.plot(df_mean.index, df_mean[metric], label=plot_name)\n",
    "                ax.axvline(\n",
    "                    x=df_mean.index.max(),\n",
    "                    linestyle=\"--\",\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "                ax.axvline(\n",
    "                    x=df_mean.index.min(),\n",
    "                    linestyle=\"--\",\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "\n",
    "            if k == 3:\n",
    "                ax.set_xlabel(\"timestamp (seconds)\")\n",
    "                ax.set_ylabel(metric)\n",
    "                ax.grid()\n",
    "                ax.set_ylim(-0.5, 1.05)\n",
    "                ax.set_yticks(np.arange(-0.5, 1.05, 0.05))\n",
    "                fig.legend(loc=\"lower left\")\n",
    "                fig.suptitle(f\"{metric} over time (mean, {plot_name})\")\n",
    "                fig.savefig(f\"images_final/{plot_name}_{metric}.png\")\n",
    "            k = (k + 1) % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "typologies = [\"2p\", \"8p\", \"3p-10s\", \"3p-40s\"]\n",
    "media_nodes = [\"kurento\", \"livekit\"]\n",
    "colors = [\"blue\", \"orange\", \"green\", \"red\"]\n",
    "for typology in typologies:\n",
    "    for media_node in media_nodes:\n",
    "        fig, ax = plt.subplots()\n",
    "        target_names = filter(\n",
    "            lambda x: typology in x and media_node in x, index_list_names\n",
    "        )\n",
    "\n",
    "        max_x = -1\n",
    "        for i, target_name in enumerate(target_names):\n",
    "            df_node = pd.read_csv(f\"dfs_final/{target_name}-medianode.csv\")\n",
    "            # remove entries with cpu < 0.001\n",
    "            # df_node = df_node[df_node[\"cpu\"] > 0.001]\n",
    "            df_node = timestamp_to_secs(df_node, target_name)\n",
    "            df_node = df_node.drop(columns=[\"@timestamp\", \"memory\"]).dropna()\n",
    "            ax.plot(\n",
    "                df_node[\"timestamp_secs\"],\n",
    "                df_node[\"cpu\"],\n",
    "                color=colors[i],\n",
    "                label=target_name,\n",
    "            )\n",
    "            plt.axvline(\n",
    "                x=df_node[\"timestamp_secs\"].max(),\n",
    "                color=colors[i],\n",
    "                linestyle=\"--\",\n",
    "                alpha=0.7,\n",
    "                label=\"End of test \" + target_name,\n",
    "            )\n",
    "            max_x = np.max(np.array([max_x, df_node[\"timestamp_secs\"].max()]))\n",
    "\n",
    "        ax.set_xlabel(\"timestamp (seconds)\")\n",
    "        ax.set_ylabel(\"CPU usage (%)\")\n",
    "        ax.grid()\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_yticks(np.arange(0, 1.05, 0.05))\n",
    "        # ax.set_xticks(np.arange(0, max_x, 120))\n",
    "\n",
    "        fig.suptitle(f\"CPU usage over time ({typology}, {media_node})\")\n",
    "        fig.legend(loc=\"right\")\n",
    "        fig.savefig(f\"images_final/{typology}_{media_node}_cpu.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "webrtc_stats_y = ax.twinx()\n",
    "\n",
    "aggregation_map = {\"@timestamp\": \"first\"}\n",
    "for metric in qoe_metrics_normalized:\n",
    "    aggregation_map[metric] = \"mean\"\n",
    "\n",
    "df_tmp = df_list[index].groupby(\"cut_index\").agg(aggregation_map)\n",
    "df_tmp = timestamp_to_secs(df_tmp, index_name)\n",
    "df_node = pd.read_csv(f\"dfs_final/{index_name}-medianode.csv\")\n",
    "df_node = timestamp_to_secs(df_node, index_name)\n",
    "df = df_tmp\n",
    "\n",
    "df_node_cpu = df_node.drop(columns=[\"memory\"]).dropna()\n",
    "df_node_memory = df_node.drop(columns=[\"cpu\"]).dropna()\n",
    "\n",
    "df_stats_inbound = pd.read_csv(f\"dfs_final/{index_name}-webrtc-stats-inbound.csv\")\n",
    "df_stats_outbound = pd.read_csv(f\"dfs_final/{index_name}-webrtc-stats-outbound.csv\")\n",
    "df_stats_inbound[\"@timestamp\"] = pd.to_datetime(\n",
    "    df_stats_inbound[\"@timestamp\"], format=\"ISO8601\"\n",
    ")\n",
    "df_stats_outbound[\"@timestamp\"] = pd.to_datetime(\n",
    "    df_stats_outbound[\"@timestamp\"], format=\"ISO8601\"\n",
    ")\n",
    "df_stats_inbound = df_stats_inbound.drop(columns=\"user_id\")\n",
    "df_stats_outbound = df_stats_outbound.drop(columns=\"user_id\")\n",
    "df_stats_inbound = df_stats_inbound.groupby(\"@timestamp\").mean()\n",
    "df_stats_outbound = df_stats_outbound.groupby(\"@timestamp\").mean()\n",
    "\n",
    "df_stats_inbound = timestamp_to_secs(df_stats_inbound, index_name)\n",
    "df_stats_outbound = timestamp_to_secs(df_stats_outbound, index_name)\n",
    "\n",
    "qoe_zorder = 10\n",
    "resource_zorder = 0\n",
    "\n",
    "qoe_linewidth = 3\n",
    "resource_linewidth = 1\n",
    "\n",
    "legend_handles = []\n",
    "for metric in qoe_metrics_normalized:\n",
    "    legend_handles.append(\n",
    "        ax.plot(\n",
    "            df[\"@timestamp\"],\n",
    "            df[metric],\n",
    "            label=metric,\n",
    "            marker=\"o\",\n",
    "            zorder=qoe_zorder,\n",
    "            lw=qoe_linewidth,\n",
    "        )\n",
    "    )\n",
    "legend_handles.append(\n",
    "    ax.plot(\n",
    "        df_node_cpu[\"@timestamp\"],\n",
    "        df_node_cpu[\"cpu\"],\n",
    "        \"g\",\n",
    "        label=\"cpu\",\n",
    "        zorder=resource_zorder,\n",
    "        lw=resource_linewidth,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    ")\n",
    "legend_handles.append(\n",
    "    ax.plot(\n",
    "        df_node_memory[\"@timestamp\"],\n",
    "        df_node_memory[\"memory\"],\n",
    "        \"c\",\n",
    "        label=\"memory\",\n",
    "        zorder=resource_zorder,\n",
    "        lw=resource_linewidth,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for column in df_stats_inbound.columns[1:]:\n",
    "    # Maybe readd gpSum?\n",
    "    if (\n",
    "        not \"bytesSent\" in column\n",
    "        and not \"packetsSent\" in column\n",
    "        and not \"qpSum\" in column\n",
    "        and not \"framesEncoded\" in column\n",
    "    ):\n",
    "        legend_handles.append(\n",
    "            webrtc_stats_y.plot(\n",
    "                df_stats_inbound.index, df_stats_inbound[column], label=column\n",
    "            )\n",
    "        )\n",
    "\n",
    "ax.set_title(f\"QOE metric (normalized) over time (mean, worker data, {index_name})\")\n",
    "ax.set_xlabel(\"timestamp (day hour:minute)\")\n",
    "ax.set_ylabel(\"QOE metric (normalized), CPU %, Memory %\")\n",
    "webrtc_stats_y.set_ylabel(\"WebRTC stats\")\n",
    "ax.grid()\n",
    "ax.legend(loc=\"upper left\")\n",
    "webrtc_stats_y.legend(loc=\"center left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols(dir, x_label, df):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    X = df[x_label]\n",
    "\n",
    "\n",
    "    X = sm.add_constant(X)  # adding a constant\n",
    "    for metric in metrics:\n",
    "        Y = df[metric]\n",
    "\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        summary = model.summary()\n",
    "\n",
    "        with open(f\"{dir}/{metric}-summary.tex\", \"w\") as fh:\n",
    "            fh.write(summary.as_latex())\n",
    "\n",
    "        with open(f\"{dir}/{metric}-summary.html\", \"w\") as fh:\n",
    "            fh.write(summary.as_html())\n",
    "\n",
    "        with open(f\"{dir}/{metric}.txt\", \"w\") as fh:\n",
    "            fh.write(summary.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.concat(df_list, ignore_index=True)\n",
    "ols(\"results_ols/results_all\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(df_list[:24], ignore_index=True)\n",
    "ols(\"results_ols/results_kurento\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(df_list[24:48], ignore_index=True)\n",
    "ols(\"results_ols/results_mediasoup\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(df_list[48:], ignore_index=True)\n",
    "ols(\"results_ols/results_livekit\", \"user_count\", df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_list = []\n",
    "\n",
    "df_webrtc_inbound_merged_list = []\n",
    "df_webrtc_outbound_merged_list = []\n",
    "\n",
    "for i, df_tmp in enumerate(df_list):\n",
    "\n",
    "    index_name = index_list_names[i]\n",
    "\n",
    "    df_node = pd.read_csv(f\"dfs_final/{index_name}-medianode.csv\")\n",
    "\n",
    "    df_node = df_node.drop(columns=[\"memory\"]).dropna()\n",
    "\n",
    "    df_tmp = timestamp_to_secs(df_tmp, index_name, False)\n",
    "\n",
    "    df_node = timestamp_to_secs(df_node, index_name, False)\n",
    "\n",
    "    df_tmp[\"timestamp_secs\"] = df_tmp[\"timestamp_secs\"].round(-1)\n",
    "\n",
    "    df_node[\"timestamp_secs\"] = df_node[\"timestamp_secs\"].round(-1)\n",
    "\n",
    "    # df_tmp = df_tmp.groupby([\"timestamp_secs\"]).mean(numeric_only=True)\n",
    "\n",
    "\n",
    "    merged_df = pd.merge(df_tmp, df_node, on=\"timestamp_secs\", how=\"inner\")\n",
    "\n",
    "\n",
    "    df_merged_list.append(merged_df)\n",
    "\n",
    "    df_webrtc_inbound = pd.read_csv(f\"dfs_final/{index_name}-webrtc-stats-inbound.csv\")\n",
    "\n",
    "    df_webrtc_inbound = timestamp_to_secs(df_webrtc_inbound, index_name, False)\n",
    "    df_webrtc_inbound[\"timestamp_secs\"] = df_webrtc_inbound[\"timestamp_secs\"].round(-1)\n",
    "\n",
    "    merged_inbound_df = pd.merge(\n",
    "        df_tmp, df_webrtc_inbound, on=\"timestamp_secs\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    df_webrtc_inbound_merged_list.append(merged_inbound_df)\n",
    "\n",
    "    df_webrtc_outbound = pd.read_csv(\n",
    "        f\"dfs_final/{index_name}-webrtc-stats-outbound.csv\"\n",
    "    )\n",
    "\n",
    "    df_webrtc_outbound = timestamp_to_secs(df_webrtc_outbound, index_name, False)\n",
    "    df_webrtc_outbound[\"timestamp_secs\"] = df_webrtc_outbound[\"timestamp_secs\"].round(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    merged_outbound_df = pd.merge(\n",
    "        df_tmp, df_webrtc_outbound, on=\"timestamp_secs\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    df_webrtc_outbound_merged_list.append(merged_outbound_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.concat(df_merged_list, ignore_index=True)\n",
    "ols(\"results_ols/results_all_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(df_merged_list[:24], ignore_index=True)\n",
    "ols(\"results_ols/results_kurento_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(df_merged_list[24:], ignore_index=True)\n",
    "ols(\"results_ols/results_mediasoup_cpu\", \"cpu\", df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[0],\n",
    "        df_list[1],\n",
    "        df_list[4],\n",
    "        df_list[5],\n",
    "        df_list[8],\n",
    "        df_list[9],\n",
    "        df_list[12],\n",
    "        df_list[13],\n",
    "        df_list[16],\n",
    "        df_list[17],\n",
    "        df_list[20],\n",
    "        df_list[21],\n",
    "        df_list[24],\n",
    "        df_list[25],\n",
    "        df_list[28],\n",
    "        df_list[29],\n",
    "        df_list[32],\n",
    "        df_list[33],\n",
    "        df_list[36],\n",
    "        df_list[37],\n",
    "        df_list[40],\n",
    "        df_list[41],\n",
    "        df_list[44],\n",
    "        df_list[45],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_default_all\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[2],\n",
    "        df_list[6],\n",
    "        df_list[10],\n",
    "        df_list[14],\n",
    "        df_list[18],\n",
    "        df_list[22],\n",
    "        df_list[26],\n",
    "        df_list[30],\n",
    "        df_list[34],\n",
    "        df_list[38],\n",
    "        df_list[42],\n",
    "        df_list[46],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_wait_all\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[3],\n",
    "        df_list[7],\n",
    "        df_list[11],\n",
    "        df_list[15],\n",
    "        df_list[19],\n",
    "        df_list[23],\n",
    "        df_list[27],\n",
    "        df_list[31],\n",
    "        df_list[35],\n",
    "        df_list[39],\n",
    "        df_list[43],\n",
    "        df_list[47],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_retry_all\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [    \n",
    "        df_list[0],\n",
    "        df_list[1],\n",
    "        df_list[4],\n",
    "        df_list[5],\n",
    "        df_list[8],\n",
    "        df_list[9],\n",
    "        df_list[12],\n",
    "        df_list[13],\n",
    "        df_list[16],\n",
    "        df_list[17],\n",
    "        df_list[20],\n",
    "        df_list[21],\n",
    "        df_list[24]\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_default_kurento\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[2],\n",
    "        df_list[6],\n",
    "        df_list[10],\n",
    "        df_list[14],\n",
    "        df_list[18],\n",
    "        df_list[22],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_wait_kurento\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[3],\n",
    "        df_list[7],\n",
    "        df_list[11],\n",
    "        df_list[15],\n",
    "        df_list[19],\n",
    "        df_list[23],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_retry_kurento\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[24],\n",
    "        df_list[25],\n",
    "        df_list[28],\n",
    "        df_list[29],\n",
    "        df_list[32],\n",
    "        df_list[33],\n",
    "        df_list[36],\n",
    "        df_list[37],\n",
    "        df_list[40],\n",
    "        df_list[41],\n",
    "        df_list[44],\n",
    "        df_list[45],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_default_mediasoup\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[26],\n",
    "        df_list[30],\n",
    "        df_list[34],\n",
    "        df_list[38],\n",
    "        df_list[42],\n",
    "        df_list[46],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_wait_mediasoup\", \"user_count\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_list[27],\n",
    "        df_list[31],\n",
    "        df_list[35],\n",
    "        df_list[39],\n",
    "        df_list[43],\n",
    "        df_list[47],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_retry_mediasoup\", \"user_count\", df_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[0],\n",
    "        df_merged_list[1],\n",
    "        df_merged_list[4],\n",
    "        df_merged_list[5],\n",
    "        df_merged_list[8],\n",
    "        df_merged_list[9],\n",
    "        df_merged_list[12],\n",
    "        df_merged_list[13],\n",
    "        df_merged_list[16],\n",
    "        df_merged_list[17],\n",
    "        df_merged_list[20],\n",
    "        df_merged_list[21],\n",
    "        df_merged_list[24],\n",
    "        df_merged_list[25],\n",
    "        df_merged_list[28],\n",
    "        df_merged_list[29],\n",
    "        df_merged_list[32],\n",
    "        df_merged_list[33],\n",
    "        df_merged_list[36],\n",
    "        df_merged_list[37],\n",
    "        df_merged_list[40],\n",
    "        df_merged_list[41],\n",
    "        df_merged_list[44],\n",
    "        df_merged_list[45],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_default_all_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[2],\n",
    "        df_merged_list[6],\n",
    "        df_merged_list[10],\n",
    "        df_merged_list[14],\n",
    "        df_merged_list[18],\n",
    "        df_merged_list[22],\n",
    "        df_merged_list[26],\n",
    "        df_merged_list[30],\n",
    "        df_merged_list[34],\n",
    "        df_merged_list[38],\n",
    "        df_merged_list[42],\n",
    "        df_merged_list[46],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_wait_all_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[3],\n",
    "        df_merged_list[7],\n",
    "        df_merged_list[11],\n",
    "        df_merged_list[15],\n",
    "        df_merged_list[19],\n",
    "        df_merged_list[23],\n",
    "        df_merged_list[27],\n",
    "        df_merged_list[31],\n",
    "        df_merged_list[35],\n",
    "        df_merged_list[39],\n",
    "        df_merged_list[43],\n",
    "        df_merged_list[47],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_retry_all_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [    \n",
    "        df_merged_list[0],\n",
    "        df_merged_list[1],\n",
    "        df_merged_list[4],\n",
    "        df_merged_list[5],\n",
    "        df_merged_list[8],\n",
    "        df_merged_list[9],\n",
    "        df_merged_list[12],\n",
    "        df_merged_list[13],\n",
    "        df_merged_list[16],\n",
    "        df_merged_list[17],\n",
    "        df_merged_list[20],\n",
    "        df_merged_list[21],\n",
    "        df_merged_list[24]\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_default_kurento_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[2],\n",
    "        df_merged_list[6],\n",
    "        df_merged_list[10],\n",
    "        df_merged_list[14],\n",
    "        df_merged_list[18],\n",
    "        df_merged_list[22],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_wait_kurento_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[3],\n",
    "        df_merged_list[7],\n",
    "        df_merged_list[11],\n",
    "        df_merged_list[15],\n",
    "        df_merged_list[19],\n",
    "        df_merged_list[23],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_retry_kurento_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[24],\n",
    "        df_merged_list[25],\n",
    "        df_merged_list[28],\n",
    "        df_merged_list[29],\n",
    "        df_merged_list[32],\n",
    "        df_merged_list[33],\n",
    "        df_merged_list[36],\n",
    "        df_merged_list[37],\n",
    "        df_merged_list[40],\n",
    "        df_merged_list[41],\n",
    "        df_merged_list[44],\n",
    "        df_merged_list[45],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_default_mediasoup_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[26],\n",
    "        df_merged_list[30],\n",
    "        df_merged_list[34],\n",
    "        df_merged_list[38],\n",
    "        df_merged_list[42],\n",
    "        df_merged_list[46],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_wait_mediasoup_cpu\", \"cpu\", df_tmp)\n",
    "\n",
    "df_tmp = pd.concat(\n",
    "    [\n",
    "        df_merged_list[27],\n",
    "        df_merged_list[31],\n",
    "        df_merged_list[35],\n",
    "        df_merged_list[39],\n",
    "        df_merged_list[43],\n",
    "        df_merged_list[47],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "ols(\"results_ols/results_retry_mediasoup_cpu\", \"cpu\", df_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_tmp in enumerate(df_list):\n",
    "    index_name = index_list_names[i]\n",
    "    try:\n",
    "        ols(f\"results_ols/indexes/{index_name}\", \"user_count\", df_tmp)\n",
    "    except:\n",
    "        print(f\"Failed for {index_name}\")\n",
    "\n",
    "for i, df_tmp in enumerate(df_merged_list):\n",
    "    index_name = index_list_names[i]\n",
    "    try:\n",
    "        ols(f\"results_ols/indexes_cpu/{index_name}\", \"cpu\", df_tmp)\n",
    "    except:\n",
    "        print(f\"Failed (cpu) for {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dir = \"test/\"\n",
    "\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "#df = pd.concat(df_merged_list, ignore_index=True)\n",
    "\n",
    "i = 24\n",
    "df = df_merged_list[i]\n",
    "\n",
    "df = df[df[\"cpu\"] < 0.95]\n",
    "\n",
    "X = df[\"user_count\"]\n",
    "\n",
    "metric = \"cpu\"\n",
    "\n",
    "X = sm.add_constant(X)  # adding a constant\n",
    "Y = df[metric]\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "summary = model.summary()\n",
    "\n",
    "# with open(f\"{dir}/{metric}-summary.tex\", \"w\") as fh:\n",
    "#     fh.write(summary.as_latex())\n",
    "\n",
    "# with open(f\"{dir}/{metric}-summary.html\", \"w\") as fh:\n",
    "#     fh.write(summary.as_html())\n",
    "\n",
    "# with open(f\"{dir}/{metric}.txt\", \"w\") as fh:\n",
    "#     fh.write(summary.as_text())\n",
    "print(index_list_names[i])\n",
    "display(pd.read_html(StringIO(summary.tables[1].as_html()), header=0, index_col=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "for i in range(0,48):\n",
    "    index_name = index_list_names[i]\n",
    "    print(index_name)\n",
    "    df = df_list[i]\n",
    "    os.makedirs(f\"images_final/sns/{index_name}\", exist_ok=True)\n",
    "    #df_merged = df_merged_list[i]\n",
    "\n",
    "\n",
    "    # plot x cpu y vmaf \n",
    "    # fig, ax = plt.subplots()\n",
    "    # sns.lineplot(data=df_merged, x=\"cpu\", y=\"vmaf\", ax=ax)\n",
    "    df = df.sort_values(by=[\"user_count\"])\n",
    "    for metric in metrics:\n",
    "        fig, ax = plt.subplots()\n",
    "        if metric in qoe_metrics_normalized:\n",
    "            ax.set_ylim(0, 1.05)\n",
    "        ax.set_xlim(2, df[\"user_count\"].max() + 1)\n",
    "        ax.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "        sns.lineplot(data=df, x=\"user_count\", y=metric, ax=ax)\n",
    "        plt.grid()\n",
    "        plt.savefig(f\"images_final/sns/{index_name}/{metric}.png\")\n",
    "\n",
    "plt.ion()\n",
    "# algo = Pelt(model=\"rbf\").fit(df[\"vmaf\"].values)\n",
    "# result = algo.predict(pen=10)\n",
    "# for breakpoint_idx in result:\n",
    "#     breakpoint_value = df[\"user_count\"].iloc[breakpoint_idx]\n",
    "#     ax.scatter(breakpoint_value, df[\"vmaf\"].iloc[breakpoint_idx], color='red', marker='o', s=100)\n",
    "\n",
    "# #vertical line\n",
    "\n",
    "# ax.axvline(x=result[0], linestyle=\"--\", alpha=0.7, label=\"Change point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,48):\n",
    "    print(index_list_names[i])\n",
    "    # TODO: paint the webrtc stats results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_data = pd.read_json(\"dfs_final/session-data.json\", orient=\"index\")\n",
    "for i, index in enumerate(index_list_names):\n",
    "    df = df_list[i]\n",
    "    user_count_max = df[\"user_count\"].max()\n",
    "    session_index_data = session_data.loc[index]\n",
    "    session_data.loc[index, \"user_count_max\"] = user_count_max\n",
    "session_data[\"registered ratio\"] = session_data[\"user_count_max\"].div(session_data[\"participants\"]).mul(100)\n",
    "\n",
    "session_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items = pd.read_csv(\"dfs_final/minio_items.csv\")\n",
    "\n",
    "\n",
    "# Initialize columns\n",
    "minio_items[\"sessions\"] = 0\n",
    "minio_items[\"videos per session\"] = 0  # or any default value you want to set initially\n",
    "\n",
    "minio_items = minio_items.value_counts([\"index\", \"error\"]).reset_index().sort_values(by=[\"index\", \"error\"])\n",
    "\n",
    "for index in minio_items[\"index\"]:\n",
    "    index_name = index_list_names.index(index)\n",
    "    df = df_list[index_name]\n",
    "\n",
    "    max_user_count = df[\"user_count\"].max().astype(int)\n",
    "    publishers = index_data.loc[index_data[\"index\"] == index, \"publishers\"].values[0]\n",
    "    subscribers = index_data.loc[index_data[\"index\"] == index, \"subscribers\"].values[0]\n",
    "\n",
    "    # Create a mask for the current index\n",
    "    index_mask = minio_items[\"index\"] == index\n",
    "\n",
    "    # Update columns using vectorized operations\n",
    "    minio_items.loc[index_mask, \"user_count\"] = max_user_count\n",
    "    minio_items.loc[index_mask, \"sessions\"] = minio_items.loc[index_mask, \"user_count\"].div(publishers + subscribers).apply(math.ceil)\n",
    "    minio_items.loc[index_mask, \"videos per session\"] = publishers * (publishers - 1) + subscribers * publishers\n",
    "    minio_items.loc[index_mask, \"total possible videos\"] = minio_items.loc[index_mask, \"sessions\"] * minio_items.loc[index_mask, \"videos per session\"]\n",
    "\n",
    "minio_items = minio_items[[\"index\", \"error\", \"total possible videos\", \"count\", \"sessions\", \"videos per session\"]]\n",
    "\n",
    "\n",
    "minio_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_data = pd.read_json(\"dfs_final/session-data.json\", orient=\"index\")\n",
    "df = pd.DataFrame(columns=[\"index\", \"first recorded user\", \"first recorded session\", \"calculated missing users data\", \"total registered users\", \"total added users\", \"missing user data ratio\"])\n",
    "for i, index in enumerate(index_list_names):\n",
    "    df_tmp = df_list[i]\n",
    "    df_short = df_tmp[[\"userFrom\", \"userTo\", \"session\"]].copy()\n",
    "\n",
    "    df_short[\"session\"] = df_short[\"session\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "    df_short[\"userFrom\"] = df_short[\"userFrom\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "    df_short[\"userTo\"] = df_short[\"userTo\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "\n",
    "    unique_sessions = df_short[\"session\"].unique()\n",
    "    possible_sessions = np.arange(1, unique_sessions.max() + 1)\n",
    "    missing_sessions = len(np.setdiff1d(possible_sessions, unique_sessions))\n",
    "    users_in_session = index_data.loc[index_data[\"index\"] == index, \"users\"].values[0]\n",
    "    missing_users = missing_sessions * users_in_session\n",
    "    combinations = users_in_session * (users_in_session - 1)\n",
    "    possible_users = np.arange(1, users_in_session + 1)\n",
    "    for session in unique_sessions:\n",
    "        if session != unique_sessions.max():\n",
    "            df_session = df_short[df_short[\"session\"] == session]\n",
    "            usersTo = df_session[\"userTo\"].unique()\n",
    "            missing_users += len(np.setdiff1d(possible_users, usersTo))\n",
    "\n",
    "\n",
    "    user_join = pd.read_csv(f\"dfs_final/{index}-user-join.csv\")\n",
    "    user_join[\"@timestamp\"] = pd.to_datetime(user_join[\"@timestamp\"], format=\"ISO8601\")\n",
    "\n",
    "    df_tmp[\"nsession\"] = df_tmp[\"session\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "    df_tmp[\"nuserFrom\"] = df_tmp[\"userFrom\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "\n",
    "    min_timestamp_row = df_tmp.loc[df_tmp[\"nsession\"].idxmin()]\n",
    "    total_registered_users = df_tmp[\"user_count\"].max()\n",
    "    total_added_users = session_data.loc[index, \"participants\"]\n",
    "    ratio = missing_users / total_added_users * 100\n",
    "    #missing_users = calculate_missing_recordings()\n",
    "    df.loc[len(df)] = [index, min_timestamp_row[\"nuserFrom\"], min_timestamp_row[\"nsession\"], missing_users, total_registered_users, total_added_users, ratio]\n",
    "\n",
    "\n",
    "df = df.sort_values(by=[\"missing user data ratio\"], ascending=False)\n",
    "\n",
    "df = df[df[\"index\"].str.contains(\"mediasoup\") & ~df[\"index\"].str.contains(\"0s\")]\n",
    "\n",
    "# Plotting histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df, x=\"missing user data ratio\")\n",
    "plt.xlabel(\"Missing User Data Ratio\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Missing User Data Ratio (Mediasoup, no subscribers)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total: {len(df.index)}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items = pd.read_csv(\"dfs_final/minio_items.csv\")\n",
    "\n",
    "minio_items = minio_items[minio_items[\"error\"] == False]\n",
    "minio_items = minio_items.value_counts([\"index\"]).reset_index().sort_values(by=[\"index\"])\n",
    "\n",
    "\n",
    "# Initialize columns\n",
    "minio_items[\"sessions\"] = 0\n",
    "minio_items[\"videos per session\"] = 0  # or any default value you want to set initially\n",
    "\n",
    "for index in minio_items[\"index\"]:\n",
    "    index_name = index_list_names.index(index)\n",
    "    df = df_list[index_name]\n",
    "    user_joins = pd.read_csv(f\"dfs_final/{index}-user-join.csv\")\n",
    "    user_joins[\"@timestamp\"] = pd.to_datetime(user_joins[\"@timestamp\"], format=\"ISO8601\")\n",
    "\n",
    "    max_user_count = df[\"user_count\"].max().astype(int)\n",
    "    min_user_count = df[\"user_count\"].min().astype(int)\n",
    "    publishers = int(index.split(\"-\")[4][:-1])\n",
    "    subscribers = int(index.split(\"-\")[5][:-1]) if index.split(\"-\")[5].endswith(\"s\") else 0\n",
    "\n",
    "    max_time: datetime = df[\"@timestamp\"].max()\n",
    "    min_time: datetime = np.min([df[\"@timestamp\"].min(), user_joins[\"@timestamp\"].min()])\n",
    "    time = (max_time - min_time).total_seconds()\n",
    "\n",
    "    start_time = start_end_times.loc[index, \"from\"]\n",
    "    total_time = (start_end_times.loc[index, \"to\"] - start_time).total_seconds()\n",
    "\n",
    "    # Create a mask for the current index\n",
    "    index_mask = minio_items[\"index\"] == index\n",
    "\n",
    "    # Update columns using vectorized operations\n",
    "    minio_items.loc[index_mask, \"data starts at user\"] = min_user_count\n",
    "    minio_items.loc[index_mask, \"data ends at user\"] = max_user_count\n",
    "    minio_items.loc[index_mask, \"data loss (users)\"] = max_user_count - min_user_count\n",
    "    minio_items.loc[index_mask, \"data loss (seconds)\"] = (min_time - start_time).total_seconds()\n",
    "    minio_items.loc[index_mask, \"sessions\"] = minio_items.loc[index_mask, \"data ends at user\"].div(publishers + subscribers).apply(math.ceil)\n",
    "    minio_items.loc[index_mask, \"videos per session\"] = publishers * (publishers - 1) + subscribers * publishers\n",
    "    minio_items.loc[index_mask, \"total possible videos\"] = minio_items.loc[index_mask, \"sessions\"] * minio_items.loc[index_mask, \"videos per session\"]\n",
    "    minio_items.loc[index_mask, \"video ratio\"] = minio_items.loc[index_mask, \"count\"] / minio_items.loc[index_mask, \"total possible videos\"] * 100\n",
    "    minio_items.loc[index_mask, \"total time (secs)\"] = total_time\n",
    "\n",
    "minio_items = minio_items[[\"index\", \"total possible videos\", \"count\", \"video ratio\", \"data starts at user\", \"data ends at user\", \"data loss (users)\", \"total time (secs)\", \"data loss (seconds)\"]]\n",
    "\n",
    "\n",
    "#minio_items = minio_items[~minio_items[\"index\"].str.contains(\"0s\")]\n",
    "\n",
    "minio_items = minio_items[minio_items[\"data starts at user\"] <= minio_items[\"data ends at user\"] * 0.25]\n",
    "\n",
    "#minio_items = minio_items[minio_items[\"video ratio\"] >= 50]\n",
    "\n",
    "#minio_items = minio_items[minio_items[\"index\"].str.contains(\"mediasoup\")]\n",
    "\n",
    "print(f\"< 5 %: {len(minio_items[minio_items['video ratio'] < 5].index)}\")\n",
    "print(f\"5 % < n < 25 %: {len(minio_items[(minio_items['video ratio'] >= 5) & (minio_items['video ratio'] < 25)].index)}\")\n",
    "print(f\"25 % < n < 50 %: {len(minio_items[(minio_items['video ratio'] >= 25) & (minio_items['video ratio'] < 50)].index)}\")\n",
    "print(f\"50 % < n < 75 %: {len(minio_items[(minio_items['video ratio'] >= 50) & (minio_items['video ratio'] < 75)].index)}\")\n",
    "print(f\"75 % < n < 95 %: {len(minio_items[(minio_items['video ratio'] >= 75) & (minio_items['video ratio'] < 95)].index)}\")\n",
    "print(f\"> 95 %: {len(minio_items[minio_items['video ratio'] >= 95].index)}\")\n",
    "print(f\"Total: {len(minio_items.index)}\")\n",
    "\n",
    "\n",
    "minio_items.sort_values(by=[\"video ratio\"], ascending=False)\n",
    "\n",
    "df = pd.DataFrame(columns=[\"media server\", \"type\", \"publishers\", \"subscribers\", \"videos we have / possible total videos\", \"data starts at user\", \"data ends at user\"])\n",
    "\n",
    "for index in minio_items[\"index\"]:\n",
    "    index_data_row = index_data.loc[index_data[\"index\"] == index]\n",
    "    media_server = index_data_row[\"media_server\"].values[0]\n",
    "    type = index_data_row[\"type\"].values[0]\n",
    "    publishers = index_data_row[\"publishers\"].values[0]\n",
    "    subscribers = index_data_row[\"subscribers\"].values[0]\n",
    "    video_ratio = minio_items.loc[minio_items[\"index\"] == index, \"video ratio\"].values[0]\n",
    "    min_user_count = minio_items.loc[minio_items[\"index\"] == index, \"data starts at user\"].values[0]\n",
    "    max_user_count = minio_items.loc[minio_items[\"index\"] == index, \"data ends at user\"].values[0]\n",
    "    df.loc[len(df)] = [media_server, type, publishers, subscribers, video_ratio, min_user_count, max_user_count]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[minio_items['video ratio'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[(minio_items['video ratio'] >= 5) & (minio_items['video ratio'] < 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[(minio_items['video ratio'] >= 25) & (minio_items['video ratio'] < 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[(minio_items['video ratio'] >= 25) & (minio_items['video ratio'] < 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[(minio_items['video ratio'] >= 50) & (minio_items['video ratio'] < 75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[(minio_items['video ratio'] >= 75) & (minio_items['video ratio'] < 95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items[(minio_items['video ratio'] >= 95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items = pd.read_csv(\"dfs_final/minio_items.csv\")\n",
    "\n",
    "minio_items = minio_items[minio_items[\"error\"] == True]\n",
    "minio_items = minio_items.value_counts([\"index\"]).reset_index().sort_values(by=[\"index\"])\n",
    "\n",
    "\n",
    "# Initialize columns\n",
    "minio_items[\"sessions\"] = 0\n",
    "minio_items[\"videos per session\"] = 0  # or any default value you want to set initially\n",
    "\n",
    "for index in minio_items[\"index\"]:\n",
    "    index_name = index_list_names.index(index)\n",
    "    df = df_list[index_name]\n",
    "\n",
    "    max_user_count = df[\"user_count\"].max().astype(int)\n",
    "    publishers = int(index.split(\"-\")[4][:-1])\n",
    "    subscribers = int(index.split(\"-\")[5][:-1]) if index.split(\"-\")[5].endswith(\"s\") else 0\n",
    "\n",
    "    # Create a mask for the current index\n",
    "    index_mask = minio_items[\"index\"] == index\n",
    "\n",
    "    # Update columns using vectorized operations\n",
    "    minio_items.loc[index_mask, \"user_count\"] = max_user_count\n",
    "    minio_items.loc[index_mask, \"sessions\"] = minio_items.loc[index_mask, \"user_count\"].div(publishers + subscribers).apply(math.ceil)\n",
    "    minio_items.loc[index_mask, \"videos per session\"] = publishers * (publishers - 1) + subscribers * publishers\n",
    "    minio_items.loc[index_mask, \"total possible videos\"] = minio_items.loc[index_mask, \"sessions\"] * minio_items.loc[index_mask, \"videos per session\"]\n",
    "    minio_items.loc[index_mask, \"success ratio\"] = minio_items.loc[index_mask, \"count\"] / minio_items.loc[index_mask, \"total possible videos\"] * 100\n",
    "\n",
    "minio_items = minio_items[[\"index\", \"total possible videos\", \"count\", \"success ratio\", \"sessions\", \"videos per session\"]]\n",
    "\n",
    "minio_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_items = pd.read_csv(\"dfs_final/minio_items.csv\")\n",
    "\n",
    "minio_items = minio_items.value_counts([\"index\", \"error\"]).reset_index().sort_values(by=[\"index\", \"error\"])\n",
    "\n",
    "# remove error column and add the count of success and error in the same index\n",
    "minio_items = minio_items.drop(columns=[\"error\"]).groupby([\"index\"]).sum().reset_index()\n",
    "\n",
    "# Initialize columns\n",
    "minio_items[\"sessions\"] = 0\n",
    "minio_items[\"videos per session\"] = 0  # or any default value you want to set initially\n",
    "\n",
    "for index in minio_items[\"index\"]:\n",
    "    index_name = index_list_names.index(index)\n",
    "    df = df_list[index_name]\n",
    "\n",
    "    max_user_count = df[\"user_count\"].max().astype(int)\n",
    "    publishers = int(index.split(\"-\")[4][:-1])\n",
    "    subscribers = int(index.split(\"-\")[5][:-1]) if index.split(\"-\")[5].endswith(\"s\") else 0\n",
    "\n",
    "    # Create a mask for the current index\n",
    "    index_mask = minio_items[\"index\"] == index\n",
    "\n",
    "    # Update columns using vectorized operations\n",
    "    minio_items.loc[index_mask, \"user_count\"] = max_user_count\n",
    "    minio_items.loc[index_mask, \"sessions\"] = minio_items.loc[index_mask, \"user_count\"].div(publishers + subscribers).apply(math.ceil)\n",
    "    minio_items.loc[index_mask, \"videos per session\"] = publishers * (publishers - 1) + subscribers * publishers\n",
    "    minio_items.loc[index_mask, \"total possible videos\"] = minio_items.loc[index_mask, \"sessions\"] * minio_items.loc[index_mask, \"videos per session\"]\n",
    "    minio_items.loc[index_mask, \"success ratio\"] = minio_items.loc[index_mask, \"count\"] / minio_items.loc[index_mask, \"total possible videos\"] * 100\n",
    "\n",
    "minio_items = minio_items[[\"index\", \"total possible videos\", \"count\", \"success ratio\", \"sessions\", \"videos per session\"]]\n",
    "print(f\"< 5 %: {len(minio_items[minio_items['success ratio'] < 5].index)}\")\n",
    "print(f\"5 % < n < 25 %: {len(minio_items[(minio_items['success ratio'] >= 5) & (minio_items['success ratio'] < 25)].index)}\")\n",
    "print(f\"25 % < n < 50 %: {len(minio_items[(minio_items['success ratio'] >= 25) & (minio_items['success ratio'] < 50)].index)}\")\n",
    "print(f\"50 % < n < 75 %: {len(minio_items[(minio_items['success ratio'] >= 50) & (minio_items['success ratio'] < 75)].index)}\")\n",
    "print(f\"75 % < n < 95 %: {len(minio_items[(minio_items['success ratio'] >= 75) & (minio_items['success ratio'] < 95)].index)}\")\n",
    "print(f\"> 95 %: {len(minio_items[minio_items['success ratio'] >= 95].index)}\")\n",
    "print(f\"Total: {len(minio_items.index)}\")\n",
    "\n",
    "minio_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "typologies = [\"2p\", \"8p\", \"3p-10s\", \"3p-40s\"]\n",
    "media_nodes = [\"kurento\", \"livekit\"]\n",
    "colors = [\"blue\", \"red\"]\n",
    "for typology in typologies:\n",
    "    fig, ax = plt.subplots()\n",
    "    target_names = filter(\n",
    "        lambda x: (typology in x) and any(media_node in x for media_node in media_nodes) and \"retry\" in x, index_list_names\n",
    "    )\n",
    "    #max_x = -1\n",
    "    for i, target_name in enumerate(target_names):\n",
    "        df_node = pd.read_csv(f\"dfs_final/{target_name}-medianode.csv\")\n",
    "        # remove entries with cpu < 0.001\n",
    "        # df_node = df_node[df_node[\"cpu\"] > 0.001]\n",
    "        df_node = timestamp_to_secs(df_node, target_name)\n",
    "        df_node = df_node.drop(columns=[\"@timestamp\", \"memory\"]).dropna()\n",
    "        ax.plot(\n",
    "            df_node[\"timestamp_secs\"],\n",
    "            df_node[\"cpu\"],\n",
    "            color=colors[i],\n",
    "            label=target_name,\n",
    "        )\n",
    "        plt.axvline(\n",
    "            x=df_node[\"timestamp_secs\"].max(),\n",
    "            color=colors[i],\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.7,\n",
    "            label=\"End of test \" + target_name,\n",
    "        )\n",
    "        #max_x = np.max(np.array([max_x, df_node[\"timestamp_secs\"].max()]))\n",
    "\n",
    "    ax.set_xlabel(\"timestamp (seconds)\")\n",
    "    ax.set_ylabel(\"CPU usage (%)\")\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_yticks(np.arange(0, 1.05, 0.05))\n",
    "    #ax.set_xticks(np.arange(0, max_x, 120))\n",
    "\n",
    "    fig.suptitle(f\"CPU usage over time ({typology})\")\n",
    "    fig.legend(loc=\"right\")\n",
    "    fig.savefig(f\"images_final/{typology}_cpu.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_data = pd.read_json(\"dfs_final/session-data.json\", orient=\"index\")\n",
    "# keep only the kurento and livekit entries\n",
    "session_data = session_data[session_data.index.str.contains(\"kurento|livekit\")]\n",
    "session_data = session_data[session_data.index.str.contains(\"2p|8p|3p-10s|3p-40s\")]\n",
    "session_data = session_data[session_data.index.str.contains(\"retry\")]\n",
    "#export data as tex table\n",
    "session_data.to_latex(\"tex/session_data.tex\")\n",
    "session_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "typologies = [\"2p\", \"8p\", \"3p-10s\", \"3p-40s\"]\n",
    "media_nodes = [\"kurento\", \"livekit\"]\n",
    "colors = [\"blue\", \"red\"]\n",
    "for typology in typologies:\n",
    "    for media_node in media_nodes:\n",
    "        fig, ax = plt.subplots()\n",
    "        target_names = filter(\n",
    "            lambda x: typology in x and media_node in x and \"retry\" in x, index_list_names\n",
    "        )\n",
    "        #max_x = -1\n",
    "        for i, target_name in enumerate(target_names):\n",
    "            df_tmp = df_list[index_list_names.index(target_name)].copy()\n",
    "            #df_tmp = pd.read_csv(f\"dfs_final/{target_name}.csv\")\n",
    "            user_join = pd.read_csv(f\"dfs_final/{target_name}-user-join.csv\")\n",
    "            cpu_mem = pd.read_csv(f\"dfs_final/{target_name}-medianode.csv\")\n",
    "            user_join = timestamp_to_secs(user_join, target_name)\n",
    "            cpu_mem = timestamp_to_secs(cpu_mem, target_name)\n",
    "            cpu_mem = cpu_mem.drop(columns=[\"@timestamp\", \"memory\"]).dropna()\n",
    "            # remove entries with cpu < 0.001\n",
    "            # df_node = df_node[df_node[\"cpu\"] > 0.001]\n",
    "            #df_node = timestamp_to_secs(df_node, target_name)\n",
    "            # aproximate cut_index to the nearest 10 seconds\n",
    "            max_timestamp = df_tmp[\"cut_index\"].max()\n",
    "            # if df_tmp[\"cut_index\"].max() > 8000:\n",
    "            #     #df_tmp[\"cut_index\"] = df_tmp[\"cut_index\"].round(-3)\n",
    "            # #elif df_tmp[\"cut_index\"].max() > 1000:\n",
    "            #     #df_tmp[\"cut_index\"] = df_tmp[\"cut_index\"].round(-2)\n",
    "            # else:\n",
    "            df_tmp[\"cut_index\"] = df_tmp[\"cut_index\"].round(-1)\n",
    "            df_mean = df_tmp.groupby([\"cut_index\"]).mean(numeric_only=True)\n",
    "\n",
    "            plot_name = target_name\n",
    "            timestamp_user_values = user_join[\"timestamp_secs\"].values\n",
    "            # fill the timestamp_user_values with the last value to make the plot continuous\n",
    "\n",
    "            timestamp_user_values = np.append(timestamp_user_values, max_timestamp)\n",
    "            count = range(1, len(timestamp_user_values) + 1)\n",
    "            if i == 0:\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.plot(timestamp_user_values, count, color=\"red\", label=\"user count\", linestyle=\"--\")\n",
    "                ax2.set_ylabel(\"user count\")\n",
    "                #ax3 = ax.twinx()\n",
    "                ax.plot(cpu_mem[\"timestamp_secs\"], cpu_mem[\"cpu\"], color=\"green\", label=\"cpu\", linestyle=\"--\")\n",
    "                #ax2.set_ylabel(\"cpu\")\n",
    "\n",
    "\n",
    "            if not df_mean.empty:\n",
    "                for metric in qoe_metrics_normalized:\n",
    "                    # TODO: Probar usando eje X user_count\n",
    "                    # TODO: Calcular media y std\n",
    "                    # TODO: Cuanto tarda en meterse X numero de usuarios en un media server\n",
    "                    # TODO: Tiempo de conexin vs nmero de usuarios (a lo mejor meter CPU)\n",
    "                    ax.plot(df_mean.index, df_mean[metric], label=metric)\n",
    "\n",
    "        ax.set_xlabel(\"seconds\")\n",
    "        ax.set_ylabel(\"QoE (normalized)\\nCPU usage (%)\")\n",
    "        ax.grid()\n",
    "        ax.set_ylim(-0.5, 1.05)\n",
    "        ax.set_yticks(np.arange(-0.5, 1.05, 0.05))\n",
    "\n",
    "        fig.suptitle(f\"QoE over time ({media_node}, {typology})\")\n",
    "        fig.legend(loc=\"right\")\n",
    "        fig.savefig(f\"images_final/{typology}_{media_node}_qoe.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "typologies = [\"2p\", \"8p\", \"3p-10s\", \"3p-40s\"]\n",
    "media_nodes = [\"kurento\", \"livekit\"]\n",
    "for typology in typologies:\n",
    "    for media_node in media_nodes:\n",
    "        target_names = filter(\n",
    "            lambda x: typology in x and media_node in x and \"retry\" in x, index_list_names\n",
    "        )\n",
    "        #max_x = -1\n",
    "        for i, target_name in enumerate(target_names):\n",
    "            # Define the data\n",
    "            df_tmp = df_list[index_list_names.index(target_name)]\n",
    "            qoe = qoe_metrics_not_normalized + qoe_metrics_normalized\n",
    "            data = {'metrics': qoe,\n",
    "                'average': [df_tmp[metric].mean() for metric in qoe],\n",
    "                'median': [df_tmp[metric].median() for metric in qoe],\n",
    "                'std': [df_tmp[metric].std() for metric in qoe],\n",
    "                'min': [df_tmp[metric].min() for metric in qoe],\n",
    "                'max': [df_tmp[metric].max() for metric in qoe]}\n",
    "\n",
    "            # Create the dataframe\n",
    "            df_stats = pd.DataFrame(data)\n",
    "\n",
    "            # To tex\n",
    "            df_stats.to_latex(f\"tex/qoe_{media_node}_{typology}.tex\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "typologies = [\"2p\", \"8p\", \"3p-10s\", \"3p-40s\"]\n",
    "media_nodes = [\"kurento\", \"livekit\"]\n",
    "colors = [\"blue\", \"red\"]\n",
    "for typology in typologies:\n",
    "    for media_node in media_nodes:\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        #also draw scatterplot\n",
    "        figs, axs = plt.subplots()\n",
    "        target_names = filter(\n",
    "            lambda x: typology in x and media_node in x and \"retry\" in x, index_list_names\n",
    "        )\n",
    "        #max_x = -1\n",
    "        for i, target_name in enumerate(target_names):\n",
    "            df_tmp = df_list[index_list_names.index(target_name)].copy()\n",
    "            #df_tmp = pd.read_csv(f\"dfs_final/{target_name}.csv\")\n",
    "            user_join = pd.read_csv(f\"dfs_final/{target_name}-user-join.csv\")\n",
    "            cpu_mem = pd.read_csv(f\"dfs_final/{target_name}-medianode.csv\")\n",
    "            user_join = timestamp_to_secs(user_join, target_name)\n",
    "            cpu_mem = timestamp_to_secs(cpu_mem, target_name)\n",
    "            cpu_mem = cpu_mem.drop(columns=[\"@timestamp\", \"memory\"]).dropna()\n",
    "            # remove entries with cpu < 0.001\n",
    "            # df_node = df_node[df_node[\"cpu\"] > 0.001]\n",
    "            #df_node = timestamp_to_secs(df_node, target_name)\n",
    "            # aproximate cut_index to the nearest 10 seconds\n",
    "            # max_timestamp = df_tmp[\"cut_index\"].max()\n",
    "            # if df_tmp[\"cut_index\"].max() > 8000:\n",
    "            #     #df_tmp[\"cut_index\"] = df_tmp[\"cut_index\"].round(-3)\n",
    "            # #elif df_tmp[\"cut_index\"].max() > 1000:\n",
    "            #     #df_tmp[\"cut_index\"] = df_tmp[\"cut_index\"].round(-2)\n",
    "            # else:\n",
    "            # df_tmp[\"cut_index\"] = df_tmp[\"cut_index\"].round(-1)\n",
    "            for metric in qoe_metrics_normalized:\n",
    "                axs.scatter(df_tmp[\"user_count\"], df_tmp[metric], label=metric)\n",
    "            df_mean = df_tmp.groupby([\"user_count\"]).mean(numeric_only=True)\n",
    "\n",
    "            plot_name = target_name\n",
    "\n",
    "            if not df_mean.empty:\n",
    "                for metric in qoe_metrics_normalized:\n",
    "                    # TODO: Probar usando eje X user_count\n",
    "                    # TODO: Calcular media y std\n",
    "                    # TODO: Cuanto tarda en meterse X numero de usuarios en un media server\n",
    "                    # TODO: Tiempo de conexin vs nmero de usuarios (a lo mejor meter CPU)\n",
    "                    ax.plot(df_mean.index, df_mean[metric], label=metric\n",
    "\n",
    "        ax.set_xlabel(\"user count\")\n",
    "        ax.set_ylabel(\"QoE (normalized)\\nCPU usage (%)\")\n",
    "        ax.grid()\n",
    "        ax.set_ylim(-0.5, 1.05)\n",
    "        ax.set_yticks(np.arange(-0.5, 1.05, 0.05))\n",
    "\n",
    "        fig.suptitle(f\"QoE over time ({media_node}, {typology})\")\n",
    "        fig.legend(loc=\"right\")\n",
    "        fig.savefig(f\"images_final/{typology}_{media_node}_qoe.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
