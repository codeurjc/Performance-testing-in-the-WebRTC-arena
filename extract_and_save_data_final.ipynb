{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from elasticsearch import Elasticsearch\n",
                "from elasticsearch.helpers import scan\n",
                "from dotenv import load_dotenv\n",
                "import os\n",
                "\n",
                "load_dotenv()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ELK_HOST = os.getenv(\"ELK_HOST\")\n",
                "\n",
                "es = Elasticsearch(hosts=[ELK_HOST], timeout=300, max_retries=10, retry_on_timeout=True, verify_certs=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "index_kurento = [\n",
                "    # \"loadtest-webrtc-stats-1690365332436\",  # 2p\n",
                "    # \"loadtest-webrtc-stats-1690895942335\",  # 2p 2\n",
                "    # \"loadtest-webrtc-stats-1690891582012\",  # 2p wait\n",
                "    # \"loadtest-webrtc-stats-1690890425599\",  # 2p retry\n",
                "    # \"loadtest-webrtc-stats-1690362665988\",  # 5p\n",
                "    # \"loadtest-webrtc-stats-1690965296328\",  # 5p 2\n",
                "    # \"loadtest-webrtc-stats-1690966854865\",  # 5p wait\n",
                "    # \"loadtest-webrtc-stats-1690970371887\",  # 5p retry\n",
                "    # \"loadtest-webrtc-stats-1690196895848\",  # 8p\n",
                "    # \"loadtest-webrtc-stats-1690969163381\",  # 8p 2\n",
                "    # \"loadtest-webrtc-stats-1690969681782\",  # 8p wait\n",
                "    # \"loadtest-webrtc-stats-1690967697727\",  # 8p retry\n",
                "    # \"loadtest-webrtc-stats-1690789379434\",  # 3p 10s\n",
                "    # \"loadtest-webrtc-stats-1691152754372\",  # 3p 10s 2\n",
                "    # \"loadtest-webrtc-stats-1691155815010\",  # 3p 10s wait\n",
                "    \"loadtest-webrtc-stats-merged-2\",  # 3p 10s retry\n",
                "    # \"loadtest-webrtc-stats-1690794060447\",  # 3p 20s\n",
                "    # \"loadtest-webrtc-stats-1691164225068\",  # 3p 20s 2\n",
                "    # \"loadtest-webrtc-stats-1691166738644\",  # 3p 20s wait\n",
                "    # \"loadtest-webrtc-stats-1691171182673\",  # 3p 20s retry\n",
                "    # \"loadtest-webrtc-stats-1690798385992\",  # 3p 40s\n",
                "    # \"loadtest-webrtc-stats-1691399145975\",  # 3p 40s 2\n",
                "    # \"loadtest-webrtc-stats-1691402396642\",  # 3p 40s wait\n",
                "    \"loadtest-webrtc-stats-merged-2\",  # 3p 40s retry\n",
                "]\n",
                "\n",
                "index_mediasoup = [\n",
                "    # \"loadtest-webrtc-stats-1690356001795\",  # 2p\n",
                "    # \"loadtest-webrtc-stats-1691416346507\",  # 2p 2\n",
                "    # \"loadtest-webrtc-stats-1691585204956\",  # 2p retry\n",
                "    # \"loadtest-webrtc-stats-1691593504959\",  # 2p wait\n",
                "    # \"loadtest-webrtc-stats-1690359898861\",  # 5p\n",
                "    # \"loadtest-webrtc-stats-1692266098118\",  # 5p 2\n",
                "    # \"loadtest-webrtc-stats-1692269700168\",  # 5p wait\n",
                "    # \"loadtest-webrtc-stats-1692272296369\",  # 5p retry\n",
                "    # \"loadtest-webrtc-stats-1690200305395\",  # 8p\n",
                "    # \"loadtest-webrtc-stats-1692274307361\",  # 8p 2\n",
                "    # \"loadtest-webrtc-stats-1692275538069\",  # 8p wait\n",
                "    # \"loadtest-webrtc-stats-1692282154006\",  # 8p retry\n",
                "    # \"loadtest-webrtc-stats-1690287488307\",  # 3p 10s\n",
                "    # \"loadtest-webrtc-stats-1692884047994\",  # 3p 10s 2\n",
                "    # \"loadtest-webrtc-stats-1693397244212\",  # 3p 10s wait\n",
                "    # \"loadtest-webrtc-stats-1693405440463\",  # 3p 10s retry\n",
                "    # \"loadtest-webrtc-stats-1690271453734\",  # 3p 20s\n",
                "    # \"loadtest-webrtc-stats-1693476766107\",  # 3p 20s 2\n",
                "    # \"loadtest-webrtc-stats-1693484700719\",  # 3p 20s wait\n",
                "    # \"loadtest-webrtc-stats-1693495106735\",  # 3p 20s retry\n",
                "    # \"loadtest-webrtc-stats-1690201527433\",  # 3p 40s\n",
                "    # \"loadtest-webrtc-stats-1693554010923\",  # 3p 40s 2\n",
                "    # \"loadtest-webrtc-stats-1693564102399\",  # 3p 40s wait\n",
                "    # \"loadtest-webrtc-stats-1693822798977\",  # 3p 40s retry\n",
                "]\n",
                "\n",
                "index_list = index_kurento + index_mediasoup\n",
                "\n",
                "index_kurento_names = [\n",
                "    # \"loadtest-webrtc-final-kurento-2p-t3medium\",\n",
                "    # \"loadtest-webrtc-final-kurento-2p-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-kurento-2p-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-kurento-2p-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-kurento-5p-t3medium\",\n",
                "    # \"loadtest-webrtc-final-kurento-5p-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-kurento-5p-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-kurento-5p-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-kurento-8p-t3medium\",\n",
                "    # \"loadtest-webrtc-final-kurento-8p-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-kurento-8p-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-kurento-8p-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-10s-t3medium\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-10s-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-10s-t3medium-wait\",\n",
                "    \"loadtest-webrtc-final-kurento-3p-10s-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-20s-t3medium\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-20s-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-20s-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-20s-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-40s-t3medium\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-40s-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-kurento-3p-40s-t3medium-wait\",\n",
                "    \"loadtest-webrtc-final-kurento-3p-40s-t3medium-retry\",\n",
                "]\n",
                "\n",
                "index_mediasoup_names = [\n",
                "    # \"loadtest-webrtc-final-mediasoup-2p-t3medium\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-2p-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-2p-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-2p-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-5p-t3medium\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-5p-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-5p-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-5p-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-8p-t3medium\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-8p-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-8p-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-8p-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-10s-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-20s-t3medium-retry\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium-2\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium-wait\",\n",
                "    # \"loadtest-webrtc-final-mediasoup-3p-40s-t3medium-retry\",\n",
                "]\n",
                "\n",
                "index_list_names = index_kurento_names + index_mediasoup_names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_end_times = pd.read_json(\"dfs_final/start-end-times.json\", orient=\"index\")\n",
                "start_end_times[\"from\"] = pd.to_datetime(\n",
                "    start_end_times[\"from\"], format=\"ISO8601\", utc=True\n",
                ")\n",
                "start_end_times[\"to\"] = pd.to_datetime(\n",
                "    start_end_times[\"to\"], format=\"ISO8601\", utc=True\n",
                ")\n",
                "\n",
                "\n",
                "def get_max_time(index):\n",
                "    # tmp_serie = pd.Series(\n",
                "    #     [df_index[\"@timestamp\"].max(), start_end_times.loc[index, \"to\"]]\n",
                "    # )\n",
                "    # return tmp_serie.max()\n",
                "    return start_end_times.loc[index, \"to\"]\n",
                "\n",
                "\n",
                "def get_min_time(index):\n",
                "    # tmp_serie = pd.Series(\n",
                "    #     [df_index[\"@timestamp\"].min(), start_end_times.loc[index, \"from\"]]\n",
                "    # )\n",
                "    # return tmp_serie.min()\n",
                "    return start_end_times.loc[index, \"from\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_qoe_data_from_elastic(index, index_name):\n",
                "    min = get_min_time(index_name)\n",
                "    max = get_max_time(index_name)\n",
                "    # query: The elasticsearch query.\n",
                "    query = {\"query\": {\"bool\": { \"must\": [{\"exists\": {\"field\": \"vmaf\"}}, {\"range\": {\"@timestamp\": {\"gte\": min, \"lte\": max}}}]}}}\n",
                "    # Scan function to get all the data.\n",
                "    rel = scan(\n",
                "        client=es,\n",
                "        query=query,\n",
                "        scroll=\"1m\",\n",
                "        index=index,\n",
                "        raise_on_error=True,\n",
                "        preserve_order=False,\n",
                "        clear_scroll=True,\n",
                "    )\n",
                "    # We need only '_source', which has all the fields required.\n",
                "    # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
                "    for hit in rel:\n",
                "        yield hit[\"_source\"]\n",
                "\n",
                "\n",
                "df_generators = (\n",
                "    pd.DataFrame(generate_qoe_data_from_elastic(index, index_list_names[i])) for i, index in enumerate(index_list)\n",
                ")\n",
                "\n",
                "df_list = list(df_generators)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for df_user in df_list:\n",
                "    if not df_user.empty:\n",
                "        df_user[\"@timestamp\"] = pd.to_datetime(df_user[\"@timestamp\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_user_data_from_elastic(index, index_name):\n",
                "    min = get_min_time(index_name)\n",
                "    max = get_max_time(index_name)\n",
                "    query = {\n",
                "        \"query\": {\n",
                "            \"bool\": {\n",
                "                \"must\": [{\n",
                "                    \"exists\": {\n",
                "                        \"field\": \"new_participant_id\"\n",
                "                    }\n",
                "                }, {\n",
                "                    \"range\": {\n",
                "                        \"@timestamp\": {\n",
                "                            \"gte\": min,\n",
                "                            \"lte\": max\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "                ]\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "    # Scan function to get all the data.\n",
                "    rel = scan(\n",
                "        client=es,\n",
                "        query=query,\n",
                "        scroll=\"1m\",\n",
                "        index=index,\n",
                "        raise_on_error=True,\n",
                "        preserve_order=False,\n",
                "        clear_scroll=True,\n",
                "    )\n",
                "    # We need only '_source', which has all the fields required.\n",
                "    # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
                "    for hit in rel:\n",
                "        source = hit[\"_source\"]\n",
                "        data_to_save = {\n",
                "            \"@timestamp\": source[\"@timestamp\"],\n",
                "            \"participant\": source[\"new_participant_id\"],\n",
                "            \"session\": source[\"new_participant_session\"],\n",
                "        }\n",
                "        yield data_to_save\n",
                "\n",
                "\n",
                "for i, index in enumerate(index_list):\n",
                "    df_generators = generate_user_data_from_elastic(index, index_list_names[i])\n",
                "\n",
                "    df_users = pd.DataFrame(df_generators)\n",
                "    df_users[\"@timestamp\"] = pd.to_datetime(df_users[\"@timestamp\"])\n",
                "    df_users = df_users.sort_values(by=\"@timestamp\")\n",
                "    df_users.to_csv(f\"dfs_final/{index_list_names[i]}-user-join.csv\", index=False)\n",
                "    df = df_list[i].sort_values(by=\"@timestamp\")\n",
                "    df['user_count'] = [len(df_users[df_users['@timestamp'] <= ts]) for ts in df['@timestamp']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"dfs_final\", exist_ok=True)\n",
                "for i, df in enumerate(df_list):\n",
                "    if not df.empty:\n",
                "        df.to_csv(f\"dfs_final/{index_list_names[i]}.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warning: this process can be long (hours).\n",
                "node_types = [\"browseremulator\", \"masternode\", \"medianode\"]\n",
                "for i, df_user in enumerate(df_list):\n",
                "    if not df_user.empty:\n",
                "        index_name = index_list_names[i]\n",
                "        current_time = pd.Timestamp.now().isoformat()\n",
                "        min = get_min_time(index_name)\n",
                "        max = get_max_time(index_name)\n",
                "        for node_type in node_types:\n",
                "            query = {\n",
                "                \"query\": {\n",
                "                    \"bool\": {\n",
                "                        \"must\": [\n",
                "                            {\"match\": {\"fields.node_role\": node_type}},\n",
                "                            {\n",
                "                                \"range\": {\n",
                "                                    \"@timestamp\": {\n",
                "                                        \"lte\": max.isoformat(),\n",
                "                                        \"gte\": min.isoformat(),\n",
                "                                    }\n",
                "                                }\n",
                "                            },\n",
                "                        ]\n",
                "                    }\n",
                "                }\n",
                "            }\n",
                "            rel = scan(\n",
                "                client=es,\n",
                "                query=query,\n",
                "                scroll=\"8h\",\n",
                "                index=\"metric*\",\n",
                "                raise_on_error=True,\n",
                "                preserve_order=False,\n",
                "                clear_scroll=True,\n",
                "                request_timeout=300,\n",
                "            )\n",
                "\n",
                "            # We need only '_source', which has all the fields required.\n",
                "            # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
                "            def generate_data():\n",
                "                i = 0\n",
                "                for hit in rel:\n",
                "                    i += 1\n",
                "                    print(\"Data read: \", i, end=\"\\r\")\n",
                "                    data = hit[\"_source\"]\n",
                "                    data_to_save = {\"@timestamp\": data[\"@timestamp\"]}\n",
                "                    if \"system\" in data:\n",
                "                        data = data[\"system\"]\n",
                "                        if \"cpu\" in data:\n",
                "                            data_to_save[\"cpu\"] = data[\"cpu\"][\"total\"][\"norm\"][\"pct\"]\n",
                "                        if \"memory\" in data:\n",
                "                            data_to_save[\"memory\"] = data[\"memory\"][\"used\"][\"pct\"]\n",
                "                    if \"cpu\" in data or \"memory\" in data:\n",
                "                        yield data_to_save\n",
                "\n",
                "            print(f\"{current_time} - Processing {index_name}-{node_type}\")\n",
                "            data_generator = generate_data()\n",
                "            # Create a dataframe.\n",
                "            df = pd.DataFrame(data_generator)\n",
                "            if not df.empty:\n",
                "                df = df.groupby(\"@timestamp\", as_index=False).mean()\n",
                "                df[\"@timestamp\"] = pd.to_datetime(df[\"@timestamp\"])\n",
                "                df.to_csv(\n",
                "                    f\"dfs_final/{index_list_names[i]}-{node_type}.csv\", index=False\n",
                "                )\n",
                "                current_time = pd.Timestamp.now().isoformat()\n",
                "                print(f\"{current_time} - Saved {index_name}-{node_type}.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "for i, df_user in enumerate(df_list):\n",
                "    index_name = index_list_names[i]\n",
                "    current_time = pd.Timestamp.now().isoformat()\n",
                "    print(f\"{current_time} - Processing {index_name}\")\n",
                "    if not df_user.empty:\n",
                "        min = get_min_time(index_list_names[i])\n",
                "        max = get_max_time(index_list_names[i])\n",
                "        query = {\n",
                "            \"query\": {\n",
                "                \"bool\": {\n",
                "                    \"must\": [\n",
                "                        {\"exists\": {\"field\": \"webrtc_stats\"}},\n",
                "                        {\n",
                "                            \"range\": {\n",
                "                                \"@timestamp\": {\n",
                "                                    \"lte\": max.isoformat(),\n",
                "                                    \"gte\": min.isoformat(),\n",
                "                                }\n",
                "                            }\n",
                "                        },\n",
                "                    ]\n",
                "                }\n",
                "            }\n",
                "        }\n",
                "        rel = scan(\n",
                "            client=es,\n",
                "            query=query,\n",
                "            scroll=\"1m\",\n",
                "            index=index_list[i],\n",
                "            raise_on_error=True,\n",
                "            preserve_order=False,\n",
                "            clear_scroll=True,\n",
                "        )\n",
                "        # Keep response in a list.\n",
                "        result = list(rel)\n",
                "        temp_inbound = []\n",
                "        temp_outbound = []\n",
                "        # We need only '_source', which has all the fields required.\n",
                "        # This eliminates the elasticsearch metdata like _id, _type, _index.\n",
                "        for hit in result:\n",
                "            data = hit[\"_source\"]\n",
                "            data_to_save = {\n",
                "                \"@timestamp\": data[\"@timestamp\"],\n",
                "                \"user_id\": json.loads(data[\"participant_id\"])[\"clientData\"].split(\"_\")[\n",
                "                    2\n",
                "                ],\n",
                "                \"session_id\": data[\"session_id\"]\n",
                "            }\n",
                "            webrtc_stats = data[\"webrtc_stats\"]\n",
                "            if \"inbound\" in webrtc_stats:\n",
                "                for key, value in webrtc_stats[\"inbound\"][\"audio\"].items():\n",
                "                    data_to_save[f\"audio_{key}\"] = value\n",
                "                for key, value in webrtc_stats[\"inbound\"][\"video\"].items():\n",
                "                    data_to_save[f\"video_{key}\"] = value\n",
                "                temp_inbound.append(data_to_save)\n",
                "            if \"outbound\" in webrtc_stats:\n",
                "                for key, value in webrtc_stats[\"outbound\"][\"audio\"].items():\n",
                "                    data_to_save[f\"audio_{key}\"] = value\n",
                "                for key, value in webrtc_stats[\"outbound\"][\"video\"].items():\n",
                "                    data_to_save[f\"video_{key}\"] = value\n",
                "                temp_outbound.append(data_to_save)\n",
                "\n",
                "        df_inbound = pd.DataFrame(temp_inbound)\n",
                "        if not df_inbound.empty:\n",
                "            df_inbound[\"@timestamp\"] = pd.to_datetime(df_inbound[\"@timestamp\"])\n",
                "            df_inbound.to_csv(\n",
                "                f\"dfs_final/{index_list_names[i]}-webrtc-stats-inbound.csv\",\n",
                "                index=False,\n",
                "            )\n",
                "\n",
                "        df_outbound = pd.DataFrame(temp_outbound)\n",
                "        if not df_outbound.empty:\n",
                "            df_outbound[\"@timestamp\"] = pd.to_datetime(df_outbound[\"@timestamp\"])\n",
                "            df_outbound.to_csv(\n",
                "                f\"dfs_final/{index_list_names[i]}-webrtc-stats-outbound.csv\", index=False\n",
                "            )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from minio import Minio\n",
                "\n",
                "client = Minio(\n",
                "        os.getenv(\"MINIO_HOST\"),\n",
                "        access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n",
                "        secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n",
                "    )\n",
                "prefix = \"openvidu-loadtest-final-\"\n",
                "\n",
                "objects_names = pd.DataFrame(columns=[\"index\", \"session\", \"userFrom\", \"userTo\", \"error\"])\n",
                "for i, index in enumerate(index_list_names):\n",
                "    index_split = index.split(\"-\")\n",
                "    bucket_name = prefix\n",
                "    if i < 24:\n",
                "        bucket_name += \"kurento-\"\n",
                "    else:\n",
                "        bucket_name += \"mediasoup-\"\n",
                "    bucket_name += index_split[4] + \"-\"\n",
                "    if \"s\" in index_split[5]:\n",
                "        bucket_name += index_split[5] + \"-t3medium\"\n",
                "    else:\n",
                "        bucket_name += \"t3medium\"\n",
                "    if index_split[-1] == \"2\":\n",
                "        bucket_name += \"-2\"\n",
                "    elif index_split[-1] == \"wait\":\n",
                "        bucket_name += \"-wait\"\n",
                "    elif index_split[-1] == \"retry\":\n",
                "        bucket_name += \"-retry\"\n",
                "        if bucket_name == \"openvidu-loadtest-final-kurento-2p-t3medium-retry\":\n",
                "            bucket_name = \"openvidu-loadtest-final-kurento-2p-t3medium-retries\"\n",
                "    \n",
                "    found = client.bucket_exists(bucket_name)\n",
                "    if not found:\n",
                "        raise Exception(\"Bucket does not exist\")\n",
                "\n",
                "    objects = client.list_objects(bucket_name, recursive=False)\n",
                "    for obj in objects:\n",
                "        name = obj.object_name\n",
                "        split = name.split(\"_\")\n",
                "        if \"error\" in name:\n",
                "            objects_names.loc[len(objects_names.index)] = {\n",
                "                \"index\": index,\n",
                "                \"session\": split[3],\n",
                "                \"userFrom\": split[4],\n",
                "                \"userTo\": split[5].split(\".\")[0],\n",
                "                \"error\": True\n",
                "            }\n",
                "        else:\n",
                "            objects_names.loc[len(objects_names.index)] = {\n",
                "                \"index\": index,\n",
                "                \"session\": split[1],\n",
                "                \"userFrom\": split[2],\n",
                "                \"userTo\": split[3].split(\".\")[0],\n",
                "                \"error\": False\n",
                "            }\n",
                "        \n",
                "objects_names.to_csv(f\"dfs_final/minio_items.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
