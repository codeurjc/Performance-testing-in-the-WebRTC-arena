{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: elasticsearch===7.8.0 in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (7.8.0)\n",
                        "Requirement already satisfied: matplotlib in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
                        "Requirement already satisfied: python-dotenv in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
                        "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (1.11.3)\n",
                        "Requirement already satisfied: minio in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 6)) (7.1.17)\n",
                        "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 7)) (3.1.2)\n",
                        "Requirement already satisfied: statsmodels in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 8)) (0.14.0)\n",
                        "Requirement already satisfied: seaborn in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 9)) (0.13.0)\n",
                        "Requirement already satisfied: ruptures in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 10)) (1.1.8)\n",
                        "Requirement already satisfied: pandas[html] in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
                        "Requirement already satisfied: urllib3>=1.21.1 in c:\\python312\\lib\\site-packages (from elasticsearch===7.8.0->-r requirements.txt (line 1)) (2.0.7)\n",
                        "Requirement already satisfied: certifi in c:\\python312\\lib\\site-packages (from elasticsearch===7.8.0->-r requirements.txt (line 1)) (2023.7.22)\n",
                        "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (1.26.1)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (2.8.2)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (2023.3.post1)\n",
                        "Requirement already satisfied: tzdata>=2022.1 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (2023.3)\n",
                        "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (4.12.2)\n",
                        "Requirement already satisfied: html5lib>=1.1 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (1.1)\n",
                        "Requirement already satisfied: lxml>=4.8.0 in c:\\python312\\lib\\site-packages (from pandas[html]->-r requirements.txt (line 2)) (4.9.3)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.1.1)\n",
                        "Requirement already satisfied: cycler>=0.10 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.43.1)\n",
                        "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.5)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (23.2)\n",
                        "Requirement already satisfied: pillow>=8 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (10.1.0)\n",
                        "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.1.1)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->-r requirements.txt (line 7)) (2.1.3)\n",
                        "Requirement already satisfied: patsy>=0.5.2 in c:\\python312\\lib\\site-packages (from statsmodels->-r requirements.txt (line 8)) (0.5.3)\n",
                        "Requirement already satisfied: soupsieve>1.2 in c:\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->pandas[html]->-r requirements.txt (line 2)) (2.5)\n",
                        "Requirement already satisfied: six>=1.9 in c:\\python312\\lib\\site-packages (from html5lib>=1.1->pandas[html]->-r requirements.txt (line 2)) (1.16.0)\n",
                        "Requirement already satisfied: webencodings in c:\\python312\\lib\\site-packages (from html5lib>=1.1->pandas[html]->-r requirements.txt (line 2)) (0.5.1)\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "from elasticsearch import Elasticsearch\n",
                "from elasticsearch.helpers import scan\n",
                "from dotenv import load_dotenv\n",
                "import os\n",
                "\n",
                "load_dotenv()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "ELK_HOST = os.getenv(\"ELK_HOST\")\n",
                "\n",
                "es = Elasticsearch(hosts=[ELK_HOST], timeout=300, max_retries=10, retry_on_timeout=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "index_lk = [\n",
                "    \"loadtest-webrtc-stats-1705318060009\",  # 2p retry\n",
                "    \"loadtest-webrtc-stats-1704977399068\",  # 8p retry\n",
                "    \"loadtest-webrtc-stats-1704970758533\",  # 3p 10 s retry\n",
                "    \"loadtest-webrtc-stats-1704987214896\",  # 3p 40 s retry\n",
                "]\n",
                "\n",
                "index_list = index_lk\n",
                "\n",
                "index_lk_names = [\n",
                "    \"loadtest-webrtc-final-livekit-2p-t3medium-retry\",\n",
                "    \"loadtest-webrtc-final-livekit-8p-t3medium-retry\",\n",
                "    \"loadtest-webrtc-final-livekit-3p-10s-t3medium-retry\",\n",
                "    \"loadtest-webrtc-final-livekit-3p-40s-t3medium-retry\",\n",
                "]\n",
                "\n",
                "index_list_names = index_lk_names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_end_times = pd.read_json(\"dfs_final/start-end-times.json\", orient=\"index\")\n",
                "start_end_times[\"from\"] = pd.to_datetime(\n",
                "    start_end_times[\"from\"], format=\"ISO8601\"\n",
                ").dt.tz_convert(\"UTC\")\n",
                "start_end_times[\"to\"] = pd.to_datetime(\n",
                "    start_end_times[\"to\"], format=\"ISO8601\"\n",
                ").dt.tz_convert(\"UTC\")\n",
                "\n",
                "\n",
                "def get_max_time(df_index, index):\n",
                "    tmp_serie = pd.Series(\n",
                "        [df_index[\"@timestamp\"].max(), start_end_times.loc[index, \"to\"]]\n",
                "    )\n",
                "    return tmp_serie.max()\n",
                "\n",
                "\n",
                "def get_min_time(df_index, index):\n",
                "    tmp_serie = pd.Series(\n",
                "        [df_index[\"@timestamp\"].min(), start_end_times.loc[index, \"from\"]]\n",
                "    )\n",
                "    return tmp_serie.min()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_qoe_data_from_elastic(index):\n",
                "    # query: The elasticsearch query.\n",
                "    query = {\"query\": {\"exists\": {\"field\": \"vmaf\"}}}\n",
                "    # Scan function to get all the data.\n",
                "    rel = scan(\n",
                "        client=es,\n",
                "        query=query,\n",
                "        scroll=\"1m\",\n",
                "        index=index,\n",
                "        raise_on_error=True,\n",
                "        preserve_order=False,\n",
                "        clear_scroll=True,\n",
                "    )\n",
                "    # We need only '_source', which has all the fields required.\n",
                "    # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
                "    for hit in rel:\n",
                "        yield hit[\"_source\"]\n",
                "\n",
                "\n",
                "df_generators = (\n",
                "    pd.DataFrame(generate_qoe_data_from_elastic(index)) for index in index_list\n",
                ")\n",
                "\n",
                "df_list = list(df_generators)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "for df_user in df_list:\n",
                "    if not df_user.empty:\n",
                "        df_user[\"@timestamp\"] = pd.to_datetime(df_user[\"@timestamp\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_user_data_from_elastic(index):\n",
                "    # query: The elasticsearch query.\n",
                "    query = {\"query\": {\"exists\": {\"field\": \"new_participant_id\"}}}\n",
                "    # Scan function to get all the data.\n",
                "    rel = scan(\n",
                "        client=es,\n",
                "        query=query,\n",
                "        scroll=\"1m\",\n",
                "        index=index,\n",
                "        raise_on_error=True,\n",
                "        preserve_order=False,\n",
                "        clear_scroll=True,\n",
                "    )\n",
                "    # We need only '_source', which has all the fields required.\n",
                "    # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
                "    for hit in rel:\n",
                "        source = hit[\"_source\"]\n",
                "        data_to_save = {\n",
                "            \"@timestamp\": source[\"@timestamp\"],\n",
                "            \"participant\": source[\"new_participant_id\"],\n",
                "            \"session\": source[\"new_participant_session\"],\n",
                "        }\n",
                "        yield data_to_save\n",
                "\n",
                "\n",
                "for i, index in enumerate(index_list):\n",
                "    df_generators = generate_user_data_from_elastic(index)\n",
                "\n",
                "    df_users = pd.DataFrame(df_generators)\n",
                "    df_users[\"@timestamp\"] = pd.to_datetime(df_users[\"@timestamp\"])\n",
                "    df_users = df_users.sort_values(by=\"@timestamp\")\n",
                "    df_users.to_csv(f\"dfs_final/{index_list_names[i]}-user-join.csv\", index=False)\n",
                "    df = df_list[i].sort_values(by=\"@timestamp\")\n",
                "    df['user_count'] = [len(df_users[df_users['@timestamp'] <= ts]) for ts in df['@timestamp']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"dfs_final\", exist_ok=True)\n",
                "for i, df in enumerate(df_list):\n",
                "    if not df.empty:\n",
                "        df.to_csv(f\"dfs_final/{index_list_names[i]}.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2024-02-26T11:42:51.292409 - Processing loadtest-webrtc-final-livekit-2p-t3medium-retry-browseremulator\n",
                        "2024-02-26T11:45:02.421444 - Saved loadtest-webrtc-final-livekit-2p-t3medium-retry-browseremulator.csv\n",
                        "2024-02-26T11:45:02.421444 - Processing loadtest-webrtc-final-livekit-2p-t3medium-retry-masternode\n",
                        "2024-02-26T11:45:02.421444 - Processing loadtest-webrtc-final-livekit-2p-t3medium-retry-medianode\n",
                        "2024-02-26T11:45:03.960667 - Saved loadtest-webrtc-final-livekit-2p-t3medium-retry-medianode.csv\n",
                        "2024-02-26T11:45:03.960667 - Processing loadtest-webrtc-final-livekit-8p-t3medium-retry-browseremulator\n",
                        "2024-02-26T11:50:16.005032 - Saved loadtest-webrtc-final-livekit-8p-t3medium-retry-browseremulator.csv\n",
                        "2024-02-26T11:50:16.005032 - Processing loadtest-webrtc-final-livekit-8p-t3medium-retry-masternode\n",
                        "2024-02-26T11:50:16.005032 - Processing loadtest-webrtc-final-livekit-8p-t3medium-retry-medianode\n",
                        "2024-02-26T11:50:19.958667 - Saved loadtest-webrtc-final-livekit-8p-t3medium-retry-medianode.csv\n",
                        "2024-02-26T11:50:19.958667 - Processing loadtest-webrtc-final-livekit-3p-10s-t3medium-retry-browseremulator\n",
                        "2024-02-26T11:56:49.403198 - Saved loadtest-webrtc-final-livekit-3p-10s-t3medium-retry-browseremulator.csv\n",
                        "2024-02-26T11:56:49.403198 - Processing loadtest-webrtc-final-livekit-3p-10s-t3medium-retry-masternode\n",
                        "2024-02-26T11:56:49.403198 - Processing loadtest-webrtc-final-livekit-3p-10s-t3medium-retry-medianode\n",
                        "2024-02-26T11:56:53.526993 - Saved loadtest-webrtc-final-livekit-3p-10s-t3medium-retry-medianode.csv\n",
                        "2024-02-26T11:56:53.526993 - Processing loadtest-webrtc-final-livekit-3p-40s-t3medium-retry-browseremulator\n",
                        "2024-02-26T12:00:55.852218 - Saved loadtest-webrtc-final-livekit-3p-40s-t3medium-retry-browseremulator.csv\n",
                        "2024-02-26T12:00:55.852218 - Processing loadtest-webrtc-final-livekit-3p-40s-t3medium-retry-masternode\n",
                        "2024-02-26T12:00:55.852218 - Processing loadtest-webrtc-final-livekit-3p-40s-t3medium-retry-medianode\n",
                        "2024-02-26T12:00:58.529197 - Saved loadtest-webrtc-final-livekit-3p-40s-t3medium-retry-medianode.csv\n"
                    ]
                }
            ],
            "source": [
                "# Warning: this process can be long (hours).\n",
                "node_types = [\"browseremulator\", \"masternode\", \"medianode\"]\n",
                "for i, df_user in enumerate(df_list):\n",
                "    if not df_user.empty:\n",
                "        index_name = index_list_names[i]\n",
                "        current_time = pd.Timestamp.now().isoformat()\n",
                "        min = get_min_time(df_user, index_name)\n",
                "        max = get_max_time(df_user, index_name)\n",
                "        for node_type in node_types:\n",
                "            query = {\n",
                "                \"query\": {\n",
                "                    \"bool\": {\n",
                "                        \"must\": [\n",
                "                            {\"match\": {\"fields.node_role\": node_type}},\n",
                "                            {\n",
                "                                \"range\": {\n",
                "                                    \"@timestamp\": {\n",
                "                                        \"lte\": max.isoformat(),\n",
                "                                        \"gte\": min.isoformat(),\n",
                "                                    }\n",
                "                                }\n",
                "                            },\n",
                "                        ]\n",
                "                    }\n",
                "                }\n",
                "            }\n",
                "            rel = scan(\n",
                "                client=es,\n",
                "                query=query,\n",
                "                scroll=\"8h\",\n",
                "                index=\"metric*\",\n",
                "                raise_on_error=True,\n",
                "                preserve_order=False,\n",
                "                clear_scroll=True,\n",
                "                request_timeout=300,\n",
                "            )\n",
                "\n",
                "            # We need only '_source', which has all the fields required.\n",
                "            # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
                "            def generate_data():\n",
                "                i = 0\n",
                "                for hit in rel:\n",
                "                    i += 1\n",
                "                    print(\"Data read: \", i, end=\"\\r\")\n",
                "                    data = hit[\"_source\"]\n",
                "                    data_to_save = {\"@timestamp\": data[\"@timestamp\"]}\n",
                "                    if \"system\" in data:\n",
                "                        data = data[\"system\"]\n",
                "                        if \"cpu\" in data:\n",
                "                            data_to_save[\"cpu\"] = data[\"cpu\"][\"total\"][\"norm\"][\"pct\"]\n",
                "                        if \"memory\" in data:\n",
                "                            data_to_save[\"memory\"] = data[\"memory\"][\"used\"][\"pct\"]\n",
                "                    if \"cpu\" in data or \"memory\" in data:\n",
                "                        yield data_to_save\n",
                "\n",
                "            print(f\"{current_time} - Processing {index_name}-{node_type}\")\n",
                "            data_generator = generate_data()\n",
                "            # Create a dataframe.\n",
                "            df = pd.DataFrame(data_generator)\n",
                "            if not df.empty:\n",
                "                df = df.groupby(\"@timestamp\", as_index=False).mean()\n",
                "                df[\"@timestamp\"] = pd.to_datetime(df[\"@timestamp\"])\n",
                "                df.to_csv(\n",
                "                    f\"dfs_final/{index_list_names[i]}-{node_type}.csv\", index=False\n",
                "                )\n",
                "                current_time = pd.Timestamp.now().isoformat()\n",
                "                print(f\"{current_time} - Saved {index_name}-{node_type}.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2024-02-26T12:16:24.275438 - Processing loadtest-webrtc-final-livekit-2p-t3medium-retry\n",
                        "2024-02-26T12:16:36.585213 - Processing loadtest-webrtc-final-livekit-8p-t3medium-retry\n",
                        "2024-02-26T12:16:51.607536 - Processing loadtest-webrtc-final-livekit-3p-10s-t3medium-retry\n",
                        "2024-02-26T12:17:01.835214 - Processing loadtest-webrtc-final-livekit-3p-40s-t3medium-retry\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "\n",
                "for i, df_user in enumerate(df_list):\n",
                "    index_name = index_list_names[i]\n",
                "    current_time = pd.Timestamp.now().isoformat()\n",
                "    print(f\"{current_time} - Processing {index_name}\")\n",
                "    if not df_user.empty:\n",
                "        min = get_min_time(df_user, index_list_names[i])\n",
                "        max = get_max_time(df_user, index_list_names[i])\n",
                "        query = {\n",
                "            \"query\": {\n",
                "                \"bool\": {\n",
                "                    \"must\": [\n",
                "                        {\"exists\": {\"field\": \"webrtc_stats\"}},\n",
                "                        {\n",
                "                            \"range\": {\n",
                "                                \"@timestamp\": {\n",
                "                                    \"lte\": max.isoformat(),\n",
                "                                    \"gte\": min.isoformat(),\n",
                "                                }\n",
                "                            }\n",
                "                        },\n",
                "                    ]\n",
                "                }\n",
                "            }\n",
                "        }\n",
                "        rel = scan(\n",
                "            client=es,\n",
                "            query=query,\n",
                "            scroll=\"1m\",\n",
                "            index=index_list[i],\n",
                "            raise_on_error=True,\n",
                "            preserve_order=False,\n",
                "            clear_scroll=True,\n",
                "        )\n",
                "        # Keep response in a list.\n",
                "        result = list(rel)\n",
                "        temp_inbound = []\n",
                "        temp_outbound = []\n",
                "        # We need only '_source', which has all the fields required.\n",
                "        # This eliminates the elasticsearch metdata like _id, _type, _index.\n",
                "        for hit in result:\n",
                "            data = hit[\"_source\"]\n",
                "            data_to_save = {\n",
                "                \"@timestamp\": data[\"@timestamp\"],\n",
                "                \"user_id\": data[\"participant_id\"],\n",
                "                \"session_id\": data[\"session_id\"]\n",
                "            }\n",
                "            webrtc_stats = data[\"webrtc_stats\"]\n",
                "            if \"inbound\" in webrtc_stats:\n",
                "                if \"audio\" in webrtc_stats[\"inbound\"]:\n",
                "                    for key, value in webrtc_stats[\"inbound\"][\"audio\"].items():\n",
                "                        data_to_save[f\"audio_{key}\"] = value\n",
                "                if \"video\" in webrtc_stats[\"inbound\"]:\n",
                "                    for key, value in webrtc_stats[\"inbound\"][\"video\"].items():\n",
                "                        data_to_save[f\"video_{key}\"] = value\n",
                "                temp_inbound.append(data_to_save)\n",
                "            if \"outbound\" in webrtc_stats:\n",
                "                if \"audio\" in webrtc_stats[\"outbound\"]:\n",
                "                    for key, value in webrtc_stats[\"outbound\"][\"audio\"].items():\n",
                "                        data_to_save[f\"audio_{key}\"] = value\n",
                "                if \"video\" in webrtc_stats[\"outbound\"]:\n",
                "                    for key, value in webrtc_stats[\"outbound\"][\"video\"].items():\n",
                "                        data_to_save[f\"video_{key}\"] = value\n",
                "                temp_outbound.append(data_to_save)\n",
                "\n",
                "        df_inbound = pd.DataFrame(temp_inbound)\n",
                "        if not df_inbound.empty:\n",
                "            df_inbound[\"@timestamp\"] = pd.to_datetime(df_inbound[\"@timestamp\"])\n",
                "            df_inbound.to_csv(\n",
                "                f\"dfs_final/{index_list_names[i]}-webrtc-stats-outbound.csv\",\n",
                "                index=False,\n",
                "            )\n",
                "\n",
                "        df_outbound = pd.DataFrame(temp_outbound)\n",
                "        if not df_outbound.empty:\n",
                "            df_outbound[\"@timestamp\"] = pd.to_datetime(df_outbound[\"@timestamp\"])\n",
                "            df_outbound.to_csv(\n",
                "                f\"dfs_final/{index_list_names[i]}-webrtc-stats-inbound.csv\", index=False\n",
                "            )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from minio import Minio\n",
                "\n",
                "client = Minio(\n",
                "        os.getenv(\"MINIO_HOST\"),\n",
                "        access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n",
                "        secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n",
                "    )\n",
                "prefix = \"openvidu-loadtest-final-\"\n",
                "\n",
                "objects_names = pd.DataFrame(columns=[\"index\", \"session\", \"userFrom\", \"userTo\", \"error\"])\n",
                "for i, index in enumerate(index_list_names):\n",
                "    index_split = index.split(\"-\")\n",
                "    bucket_name = prefix\n",
                "    bucket_name += \"livekit-\"\n",
                "\n",
                "    bucket_name += index_split[4] + \"-\"\n",
                "    if \"s\" in index_split[5]:\n",
                "        bucket_name += index_split[5] + \"-t3medium\"\n",
                "    else:\n",
                "        bucket_name += \"t3medium\"\n",
                "    bucket_name += \"-retry\"\n",
                "    \n",
                "    found = client.bucket_exists(bucket_name)\n",
                "    if not found:\n",
                "        raise Exception(\"Bucket does not exist\")\n",
                "\n",
                "    objects = client.list_objects(bucket_name, recursive=False)\n",
                "    for obj in objects:\n",
                "        name = obj.object_name\n",
                "        split = name.split(\"_\")\n",
                "        if \"error\" in name:\n",
                "            objects_names.loc[len(objects_names.index)] = {\n",
                "                \"index\": index,\n",
                "                \"session\": split[3],\n",
                "                \"userFrom\": split[4],\n",
                "                \"userTo\": split[5].split(\".\")[0],\n",
                "                \"error\": True\n",
                "            }\n",
                "        else:\n",
                "            objects_names.loc[len(objects_names.index)] = {\n",
                "                \"index\": index,\n",
                "                \"session\": split[1],\n",
                "                \"userFrom\": split[2],\n",
                "                \"userTo\": split[3].split(\".\")[0],\n",
                "                \"error\": False\n",
                "            }\n",
                "        \n",
                "objects_names.to_csv(f\"dfs_final/minio_items_lk.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
