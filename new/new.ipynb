{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import orjson\n",
    "from collections import Counter\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [25, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set list of test done\n",
    "index_list_names = [\n",
    "    \"loadtest-webrtc-2024-kurento-2p\",\n",
    "    \"loadtest-webrtc-2024-kurento-8p\",\n",
    "    \"loadtest-webrtc-2024-kurento-3p-10s\",\n",
    "    \"loadtest-webrtc-2024-kurento-3p-40s\",\n",
    "    \"loadtest-webrtc-2024-pion-2p\",\n",
    "    \"loadtest-webrtc-2024-pion-8p\",\n",
    "    \"loadtest-webrtc-2024-pion-3p-10s\",\n",
    "    \"loadtest-webrtc-2024-pion-3p-40s\",\n",
    "    \"loadtest-webrtc-2024-mediasoup-2p\",\n",
    "    \"loadtest-webrtc-2024-mediasoup-8p\",\n",
    "    \"loadtest-webrtc-2024-mediasoup-3p-10s\",\n",
    "    \"loadtest-webrtc-2024-mediasoup-3p-40s\",\n",
    "]\n",
    "\n",
    "index_list_full_names = [\n",
    "    \"Kurento, 2 publishers per session\",\n",
    "    \"Kurento, 8 publishers per session\",\n",
    "    \"Kurento, 3 publishers and 10 subscribers per session\",\n",
    "    \"Kurento, 3 publishers and 40 subscribers per session\",\n",
    "    \"Pion, 2 publishers per session\",\n",
    "    \"Pion, 8 publishers per session\",\n",
    "    \"Pion, 3 publishers and 10 subscribers per session\",\n",
    "    \"Pion, 3 publishers and 40 subscribers per session\",\n",
    "    \"Mediasoup, 2 publishers per session\",\n",
    "    \"Mediasoup, 8 publishers per session\",\n",
    "    \"Mediasoup, 3 publishers and 10 subscribers per session\",\n",
    "    \"Mediasoup, 3 publishers and 40 subscribers per session\",\n",
    "]\n",
    "\n",
    "# index_list_names = [\n",
    "#     \"loadtest-webrtc-ov3-8p-pion\",\n",
    "#     \"loadtest-webrtc-ov3-8p-mediasoup\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def read_file(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            return orjson.loads(f.read())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_index_data(index_list_name, data_type):\n",
    "    full_user_data = []\n",
    "\n",
    "    for entry in os.scandir(f'stats/{index_list_name}'):\n",
    "        if entry.is_dir():\n",
    "            for sub_entry in os.scandir(entry.path):\n",
    "                if sub_entry.is_dir():\n",
    "                    data_path = os.path.join(sub_entry.path, data_type + '.json')\n",
    "                    if os.path.exists(data_path):\n",
    "                        data = read_file(data_path)\n",
    "                        full_user_data.append({\n",
    "                            'user': sub_entry.name,\n",
    "                            'session': entry.name,\n",
    "                            data_type: data\n",
    "                        })\n",
    "    return full_user_data\n",
    "\n",
    "\n",
    "def is_publisher(index, user):\n",
    "    return (\"8p\" in index) or (user <= 3)\n",
    "\n",
    "def process_user_data(user_data):\n",
    "    data = []\n",
    "    for user in user_data:\n",
    "        for event in user['events']:\n",
    "            data.append({\n",
    "                'user': int(user['user'].replace(\"User\", \"\")),\n",
    "                'session': int(user['session'].replace(\"LoadTestSession\", \"\")),\n",
    "                'event': event['event'] + (f\"-{event['connection']}\" if 'connection' in event else \"\"),\n",
    "                'connection': event['connection'] if 'connection' in event else None,\n",
    "                'timestamp': event['timestamp'],\n",
    "            })\n",
    "    return data\n",
    "\n",
    "def process_index_list(index_list_name):\n",
    "    user_data = get_index_data(index_list_name, \"events\")\n",
    "    data = process_user_data(user_data)\n",
    "    events_df = pd.DataFrame(data)\n",
    "    events_df['timestamp'] = pd.to_datetime(events_df['timestamp'])\n",
    "    events_df = events_df.sort_values(by='timestamp')\n",
    "    return index_list_name, events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all events files and save relevant data in DataFrames\n",
    "events_dfs = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = {executor.submit(process_index_list, index_list_name): index_list_name for index_list_name in index_list_names}\n",
    "    for future in as_completed(futures):\n",
    "        index_list_name, events_df = future.result()\n",
    "        events_dfs[index_list_name] = events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read log file and get specific lines with regex to get the connection failures\n",
    "import re\n",
    "start_end_times = {}\n",
    "def process_log_file(index, log_file):\n",
    "    regex_error = re.compile(r'(.*) ERROR .* Participant (.*) in session (.*) failed (\\d+) times')\n",
    "    regex_start = re.compile(r'(.*) INFO .* Starting test with .*')\n",
    "    regex_end = re.compile(r'(.*) INFO .* Saved result in a .*')\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S.%f %z\"\n",
    "    failed_attempts = []\n",
    "    start_match = None\n",
    "    end_match = None\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Participant' in line:\n",
    "                error_match = regex_error.search(line)\n",
    "                if error_match:\n",
    "                    failed_attempts.append({\n",
    "                        'user': error_match.group(2).replace('User', ''),\n",
    "                        'session': error_match.group(3).replace('LoadTestSession', ''),\n",
    "                        'attempts': int(error_match.group(4)),  # Ensure attempts is an integer\n",
    "                        'timestamp': datetime.strptime(error_match.group(1) + \" +0200\", date_format),\n",
    "                        'full_user': f\"{error_match.group(2).replace('User', '')}-{error_match.group(3).replace('LoadTestSession', '')}\",\n",
    "                    })\n",
    "            if not start_match:\n",
    "                start_match = regex_start.search(line)\n",
    "            if not end_match:\n",
    "                end_match = regex_end.search(line)\n",
    "    if start_match and end_match:\n",
    "        start_end_times[index] = (datetime.strptime(start_match.group(1) + \" +0200\", date_format),\n",
    "                                  datetime.strptime(end_match.group(1) + \" +0200\", date_format))\n",
    "    return log_file, pd.DataFrame(failed_attempts)\n",
    "\n",
    "failures = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = {executor.submit(process_log_file, index_list_name, f'logs/{index_list_name}.log'): index_list_name for index_list_name in index_list_names}\n",
    "    for future in as_completed(futures):\n",
    "        log_file, df = future.result()\n",
    "        index_list_name = log_file.split('/')[-1].replace('.log', '')\n",
    "        failures[index_list_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test purposes, paint all events in a timeline\n",
    "# for test, events_df in events_dfs.items():\n",
    "#     events_df = events_df.sort_values(by=[\"session\", \"user\", \"timestamp\"])\n",
    "#     fig, ax = plt.subplots()\n",
    "#     event_types = events_df['event'].unique()\n",
    "#     event_types.sort()\n",
    "#     colors = mpl.colormaps.get_cmap('tab20')(np.linspace(0, 1, len(event_types)))\n",
    "#     event_color = dict(zip(event_types, colors))\n",
    "#     y_labels = []\n",
    "#     y_ticks = 0\n",
    "#     for session in events_dfs[test]['session'].unique():\n",
    "#         events_df_session = events_df[events_df['session'] == session]\n",
    "#         for user in events_df_session['user'].unique():\n",
    "#             events_df_user = events_df_session[events_df_session['user'] == user]\n",
    "#             y_ticks += 1\n",
    "#             y_pos = [y_ticks] * len(events_df_user)\n",
    "#             ax.scatter(events_df_user['timestamp'], y_pos, c=[event_color[event] for event in events_df_user['event']])\n",
    "#             y_labels.append(f'{user} - {session}')\n",
    "\n",
    "#     # add legend\n",
    "#     legend_labels = []\n",
    "#     for event, color in event_color.items():\n",
    "#         legend_labels.append(plt.Line2D([0], [0], marker='o', color='w', label=event, markerfacecolor=color))\n",
    "#     ax.legend(handles=legend_labels, loc='upper left')\n",
    "#     fig.suptitle(f'{test}')\n",
    "#     # y labels should be user - session\n",
    "#     ax.set_yticks(range(1, y_ticks + 1))\n",
    "#     ax.set_yticklabels(y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test purposes, paint all connection relevant events in a timeline\n",
    "# for test in events_dfs.keys():\n",
    "#     events_df = events_dfs[test]\n",
    "#     events_df = events_df.sort_values(by=[\"session\", \"user\", \"timestamp\"])\n",
    "#     # events_df = events_df[\n",
    "#     #     (events_df['event'] == 'connectionStart') |\n",
    "#     #     # (events_df['event'] == 'signalConnected') |\n",
    "#     #     # (events_df['event'] == 'connectedPublisher') |\n",
    "#     #     ((events_df['event'] == 'streamCreated') & (events_df['connection'] == 'local')) |\n",
    "#     #     ((events_df['event'] == 'streamDestroyed') & (events_df['connection'] == 'local')) |\n",
    "#     #     (events_df['event'] == 'connectionEnd')\n",
    "#     # ]\n",
    "#     events_df = events_df[\n",
    "#         ~(events_df['event'] == 'publisherStartSpeaking-local') & ~(events_df['event'] == 'publisherStopSpeaking-local') &\n",
    "#         ~(events_df['event'] == 'accessAllowed-local')\n",
    "#     ]\n",
    "#     fig, ax = plt.subplots()\n",
    "#     event_types = events_df['event'].unique()\n",
    "#     event_types.sort()\n",
    "#     colors = mpl.colormaps.get_cmap('tab20')(np.linspace(0, 1, len(event_types)))\n",
    "#     event_color = dict(zip(event_types, colors))\n",
    "#     y_labels = []\n",
    "#     y_ticks = 0\n",
    "#     for session in events_df['session'].unique():\n",
    "#         events_df_session = events_df[events_df['session'] == session]\n",
    "#         for user in events_df_session['user'].unique():\n",
    "#             events_df_user = events_df_session[events_df_session['user'] == user]\n",
    "#             y_ticks += 1\n",
    "#             y_pos = [y_ticks] * len(events_df_user)\n",
    "#             ax.scatter(events_df_user['timestamp'], y_pos, c=[event_color[event] for event in events_df_user['event']])\n",
    "#             y_labels.append(f'{user} - {session}')\n",
    "\n",
    "\n",
    "#     # add legend\n",
    "#     legend_labels = []\n",
    "#     for event, color in event_color.items():\n",
    "#         legend_labels.append(plt.Line2D([0], [0], marker='o', color='w', label=event, markerfacecolor=color))\n",
    "#     ax.legend(handles=legend_labels, loc='upper left')\n",
    "#     fig.suptitle(f'{test}')\n",
    "#     # y labels should be user - session\n",
    "#     ax.set_yticks(range(1, y_ticks + 1))\n",
    "#     ax.set_yticklabels(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CPU data and save it in DataFrames\n",
    "df_cpu_list = []\n",
    "df_mem_list = []\n",
    "for index in index_list_names:\n",
    "    df_cpu = pd.read_csv(f\"dfs_final/{index}-medianode.csv\")\n",
    "    df_mem = pd.read_csv(f\"dfs_final/{index}-medianode.csv\")\n",
    "    df_cpu = df_cpu.drop(columns=[\"memory\"]).dropna()\n",
    "    df_mem = df_mem.drop(columns=[\"cpu\"]).dropna()\n",
    "    df_cpu[\"@timestamp\"] = pd.to_datetime(df_cpu[\"@timestamp\"], format=\"ISO8601\")\n",
    "    df_mem[\"@timestamp\"] = pd.to_datetime(df_mem[\"@timestamp\"], format=\"ISO8601\")\n",
    "    df_cpu_list.append(df_cpu)\n",
    "    df_mem_list.append(df_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all QoE data and save it processed in DataFrames\n",
    "index_qoe_df_list = []\n",
    "for index in index_list_names:\n",
    "    index_qoe_df = pd.DataFrame()\n",
    "    for entry in os.scandir(f\"qoe/{index}\"):\n",
    "        if entry.is_file():\n",
    "            with open(f\"qoe/{index}/{entry.name}\", 'r') as f:\n",
    "                qoe = orjson.loads(f.read())\n",
    "            qoe_df = pd.DataFrame.from_dict(qoe)\n",
    "            if not qoe_df.empty:\n",
    "                qoe_df[\"vmaf\"] = qoe_df[\"vmaf\"] * 5\n",
    "                qoe_df[\"visqol\"] = qoe_df[\"visqol\"] * 5\n",
    "                info = entry.name.split(\"_\")[0].split(\"-\")\n",
    "                session = info[1]\n",
    "                userRecording = info[2]\n",
    "                userBeingRecorded = info[3]\n",
    "                qoe_df[\"userRecording\"] = userRecording\n",
    "                qoe_df[\"userBeingRecorded\"] = userBeingRecorded\n",
    "                qoe_df[\"session\"] = session\n",
    "                if index_qoe_df.empty:\n",
    "                    index_qoe_df = qoe_df\n",
    "                else:\n",
    "                    index_qoe_df = pd.concat([index_qoe_df, qoe_df])\n",
    "    index_qoe_df_list.append(index_qoe_df)\n",
    "\n",
    "\n",
    "seconds_per_fragment = 17\n",
    "seconds_per_padding = 2\n",
    "seconds_per_video = seconds_per_fragment - seconds_per_padding\n",
    "def set_qoe_times(df, user_1_start_connection_time, user_2_start_connection_time):\n",
    "    cut_index_0_time = 5 # recording stats 5 seconds after connection is successful\n",
    "    max_time = max(user_1_start_connection_time, user_2_start_connection_time)\n",
    "    time_fn = lambda cut_index: max_time + timedelta(seconds=(cut_index * seconds_per_fragment) + cut_index_0_time).total_seconds()\n",
    "    df.loc[:, \"timestamp\"] = df[\"cut_index\"].apply(time_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read batches data\n",
    "with open(f\"dfs_final/batches.json\", 'r') as f:\n",
    "    batches = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process events to get when users' connections started, when they connected to the session and when the connection failed, and how long it lasted\n",
    "user_events_obj = {}\n",
    "\n",
    "def get_connection_times(events):\n",
    "    connection_times_recorded = []\n",
    "    recorded_media_events = events[\n",
    "        (events[\"event\"] != \"connectionStart\") &\n",
    "        (events[\"event\"] != \"connectionFail\")\n",
    "    ]\n",
    "    for m, row in recorded_media_events.iterrows():\n",
    "        timestamp = row[\"timestamp\"]\n",
    "        con_time = (timestamp - start_time).total_seconds()\n",
    "        connection_times_recorded.append(con_time)\n",
    "    return connection_times_recorded\n",
    "\n",
    "\n",
    "def get_failure_times(events):\n",
    "    connection_times_recorded = []\n",
    "    recorded_media_events = events[\n",
    "        events[\"event\"] == \"connectionFail\"\n",
    "    ]\n",
    "    for m, row in recorded_media_events.iterrows():\n",
    "        timestamp = row[\"timestamp\"]\n",
    "        con_time = (timestamp - start_time).total_seconds()\n",
    "        connection_times_recorded.append(con_time)\n",
    "    return connection_times_recorded\n",
    "\n",
    "df_cpu_filtered_list = []\n",
    "df_mem_filtered_list = []\n",
    "for i, index_list_name in enumerate(index_list_names):\n",
    "    #stats_df = get_index_data(index_list_name, \"stats\")\n",
    "    events_df = events_dfs[index_list_name]\n",
    "    events_df = events_df.sort_values(by=[\"session\", \"user\", \"timestamp\"])\n",
    "    start_time = events_df[\"timestamp\"].min()\n",
    "    end_time = events_df[\"timestamp\"].max()\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    cpu = df_cpu_list[i]\n",
    "    mem = df_mem_list[i]\n",
    "    cpu_filtered = cpu[(cpu[\"@timestamp\"] >= start_time) & (cpu[\"@timestamp\"] <= end_time)]\n",
    "    cpu_filtered.loc[:, \"@timestamp\"] = cpu_filtered[\"@timestamp\"].apply(lambda x: (x - start_time).total_seconds())\n",
    "    cpu_filtered = cpu_filtered.sort_values(by=\"@timestamp\")\n",
    "    mem_filtered = mem[(mem[\"@timestamp\"] >= start_time) & (mem[\"@timestamp\"] <= end_time)]\n",
    "    mem_filtered.loc[:, \"@timestamp\"] = mem_filtered[\"@timestamp\"].apply(lambda x: (x - start_time).total_seconds())\n",
    "    mem_filtered = mem_filtered.sort_values(by=\"@timestamp\")\n",
    "    df_cpu_filtered_list.append(cpu_filtered)\n",
    "    df_mem_filtered_list.append(mem_filtered)\n",
    "    index_qoe_df = index_qoe_df_list[i]\n",
    "    sessions = events_df[\"session\"].unique()\n",
    "    user_events_obj[index_list_name] = {}\n",
    "    prev_row_event = \"\"\n",
    "    for session in sessions:\n",
    "        users = events_df[events_df[\"session\"] == session][\"user\"].unique()\n",
    "        for user in users:\n",
    "            full_user = f\"{user}-{session}\"\n",
    "            user_events = events_df[\n",
    "                (events_df[\"session\"] == session) & (events_df[\"user\"] == user)\n",
    "            ]\n",
    "            connection_starts = user_events[user_events[\"event\"] == \"connectionStart\"]\n",
    "            failures_user = failures[index_list_name][\n",
    "                (failures[index_list_name][\"full_user\"] == full_user) &\n",
    "                (failures[index_list_name][\"timestamp\"] >= start_time) &\n",
    "                (failures[index_list_name][\"timestamp\"] <= end_time)\n",
    "            ].copy()\n",
    "            failures_user[\"event\"] = \"connectionFail\"\n",
    "            failures_user[\"recoverable\"] = False\n",
    "            if \"kurento\" in index_list_name or \"mediasoup\" in index_list_name:\n",
    "                if is_publisher(index_list_name, user):\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"] == \"streamCreated-local\"\n",
    "                    ]\n",
    "                    stream_playing_locals = user_events[\n",
    "                        user_events[\"event\"] == \"streamPlaying-local\"\n",
    "                    ]\n",
    "                else:\n",
    "                    # we choose the first stream created as confirmation of media connection if the user is subscriber\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"].str.startswith(\"streamCreated\")\n",
    "                    ]\n",
    "                    stream_playing_locals = user_events[\n",
    "                        user_events[\"event\"].str.startswith(\"streamPlaying\")\n",
    "                    ]\n",
    "                # combine connection starts and stream created locals and failures_user\n",
    "                combined = pd.concat(\n",
    "                    [\n",
    "                        connection_starts,\n",
    "                        stream_created_locals,\n",
    "                        stream_playing_locals,\n",
    "                        failures_user,\n",
    "                    ]\n",
    "                )\n",
    "                # if there is more than one stream created and a stream playing, we only keep the first one (marks connection success)\n",
    "                combined = combined.sort_values(by=\"timestamp\")\n",
    "                prev_row_event = \"\"\n",
    "                for r, row in combined.iterrows():\n",
    "                    is_prev_row_connected = prev_row_event.startswith(\"streamCreated\") or prev_row_event.startswith(\"streamPlaying\")\n",
    "                    is_current_row_connected = row[\"event\"].startswith(\"streamCreated\") or row[\"event\"].startswith(\"streamPlaying\")\n",
    "                    if is_prev_row_connected and is_current_row_connected:\n",
    "                        combined = combined.drop(r)\n",
    "                    prev_row_event = row[\"event\"]\n",
    "            else:\n",
    "                reconnecting_events = user_events[user_events[\"event\"].str.contains(\"Reconnecting\")].copy()\n",
    "                if not reconnecting_events.empty:\n",
    "                    reconnecting_events[\"event\"] = \"connectionFail\"\n",
    "                    reconnecting_events[\"recoverable\"] = True\n",
    "                    failures_user = pd.concat([failures_user, reconnecting_events])\n",
    "                # combine connection starts and stream created locals and failures_user\n",
    "                if is_publisher(index_list_name, user):\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"] == \"LocalTrackPublished-local\"\n",
    "                    ]\n",
    "                    stream_created_locals = stream_created_locals.sort_values(by=\"timestamp\")\n",
    "                    # The second one means the publisher has fully connected\n",
    "                    stream_created_locals = stream_created_locals.iloc[::2, :]\n",
    "                else:\n",
    "                    # we choose the first stream created as confirmation of media connection if the user is subscriber\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"].str.startswith(\"TrackSubscribed\")\n",
    "                    ]\n",
    "                # combine connection starts and stream created locals and failures_user\n",
    "                combined = pd.concat(\n",
    "                    [connection_starts, stream_created_locals, failures_user]\n",
    "                )\n",
    "                combined = combined.sort_values(by=\"timestamp\")\n",
    "                for r, row in combined.iterrows():\n",
    "                    is_prev_row_connected = prev_row_event.startswith(\"TrackSubscribed\")\n",
    "                    is_current_row_connected = row[\"event\"].startswith(\"TrackSubscribed\")\n",
    "                    if is_prev_row_connected and is_current_row_connected:\n",
    "                        combined = combined.drop(r)\n",
    "                    prev_row_event = row[\"event\"]\n",
    "            if index_qoe_df.empty:\n",
    "                index_qoe_df_filtered = pd.DataFrame()\n",
    "            else:\n",
    "                index_qoe_df_filtered = index_qoe_df[\n",
    "                    (index_qoe_df[\"session\"] == \"LoadTestSession\" + str(session)) &\n",
    "                    (index_qoe_df[\"userRecording\"] == \"User\" + str(user))\n",
    "                ].copy()\n",
    "                index_qoe_df_filtered = index_qoe_df_filtered.sort_values(by=\"cut_index\")\n",
    "            user_events_obj[index_list_name][full_user] = {}\n",
    "            user_events_obj[index_list_name][full_user][\"combined\"] = combined\n",
    "            user_events_obj[index_list_name][full_user][\"qoe\"] = index_qoe_df_filtered\n",
    "            user_events_obj[index_list_name][full_user][\"user\"] = user\n",
    "            user_events_obj[index_list_name][full_user][\"session\"] = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the intervals of time when a user is connected\n",
    "def get_connection_time_intervals(user_events, start_time, test_duration):\n",
    "    connection_time_intervals = {}\n",
    "    users = user_events.keys()\n",
    "    for username in users:\n",
    "        group = user_events[username]['combined'].copy()\n",
    "        group[\"timestamp\"] = group[\"timestamp\"].apply(lambda x: (x - start_time).total_seconds())\n",
    "        connection_success_times = group.loc[~((group['event'] == 'connectionFail') | (group['event'] == 'connectionStart'))]['timestamp']\n",
    "        connection_fail_times = group.loc[group['event'] == 'connectionFail']['timestamp']\n",
    "\n",
    "        intervals = []\n",
    "        for success_time in connection_success_times:\n",
    "            fail_time = connection_fail_times[connection_fail_times > success_time].min()\n",
    "            if pd.isnull(fail_time):\n",
    "                intervals.append((success_time, test_duration))\n",
    "            else:\n",
    "                intervals.append((success_time, min(fail_time, test_duration)))\n",
    "\n",
    "        connection_time_intervals[username] = intervals\n",
    "\n",
    "    return connection_time_intervals\n",
    "\n",
    "def intersect_intervals(intervals1, intervals2):\n",
    "    intersection = []\n",
    "    for interval1 in intervals1:\n",
    "        for interval2 in intervals2:\n",
    "            start = max(interval1[0], interval2[0])\n",
    "            end = min(interval1[1], interval2[1])\n",
    "            # there is a wait time of 5 seconds before starting recording\n",
    "            start += 5\n",
    "            if start < end:\n",
    "                intersection.append((start, end))\n",
    "    return intersection\n",
    "\n",
    "intersections = {}\n",
    "connection_time_intervals = {}\n",
    "for i, index_list_name in enumerate(index_list_names):\n",
    "    intersections[index_list_name] = {}\n",
    "    start_time, end_time = start_end_times[index_list_name]\n",
    "    cpu = df_cpu_list[i]\n",
    "    last_cpu_timestamp = cpu.iloc[-1][\"@timestamp\"] + timedelta(seconds=10) # media server can be considered crashed if there aren't any more CPU data points 10 seconds past the last one\n",
    "    end_time = min(end_time, last_cpu_timestamp)\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    connection_time_intervals_index = get_connection_time_intervals(user_events_obj[index_list_name], start_time, test_duration)\n",
    "    connection_time_intervals[index_list_name] = connection_time_intervals_index\n",
    "    usernames = list(connection_time_intervals_index.keys())\n",
    "    for i in range(len(usernames)):\n",
    "        for j in range(i + 1, len(usernames)):\n",
    "            _, session1 = usernames[i].split(\"-\")\n",
    "            _, session2 = usernames[j].split(\"-\")\n",
    "            if session1 == session2:\n",
    "                user1_intervals = connection_time_intervals_index[usernames[i]]\n",
    "                user2_intervals = connection_time_intervals_index[usernames[j]]\n",
    "                intersection = intersect_intervals(user1_intervals, user2_intervals)\n",
    "                if not usernames[i] in intersections[index_list_name]:\n",
    "                    intersections[index_list_name][usernames[i]] = {}\n",
    "                intersections[index_list_name][usernames[i]][usernames[j]] = intersection\n",
    "                if not usernames[j] in intersections[index_list_name]:\n",
    "                    intersections[index_list_name][usernames[j]] = {}\n",
    "                intersections[index_list_name][usernames[j]][usernames[i]] = intersection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process QoE data to get the start and end of each fragment of time a QoE value is recorded\n",
    "def set_frag_start_end(index_qoe_df, bi_intersections, fragment_duration, indexes):\n",
    "    if len(bi_intersections) > 0:\n",
    "        start = bi_intersections[0][0]\n",
    "        end = bi_intersections[0][1]\n",
    "        set_frag_start_end_aux(index_qoe_df, bi_intersections, fragment_duration, indexes, start, end, 0, 0)\n",
    "\n",
    "def set_frag_start_end_aux(index_qoe_df, bi_intersections, fragment_duration, indexes, start, end, inters_j, index_i):\n",
    "    if index_i < len(indexes):\n",
    "        index = indexes[index_i]\n",
    "        frag_end_time = start + fragment_duration\n",
    "        if frag_end_time < end or math.isclose(frag_end_time, end, rel_tol=1e-9):\n",
    "            index_qoe_df.loc[index, \"start\"] = start\n",
    "            index_qoe_df.loc[index, \"end\"] = frag_end_time\n",
    "            index_qoe_df.loc[index, \"fragment_duration\"] = fragment_duration\n",
    "            start = frag_end_time\n",
    "            set_frag_start_end_aux(index_qoe_df, bi_intersections, fragment_duration, indexes, start, end, inters_j, index_i + 1)\n",
    "        else:\n",
    "            j = inters_j + 1\n",
    "            if j < len(bi_intersections):\n",
    "                start = bi_intersections[j][0]\n",
    "                end = bi_intersections[j][1]\n",
    "                set_frag_start_end_aux(index_qoe_df, bi_intersections, fragment_duration, indexes, start, end, j, index_i)\n",
    "            else:\n",
    "                index_qoe_df.loc[index, \"start\"] = start\n",
    "                index_qoe_df.loc[index, \"end\"] = end\n",
    "                index_qoe_df.loc[index, \"fragment_duration\"] = end - start\n",
    "                set_frag_start_end_aux(index_qoe_df, bi_intersections, fragment_duration, indexes, start, end, inters_j, index_i + 1)\n",
    "\n",
    "for idx, index_list_name in enumerate(index_list_names):\n",
    "    idx_intersections = intersections[index_list_name]\n",
    "    index_qoe_df = index_qoe_df_list[idx]\n",
    "    if not index_qoe_df.empty:\n",
    "        index_qoe_df[\"session\"] = index_qoe_df[\"session\"].str.replace(\"LoadTestSession\", \"\")\n",
    "        index_qoe_df[\"userRecording\"] = index_qoe_df[\"userRecording\"].str.replace(\"User\", \"\")\n",
    "        index_qoe_df[\"userBeingRecorded\"] = index_qoe_df[\"userBeingRecorded\"].str.replace(\"User\", \"\")\n",
    "        index_qoe_df[\"full_user_recording\"] = index_qoe_df[\"userRecording\"] + \"-\" + index_qoe_df[\"session\"]\n",
    "        index_qoe_df[\"full_user_being_recorded\"] = index_qoe_df[\"userBeingRecorded\"] + \"-\" + index_qoe_df[\"session\"]\n",
    "        index_qoe_df = index_qoe_df.drop(columns=[\"userRecording\", \"userBeingRecorded\", \"session\"]).reset_index(drop=True)\n",
    "        # for user recording\n",
    "        for full_user in idx_intersections.keys():\n",
    "            qoe = index_qoe_df[index_qoe_df[\"full_user_recording\"] == full_user]\n",
    "            intersections_user = idx_intersections[full_user]\n",
    "            # for user being recorded\n",
    "            for full_user_being_recorded in intersections_user.keys():\n",
    "                qoe_being_recorded = qoe.loc[index_qoe_df[\"full_user_being_recorded\"] == full_user_being_recorded]\n",
    "                bi_intersections = intersections_user[full_user_being_recorded]\n",
    "                con_duration = 0\n",
    "                for intersection in bi_intersections:\n",
    "                    con_duration += intersection[1] - intersection[0]\n",
    "                max_cut_index = qoe_being_recorded[\"cut_index\"].max() + 1\n",
    "                fragment_duration = con_duration / max_cut_index\n",
    "                set_frag_start_end(index_qoe_df, bi_intersections, fragment_duration, qoe_being_recorded.index)\n",
    "        index_qoe_df_list[idx] = index_qoe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for getting the average QoE values for a specific user and in which times\n",
    "def avg_qoe_intervals(index_qoe_df, metric, user):\n",
    "    if index_qoe_df.empty:\n",
    "        return []\n",
    "    qoe = index_qoe_df[index_qoe_df[\"full_user_recording\"] == user]\n",
    "    qoe = qoe.sort_values(by=[\"start\"])\n",
    "    qoe[\"start\"] = qoe[\"start\"].round(decimals=5)\n",
    "    qoe[\"end\"] = qoe[\"end\"].round(decimals=5)\n",
    "    times = set()\n",
    "    for index, row in qoe.iterrows():\n",
    "        times.add(row[\"start\"])\n",
    "        times.add(row[\"end\"])\n",
    "    times = sorted(times)\n",
    "    sub_intervals = []\n",
    "    for i in range(len(times) - 1):\n",
    "        start = times[i]\n",
    "        end = times[i + 1]\n",
    "        overlapping = []\n",
    "        for r, row in qoe.iterrows():\n",
    "            if (row[\"start\"] < end and row[\"end\"] > start):\n",
    "                overlapping.append(row[metric])\n",
    "        if len(overlapping) > 0:\n",
    "            avg_qoe = sum(overlapping) / len(overlapping)\n",
    "            sub_intervals.append(([start, end], avg_qoe))\n",
    "    return sub_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint the connection progression of all users in a test and save the relevant data to DataFrames\n",
    "plt.rcParams[\"figure.figsize\"] = [40, 35]\n",
    "\n",
    "colors_list = [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"]\n",
    "success_cmap = mcolors.LinearSegmentedColormap.from_list(\"success\", colors_list)\n",
    "normalize = mcolors.Normalize(vmin=0, vmax=5)\n",
    "\n",
    "connection_stats_global = pd.DataFrame()\n",
    "\n",
    "def get_con_type(next_color):\n",
    "    if next_color == \"magenta\":\n",
    "        return \"connectionStart\"\n",
    "    elif next_color == \"indigo\":\n",
    "        return \"connectionSuccess\"\n",
    "    elif next_color == \"red\":\n",
    "        return \"connectionFail\"\n",
    "    else:\n",
    "        return \"QOE\"\n",
    "\n",
    "def calc_stats(cpu_filtered, prev_time, current_time, prev_attempt, n_batch, user, session, full_user, next_color, connection_stats, qoe=None, qoe_metric=None):\n",
    "    total_duration = current_time - prev_time\n",
    "    cpu_subset = cpu_filtered[(cpu_filtered['@timestamp'] >= prev_time) & (cpu_filtered['@timestamp'] <= current_time)]\n",
    "    con_type = get_con_type(next_color)\n",
    "    stats = {\n",
    "        \"type\": con_type,\n",
    "        \"start_time\": prev_time,\n",
    "        \"end_time\": current_time,\n",
    "        \"total_duration\": total_duration,\n",
    "        \"attempt\": prev_attempt,\n",
    "        \"batch\": n_batch,\n",
    "        \"user\": user,\n",
    "        \"session\": session,\n",
    "        \"full_user\": full_user\n",
    "    }\n",
    "    if con_type == \"QOE\":\n",
    "        stats[\"qoe\"] = qoe\n",
    "        stats[\"qoe_metric\"] = qoe_metric\n",
    "    # if there is no cpu data, add the next cpu data available\n",
    "    if cpu_subset.empty:\n",
    "        # seek closest cpu data to start_time or end_time\n",
    "        cpu_prev_df = cpu_filtered[cpu_filtered['@timestamp'] < prev_time]\n",
    "        cpu_next_df = cpu_filtered[cpu_filtered['@timestamp'] > current_time]\n",
    "        # if one of the dfs is empty, we take the other\n",
    "        if cpu_prev_df.empty:\n",
    "            cpu_next = cpu_next_df.iloc[0]\n",
    "            cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_next['@timestamp']]\n",
    "        elif cpu_next_df.empty:\n",
    "            cpu_prev = cpu_prev_df.iloc[-1]\n",
    "            cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_prev['@timestamp']]\n",
    "        else:\n",
    "            cpu_next = cpu_next_df.iloc[0]\n",
    "            cpu_prev = cpu_prev_df.iloc[-1]\n",
    "            cpu_prev_diff = prev_time - cpu_prev['@timestamp']\n",
    "            cpu_next_diff = cpu_next['@timestamp'] - current_time\n",
    "            if cpu_prev_diff < cpu_next_diff:\n",
    "                cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_prev['@timestamp']]\n",
    "            else:\n",
    "                cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_next['@timestamp']]\n",
    "    stats[\"cpu_mean\"] = cpu_subset['cpu'].mean()\n",
    "    stats[\"cpu_start\"] = cpu_subset['cpu'].iloc[0]\n",
    "    stats[\"cpu_end\"] = cpu_subset['cpu'].iloc[-1]\n",
    "    stats[\"cpu_median\"] = cpu_subset['cpu'].median()\n",
    "    stats[\"cpu_max\"] = cpu_subset['cpu'].max()\n",
    "    stats[\"cpu_min\"] = cpu_subset['cpu'].min()\n",
    "    connection_stats.append(stats)\n",
    "    return connection_stats\n",
    "\n",
    "def paint_bar(ax, prev_time, current_time, y_ticks, prev_state, next_color):\n",
    "    total_duration = current_time - prev_time\n",
    "    yrange = (y_ticks - 0.5, 0.8)\n",
    "    if prev_state != \"connectionSuccess\":\n",
    "        segment = [(prev_time, total_duration)]\n",
    "        ax.broken_barh(segment, yrange, facecolors=next_color)\n",
    "\n",
    "def paint_success_bars(ax, segments, y_ticks):\n",
    "    yrange = (y_ticks - 0.5, 0.8)\n",
    "    batched_segments = []\n",
    "    batched_colors = []\n",
    "\n",
    "    for prev_time, current_time, qoe in segments:\n",
    "        total_duration = current_time - prev_time\n",
    "        segment = (prev_time, total_duration)\n",
    "        color = success_cmap(normalize(qoe))\n",
    "\n",
    "        batched_segments.append(segment)\n",
    "        batched_colors.append(color)\n",
    "\n",
    "    ax.broken_barh(batched_segments, yrange, facecolors=batched_colors)\n",
    "    return batched_colors\n",
    "\n",
    "def draw_cons_plot(i, index_list_name, metric):\n",
    "    global connection_stats_global\n",
    "    fig, ax = plt.subplots()\n",
    "    y_labels = []\n",
    "    y_ticks = 0\n",
    "    user_count = 0\n",
    "    connection_stats = []\n",
    "    batch_info = batches[index_list_name]\n",
    "    cpu_filtered = df_cpu_filtered_list[i]\n",
    "    start_time, end_time = start_end_times[index_list_name]\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    index_qoe_df = index_qoe_df_list[i]\n",
    "    for full_user, full_user_obj in user_events_obj[index_list_name].items():\n",
    "        user_count += 1\n",
    "        if user_count <= batch_info[\"startingParticipants\"]:\n",
    "            n_batch = 0\n",
    "        else:\n",
    "            n_batch = math.floor((user_count - batch_info[\"startingParticipants\"] - 1) / batch_info[\"batchSize\"]) + 1\n",
    "        user_split = full_user.split(\"-\")\n",
    "        user = user_split[0]\n",
    "        session = user_split[1]\n",
    "        y_labels.append(full_user)\n",
    "        y_ticks += 1\n",
    "\n",
    "        next_color = \"white\"\n",
    "        prev_time = None\n",
    "        prev_state = None\n",
    "        prev_attempt = 0\n",
    "        combined = full_user_obj[\"combined\"]\n",
    "        full_con_segments = connection_time_intervals[index_list_name][full_user]\n",
    "        con_segments = avg_qoe_intervals(index_qoe_df, metric, full_user)\n",
    "        # connection success\n",
    "        for full_segment in full_con_segments:\n",
    "            paint_bar(ax, full_segment[0], full_segment[1], y_ticks, \"\", \"indigo\")\n",
    "            calc_stats(cpu_filtered, full_segment[0], full_segment[1], prev_attempt, n_batch, user, session, full_user, \"indigo\", connection_stats)\n",
    "        segments_to_plot = [(segment[0][0], segment[0][1], segment[1]) for segment in con_segments]\n",
    "        colors = paint_success_bars(ax, segments_to_plot, y_ticks)\n",
    "        for segment, color in zip(con_segments, colors):\n",
    "            calc_stats(cpu_filtered, segment[0][0], segment[0][1], prev_attempt, n_batch, user, session, full_user, color, connection_stats, segment[1], metric)\n",
    "        # other connection events\n",
    "        for r, row in combined.iterrows():\n",
    "            current_time = (row[\"timestamp\"] - start_time).total_seconds()\n",
    "            if prev_time is not None:\n",
    "                paint_bar(ax, prev_time, current_time, y_ticks, prev_state, next_color)\n",
    "                calc_stats(cpu_filtered, prev_time, current_time, prev_attempt, n_batch, user, session, full_user, next_color, connection_stats)\n",
    "            connectionSuccessful = (\n",
    "                row[\"event\"].startswith(\"streamCreated\")\n",
    "                or row[\"event\"].startswith(\"streamPlaying\")\n",
    "                or row[\"event\"].startswith(\"LocalTrackPublished\")\n",
    "                or row[\"event\"].startswith(\"TrackSubscribed\")\n",
    "            )\n",
    "            if row[\"event\"] == \"connectionStart\":\n",
    "                next_color = \"magenta\"\n",
    "                prev_state = \"connectionStart\"\n",
    "            elif connectionSuccessful:\n",
    "                next_color = \"indigo\"\n",
    "                prev_state = \"connectionSuccess\"\n",
    "            else:\n",
    "                next_color = \"red\"\n",
    "                prev_attempt = row[\"attempts\"]\n",
    "                prev_state = \"connectionFail\"\n",
    "            prev_time = current_time\n",
    "        if prev_time is not None:\n",
    "            paint_bar(ax, prev_time, test_duration, y_ticks, prev_state, next_color)\n",
    "            calc_stats(cpu_filtered, prev_time, test_duration, prev_attempt, n_batch, user, session, full_user, next_color, connection_stats)\n",
    "    connection_stats_df = pd.DataFrame(connection_stats)\n",
    "    # add all rows to connection_stats_global\n",
    "    if connection_stats_global.empty:\n",
    "        connection_stats_global = connection_stats_df\n",
    "        connection_stats_global[\"index\"] = index_list_name\n",
    "    else:\n",
    "        connection_stats_df[\"index\"] = index_list_name\n",
    "        connection_stats_global = pd.concat([connection_stats_global, connection_stats_df])\n",
    "\n",
    "    ax.set_yticks(range(1, y_ticks + 1))\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_title(f\"Connection Progression ({metric}) - {index_list_full_names[i]}\", pad=10)\n",
    "    # y axis label\n",
    "    ax.set_ylabel(\"User - Session\")\n",
    "    if user_count >= 150:\n",
    "        # remove y labels\n",
    "        ax.set_yticklabels([])\n",
    "    # x axis label\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xlim(-5, test_duration + 5)\n",
    "    # grid\n",
    "    ax.grid(True)\n",
    "    # x axis ticks every 10 seconds\n",
    "    if test_duration > 1000:\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(50))\n",
    "    else:\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    # cpu in new axis\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel(\"CPU (%)\")\n",
    "    # ax2 y axis should go from 0 to 1\n",
    "    ax2.set_ylim(0, 1.01)\n",
    "    ax2.set_yticks(np.arange(0, 1.01, 0.05))\n",
    "    ax2.plot(cpu_filtered[\"@timestamp\"], cpu_filtered[\"cpu\"], color=\"black\", marker=\"o\", linewidth=4, markersize=10)\n",
    "    # create legend\n",
    "    legend_labels = [\n",
    "        plt.Line2D([0], [0], color=\"magenta\", lw=4, label=\"Connection Start\"),\n",
    "        plt.Line2D([0], [0], color=\"indigo\", lw=4, label=\"Stream Created/Playing, No Video\"),\n",
    "        plt.Line2D([0], [0], color=\"darkblue\", lw=4, label=\"Excellent QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"green\", lw=4, label=\"Good QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"yellow\", lw=4, label=\"Fair QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"orange\", lw=4, label=\"Poor QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"black\", lw=4, label=\"Bad QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"red\", lw=4, label=\"Failed Connection\"),\n",
    "        plt.Line2D([0], [0], color=\"black\", lw=4, label=\"CPU\", marker=\"o\", markersize=10)\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_labels, loc=\"upper left\")\n",
    "    fig.savefig(f\"plots/qoe/connection_progression_{index_list_name[21:]}_{metric}.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint all tests\n",
    "for i, index_list_name in enumerate(index_list_names):\n",
    "    draw_cons_plot(i, index_list_name, \"vmaf\")\n",
    "    draw_cons_plot(i, index_list_name, \"visqol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save connection relevant data to file for later reuse\n",
    "os.makedirs(\"dfs_aux\", exist_ok=True)\n",
    "connection_stats_global.to_csv(\"dfs_aux/connection_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative, read from file (avoids running long process)\n",
    "connection_stats_global = pd.read_csv(\"dfs_aux/connection_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many publishers, subscribers, streams in and streams out there are at each time\n",
    "def add_to(array, number, user, session, timestamp):\n",
    "    array.append({\n",
    "        \"number\": number,\n",
    "        \"user\": user,\n",
    "        \"session\": session,\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "full_publishers_progression = {}\n",
    "full_subscribers_progression = {}\n",
    "full_streams_in_progression = {}\n",
    "full_streams_out_progression = {}\n",
    "def process_events_df(index_list_name, connection_stats):\n",
    "    sessions = connection_stats['session'].unique()\n",
    "    publishers_progression = []\n",
    "    subscribers_progression = []\n",
    "    streams_in_progression = []\n",
    "    streams_out_progression = []\n",
    "\n",
    "    previous_data = {'publishers': 0, 'subscribers': 0, 'streams_in': 0, 'streams_out': 0}\n",
    "    for session in sessions:\n",
    "        previous_data[session] = {'publishers': 0, 'subscribers': 0}\n",
    "\n",
    "    for i, row in connection_stats.iterrows():\n",
    "        session = row['session']\n",
    "        user = row['user']\n",
    "        type = row['type']\n",
    "        full_user = row['full_user']\n",
    "        timestamp = row['start_time']\n",
    "        connected_users = set()\n",
    "\n",
    "        if type == \"connectionSuccess\":\n",
    "            if \"0s\" in index_list_name and int(user) > 3:\n",
    "                subscribers = previous_data['subscribers'] + 1\n",
    "                publishers_in_session = previous_data[session]['publishers']\n",
    "                subscribers_in_session = previous_data[session]['subscribers'] + 1\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['subscribers'] = subscribers\n",
    "                previous_data[session]['subscribers'] = subscribers_in_session\n",
    "            else:\n",
    "                publishers = previous_data['publishers'] + 1\n",
    "                streams_in = previous_data['streams_in'] + 2\n",
    "                publishers_in_session = previous_data[session]['publishers'] + 1\n",
    "                subscribers_in_session = previous_data[session]['subscribers']\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['publishers'] = publishers\n",
    "                previous_data['streams_in'] = streams_in\n",
    "                previous_data[session]['publishers'] = publishers_in_session\n",
    "\n",
    "            previous_data['streams_out'] = streams_out\n",
    "\n",
    "            add_to(publishers_progression, previous_data['publishers'], user, session, timestamp)\n",
    "            add_to(subscribers_progression, previous_data['subscribers'], user, session, timestamp)\n",
    "            add_to(streams_in_progression, previous_data['streams_in'], user, session, timestamp)\n",
    "            add_to(streams_out_progression, previous_data['streams_out'], user, session, timestamp)\n",
    "\n",
    "            connected_users.add(full_user)\n",
    "\n",
    "        elif type == \"connectionFail\" and full_user in connected_users:\n",
    "            if \"0s\" in index_list_name and int(user) > 3:\n",
    "                subscribers = previous_data['subscribers'] - 1\n",
    "                publishers_in_session = previous_data[session]['publishers']\n",
    "                subscribers_in_session = previous_data[session]['subscribers'] - 1\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['subscribers'] = subscribers\n",
    "                previous_data[session]['subscribers'] = subscribers_in_session\n",
    "            else:\n",
    "                publishers = previous_data['publishers'] - 1\n",
    "                streams_in = previous_data['streams_in'] - 2\n",
    "                publishers_in_session = previous_data[session]['publishers'] - 1\n",
    "                subscribers_in_session = previous_data[session]['subscribers']\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['publishers'] = publishers\n",
    "                previous_data['streams_in'] = streams_in\n",
    "                previous_data[session]['publishers'] = publishers_in_session\n",
    "\n",
    "            previous_data['streams_out'] = streams_out\n",
    "\n",
    "            add_to(publishers_progression, previous_data['publishers'], user, session, timestamp)\n",
    "            add_to(subscribers_progression, previous_data['subscribers'], user, session, timestamp)\n",
    "            add_to(streams_in_progression, previous_data['streams_in'], user, session, timestamp)\n",
    "            add_to(streams_out_progression, previous_data['streams_out'], user, session, timestamp)\n",
    "\n",
    "            connected_users.remove(full_user)\n",
    "\n",
    "    publishers_progression = pd.DataFrame(publishers_progression)\n",
    "    subscribers_progression = pd.DataFrame(subscribers_progression)\n",
    "    streams_in_progression = pd.DataFrame(streams_in_progression)\n",
    "    streams_out_progression = pd.DataFrame(streams_out_progression)\n",
    "\n",
    "    return (index_list_name, publishers_progression, subscribers_progression, streams_in_progression, streams_out_progression)\n",
    "\n",
    "# Use parallel processing to handle events dataframes\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = [executor.submit(process_events_df, index_list_name, connection_stats_global[connection_stats_global[\"index\"] == index_list_name]) for index_list_name in index_list_names]\n",
    "    for future in as_completed(futures):\n",
    "        (index_list_name, publishers_progression, subscribers_progression, streams_in_progression, streams_out_progression) = future.result()\n",
    "        full_publishers_progression[index_list_name] = publishers_progression\n",
    "        full_subscribers_progression[index_list_name] = subscribers_progression\n",
    "        full_streams_in_progression[index_list_name] = streams_in_progression\n",
    "        full_streams_out_progression[index_list_name] = streams_out_progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate certain values when in a CPU threshold interval\n",
    "def calculate_connection_data(connection_type, connection_stats, metric_to_check, label, qoe_metric=None):\n",
    "    connection_stats = connection_stats[connection_stats[\"type\"] == connection_type].drop(columns=[\"type\", \"index\", \"user\", \"session\"])\n",
    "    connection_stats = connection_stats.sort_values(by=\"cpu_median\").reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "    cols_to_keep = [\"full_user\", \"attempt\", \"batch\", \"total_duration\", \"cpu_median\", \"cpu_mean\", \"cpu_max\", \"cpu_min\"]\n",
    "    if connection_type == \"QOE\":\n",
    "        cols_to_keep += [\"qoe\", \"qoe_metric\"]\n",
    "    connection_stats = connection_stats[cols_to_keep]\n",
    "    \n",
    "    if connection_type == \"QOE\":\n",
    "        connection_stats = connection_stats[connection_stats[\"qoe_metric\"] == qoe_metric]\n",
    "\n",
    "    thresholds = [\n",
    "        { \"threshold\": \"Full test\", \"condition\": connection_stats[\"total_duration\"] == connection_stats[\"total_duration\"] },\n",
    "        {\"threshold\": \"Median CPU < 0.8\", \"condition\": connection_stats[\"cpu_median\"] < 0.8},\n",
    "        {\"threshold\": \"Median CPU < 0.9\", \"condition\": connection_stats[\"cpu_median\"] < 0.9},\n",
    "        {\"threshold\": \"Median CPU < 0.95\", \"condition\": connection_stats[\"cpu_median\"] < 0.95},\n",
    "        {\"threshold\": \"0.8 <= Median CPU < 0.9\", \"condition\": (connection_stats[\"cpu_median\"] >= 0.8) & (connection_stats[\"cpu_median\"] < 0.9)},\n",
    "        {\"threshold\": \"0.9 <= Median CPU <= 1\", \"condition\": (connection_stats[\"cpu_median\"] >= 0.9)},\n",
    "        {\"threshold\": \"0.9 < Median CPU <= 0.95\", \"condition\": (connection_stats[\"cpu_median\"] >= 0.9) & (connection_stats[\"cpu_median\"] < 0.95)},\n",
    "        {\"threshold\": \"0.95 < Median CPU\", \"condition\": connection_stats[\"cpu_median\"] >= 0.95},\n",
    "    ]\n",
    "    \n",
    "    cpu_start_durations = []\n",
    "    filtered_connection_stats = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        filtered_stats = connection_stats[threshold[\"condition\"]]\n",
    "        mean = filtered_stats[metric_to_check].mean()\n",
    "        median = filtered_stats[metric_to_check].median()\n",
    "        min_val = filtered_stats[metric_to_check].min()\n",
    "        max_val = filtered_stats[metric_to_check].max()\n",
    "        std = filtered_stats[metric_to_check].std()\n",
    "        \n",
    "        cpu_start_durations.append({\n",
    "            \"threshold\": threshold[\"threshold\"],\n",
    "            f\"median {label}\": median,\n",
    "            f\"mean {label}\": mean,\n",
    "            f\"min {label}\": min_val,\n",
    "            f\"max {label}\": max_val,\n",
    "            f\"std {label}\": std,\n",
    "        })\n",
    "        \n",
    "        filtered_connection_stats.append(filtered_stats)\n",
    "    \n",
    "    cpu_start_durations_df = pd.DataFrame(cpu_start_durations)\n",
    "\n",
    "    return (\n",
    "        cpu_start_durations_df,\n",
    "        *filtered_connection_stats,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for comparing the certain values averages between different CPU thresholds using Mann-Whitney U test and t-test\n",
    "from scipy import stats\n",
    "\n",
    "def find_outliers_iqr(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "def get_cases(connection_stats, con_type=\"QOE\", label=\"VMAF\", qoe_metric=\"vmaf\"):\n",
    "    if con_type == \"QOE\":\n",
    "        metric_to_check = \"qoe\"\n",
    "    else:\n",
    "        metric_to_check = \"total_duration\"\n",
    "    cpu_start_durations_df, con_0_100, con_0_80, con_0_90, con_0_95, con_80_90, con_90_100, con_90_95, con_95_100 = calculate_connection_data(\n",
    "        con_type, connection_stats, metric_to_check, label, qoe_metric\n",
    "    )\n",
    "    display(cpu_start_durations_df)\n",
    "    cases = [\n",
    "        # {\n",
    "        #     \"X\": \"CPU < 0.8\",\n",
    "        #     \"Y\": \"CPU >= 0.8 and < 0.9\",\n",
    "        #     \"len(X)\": len(con_0_80),\n",
    "        #     \"len(Y)\": len(con_80_90),\n",
    "        #     \"XSeries\": con_0_80[metric_to_check],\n",
    "        #     \"YSeries\": con_80_90[metric_to_check],\n",
    "        # },\n",
    "        # {\n",
    "        #     \"X\": \"CPU >= 0.8 and < 0.9\",\n",
    "        #     \"Y\": \"CPU >= 0.9\",\n",
    "        #     \"len(X)\": len(con_80_90),\n",
    "        #     \"len(Y)\": len(con_90_100),\n",
    "        #     \"XSeries\": con_80_90[metric_to_check],\n",
    "        #     \"YSeries\": con_90_100[metric_to_check],\n",
    "        # },\n",
    "        {\n",
    "            \"X\": \"CPU < 0.9\",\n",
    "            \"Y\": \"CPU >= 0.9\",\n",
    "            \"metric\": label if con_type == \"QOE\" else \"Total duration (s)\",\n",
    "            \"len(X)\": len(con_0_90),\n",
    "            \"len(Y)\": len(con_90_100),\n",
    "            \"XSeries\": con_0_90[metric_to_check],\n",
    "            \"YSeries\": con_90_100[metric_to_check],\n",
    "            \"XSeriesCPU\": con_0_90[\"cpu_median\"],\n",
    "            \"YSeriesCPU\": con_90_100[\"cpu_median\"],\n",
    "        },\n",
    "        {\n",
    "            \"X\": \"CPU < 0.95\",\n",
    "            \"Y\": \"CPU >= 0.95\",\n",
    "            \"metric\": label if con_type == \"QOE\" else \"Total duration (s)\",\n",
    "            \"len(X)\": len(con_0_95),\n",
    "            \"len(Y)\": len(con_95_100),\n",
    "            \"XSeries\": con_0_95[metric_to_check],\n",
    "            \"YSeries\": con_95_100[metric_to_check],\n",
    "            \"XSeriesCPU\": con_0_95[\"cpu_median\"],\n",
    "            \"YSeriesCPU\": con_95_100[\"cpu_median\"],\n",
    "        },\n",
    "    ]\n",
    "    return cases\n",
    "\n",
    "def clean_outliers(serie, outliers):\n",
    "    serie = serie[~serie.isin(outliers)]\n",
    "    return serie\n",
    "\n",
    "def calculate_comparisons(X, Y, XSeries, YSeries, lenX, lenY, prev_80_data=None, prev_90_data=None, prev_95_data=None):\n",
    "    ustat, p = stats.mannwhitneyu(XSeries, YSeries)\n",
    "    tstat, tp = stats.ttest_ind(XSeries, YSeries)\n",
    "    # wstat, wp, zw = stats.wilcoxon(XSeries, YSeries)\n",
    "    shapiroX = stats.shapiro(XSeries)\n",
    "    shapiroY = stats.shapiro(YSeries)\n",
    "    brown_forsythe = stats.levene(XSeries, YSeries, center=\"median\")\n",
    "    fligner = stats.fligner(XSeries, YSeries, center=\"median\")\n",
    "    ks = stats.ks_2samp(XSeries, YSeries)\n",
    "    ad = stats.anderson_ksamp([XSeries, YSeries])\n",
    "    kw = stats.kruskal(XSeries, YSeries)\n",
    "\n",
    "    meanX = XSeries.mean()\n",
    "    meanY = YSeries.mean()\n",
    "    medianX = XSeries.median()\n",
    "    medianY = YSeries.median()\n",
    "    stdX = XSeries.std()\n",
    "    stdY = YSeries.std()\n",
    "    pooled_std = np.sqrt(((lenX - 1) * stdX ** 2 + (lenY - 1) * stdY ** 2) / (lenX + lenY - 2))\n",
    "    cohen_d = (meanX - meanY) / pooled_std\n",
    "    rankBiserialCorrelation = (2 * ustat) / (lenX * lenY) - 1\n",
    "    mean_diff = meanX - meanY\n",
    "    median_diff = medianX - medianY\n",
    "    std_diff = stdX - stdY\n",
    "    mean_diff_percent = (mean_diff / meanY) * 100\n",
    "    median_diff_percent = (median_diff / medianY) * 100\n",
    "    std_diff_percent = (std_diff / stdY) * 100\n",
    "    basics = {\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"len(X)\": lenX,\n",
    "        \"len(Y)\": lenY,\n",
    "        \"mean(X)\": meanX,\n",
    "        \"mean(Y)\": meanY,\n",
    "        \"median(X)\": medianX,\n",
    "        \"median(Y)\": medianY,\n",
    "        \"std(X)\": stdX,\n",
    "        \"std(Y)\": stdY,\n",
    "        \"mean diff\": mean_diff,\n",
    "        \"median diff\": median_diff,\n",
    "        \"std diff\": std_diff,\n",
    "        \"mean diff %\": mean_diff_percent,\n",
    "        \"median diff %\": median_diff_percent,\n",
    "        \"std diff %\": std_diff_percent,\n",
    "    }\n",
    "    comparisons = {\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"len(X)\": lenX,\n",
    "        \"len(Y)\": lenY,\n",
    "        # \"Wilcoxon (W)\": wstat,\n",
    "        # \"p-value (Wilcoxon)\": wp,\n",
    "        # \"z-statistic (Wilcoxon)\": zw,\n",
    "        \"U (MWU)\": ustat,\n",
    "        \"p-value (MWU)\": p,\n",
    "        \"effect size (MWU, rank-biserial correlation)\": rankBiserialCorrelation,\n",
    "        \"t (t-test)\": tstat,\n",
    "        \"p-value (t-test)\": tp,\n",
    "        \"effect size (t-test, Cohen's d)\": cohen_d,\n",
    "        \"Shapiro X\": shapiroX,\n",
    "        \"Shapiro Y\": shapiroY,\n",
    "        \"Brown-Forsythe\": brown_forsythe,\n",
    "        \"Fligner\": fligner,\n",
    "        \"K-S\": ks,\n",
    "        \"Anderson-Darling\": ad,\n",
    "        \"Kruskal-Wallis\": kw,\n",
    "    }\n",
    "    return basics, comparisons\n",
    "\n",
    "def make_comparisons(cases, index=None, plots=True):\n",
    "    comparisons = []\n",
    "    comparisons_clean = []\n",
    "    basics = []\n",
    "    basics_clean = []\n",
    "    if plots:\n",
    "        figScatter, axsScatter = plt.subplots(1, 1, figsize=(30, 15))\n",
    "        fig, axs = plt.subplots(1, len(cases)*2, figsize=(20, 8))\n",
    "    prevFullSeries = None\n",
    "    for i, case in enumerate(cases):\n",
    "        X = case[\"X\"]\n",
    "        Y = case[\"Y\"]\n",
    "        lenX = case[\"len(X)\"]\n",
    "        lenY = case[\"len(Y)\"]\n",
    "        XSeries = case[\"XSeries\"]\n",
    "        YSeries = case[\"YSeries\"]\n",
    "        outliersX = find_outliers_iqr(XSeries)\n",
    "        outliersY = find_outliers_iqr(YSeries)\n",
    "        # clean outliers\n",
    "        clean_XSeries = clean_outliers(XSeries, outliersX)\n",
    "        clean_YSeries = clean_outliers(YSeries, outliersY)\n",
    "        XSeriesCPU = case[\"XSeriesCPU\"]\n",
    "        YSeriesCPU = case[\"YSeriesCPU\"]\n",
    "        if plots:\n",
    "            fullSeries = pd.concat([XSeries, YSeries])\n",
    "            fullSeriesCPU = pd.concat([XSeriesCPU, YSeriesCPU])\n",
    "            y_ticks = np.arange(0, 1.01, 0.05)\n",
    "            x_ticks = y_ticks\n",
    "            title = f\"{index_list_full_names[index_list_names.index(index)] if index else 'All cases'} - CPU comparison\"\n",
    "            # plot scatter if not plotted previously over the same data\n",
    "            if prevFullSeries is None or not prevFullSeries.equals(fullSeries):\n",
    "                figScatter.set_tight_layout(True)\n",
    "                figScatter.suptitle(f\"{index if index else 'All cases'}\")\n",
    "                axsScatter.scatter(fullSeriesCPU, fullSeries)\n",
    "                axsScatter.set_title(f\"CPU vs {case['metric']}\")\n",
    "                axsScatter.set_xlabel(\"CPU\")\n",
    "                axsScatter.set_ylabel(case[\"metric\"])\n",
    "                axsScatter.set_xticks(x_ticks)\n",
    "                axsScatter.set_yticks(y_ticks)\n",
    "                axsScatter.grid(True)\n",
    "                prevFullSeries = fullSeries\n",
    "                figScatter.savefig(f\"plots/scatters/{title if index else 'all'}.png\")\n",
    "            fig.set_tight_layout(True)\n",
    "            fig.suptitle(f'{title if index else \"All cases\"}')\n",
    "            # ax index\n",
    "            idx = i * 2\n",
    "            # plot boxplots\n",
    "            axs[idx].boxplot([XSeries, YSeries], labels=[X + f\"\\n(n = {str(lenX)})\", Y + f\"\\n(n = {str(lenY)})\"])\n",
    "            axs[idx].set_title(f\"{X} vs {Y}\")\n",
    "            axs[idx].set_ylabel(case[\"metric\"])\n",
    "            axs[idx].set_ylim(0, 1)\n",
    "            axs[idx].set_yticks(y_ticks)\n",
    "            axs[idx].grid(True)\n",
    "            # plot violin plots\n",
    "            axs[idx+1].violinplot([XSeries, YSeries], showmeans=False, showmedians=True)\n",
    "            axs[idx+1].set_title(f\"{X} vs {Y}\")\n",
    "            axs[idx+1].set_ylabel(case[\"metric\"])\n",
    "            axs[idx+1].set_yticks(y_ticks)\n",
    "            axs[idx+1].set_xticks([1, 2])\n",
    "            axs[idx+1].set_xticklabels([X + f\"\\n(n = {str(lenX)})\", Y + f\"\\n(n = {str(lenY)})\"])\n",
    "            axs[idx+1].grid(True)\n",
    "            if i == len(cases) - 1:\n",
    "                fig.savefig(f\"plots/boxplots/{index if index else 'all'}.png\")\n",
    "        if lenX != 0 and lenY != 0:\n",
    "            basic, comparison = calculate_comparisons(X, Y, XSeries, YSeries, lenX, lenY)\n",
    "            basic[\"X\"] = X\n",
    "            basic[\"Y\"] = Y\n",
    "            basic[\"len(outliers(X))\"] = len(outliersX)\n",
    "            basic[\"len(outliers(Y))\"] = len(outliersY)\n",
    "            comparison[\"X\"] = X\n",
    "            comparison[\"Y\"] = Y\n",
    "            pearsonX = stats.pearsonr(XSeriesCPU, XSeries)\n",
    "            pearsonY = stats.pearsonr(YSeriesCPU, YSeries)\n",
    "            comparison[\"Pearson X\"] = pearsonX\n",
    "            comparison[\"Pearson Y\"] = pearsonY\n",
    "            comparisons.append(comparison)\n",
    "            basics.append(basic)\n",
    "            basic_clean, comparison_clean = calculate_comparisons(X, Y, clean_XSeries, clean_YSeries, lenX - len(outliersX), lenY - len(outliersY))\n",
    "            basic_clean[\"X\"] = X\n",
    "            basic_clean[\"Y\"] = Y\n",
    "            basic_clean[\"len(outliers(X))\"] = len(outliersX)\n",
    "            basic_clean[\"len(outliers(Y))\"] = len(outliersY)\n",
    "            comparison_clean[\"X\"] = X\n",
    "            comparison_clean[\"Y\"] = Y\n",
    "            pearsonX_clean = stats.pearsonr(XSeriesCPU, clean_XSeries)\n",
    "            pearsonY_clean = stats.pearsonr(YSeriesCPU, clean_YSeries)\n",
    "            comparison_clean[\"Pearson X\"] = pearsonX_clean\n",
    "            comparison_clean[\"Pearson Y\"] = pearsonY_clean\n",
    "            comparisons_clean.append(comparison_clean)\n",
    "            basics_clean.append(basic_clean)\n",
    "    basics_df = pd.DataFrame(basics)\n",
    "    basics_df.to_csv(f\"tables/basics_{index if index else 'all'}.csv\", index=False)\n",
    "    comparisons_df = pd.DataFrame(comparisons)\n",
    "    comparisons_df.to_csv(f\"tables/comparisons_{index if index else 'all'}.csv\", index=False)\n",
    "    basics_clean_df = pd.DataFrame(basics_clean)\n",
    "    basics_clean_df.to_csv(f\"tables/basics_clean_{index if index else 'all'}.csv\", index=False)\n",
    "    comparisons_clean_df = pd.DataFrame(comparisons_clean)\n",
    "    comparisons_clean_df.to_csv(f\"tables/comparisons_clean_{index if index else 'all'}.csv\", index=False)\n",
    "    return basics_df, comparisons_df, basics_clean_df, comparisons_clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paint simple line chart with QoE values, averaging all users\n",
    "for idx, index_list_name in enumerate(index_list_names):\n",
    "    df_cpu = df_cpu_filtered_list[idx]\n",
    "    df_mem = df_mem_filtered_list[idx]\n",
    "    connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index_list_names[idx]]\n",
    "    connection_stats[\"time_midpoint\"] = (connection_stats[\"start_time\"] + connection_stats[\"end_time\"]) / 2\n",
    "    if \"kurento\" in index_list_name:\n",
    "        interval = 0.001\n",
    "    else:\n",
    "        interval = 1\n",
    "    connection_stats[\"time_group\"] = connection_stats[\"time_midpoint\"] // interval * interval\n",
    "    connection_stats_vmaf = connection_stats[connection_stats[\"qoe_metric\"] == \"vmaf\"]\n",
    "    connection_stats_vmaf = connection_stats_vmaf.groupby(\"time_group\").agg({\"qoe\": \"mean\"}).reset_index()\n",
    "    connection_stats_visqol = connection_stats[connection_stats[\"qoe_metric\"] == \"visqol\"]\n",
    "    connection_stats_visqol = connection_stats_visqol.groupby(\"time_group\").agg({\"qoe\": \"mean\"}).reset_index()\n",
    "    fig, ax1 = plt.subplots(figsize=(20, 6))\n",
    "    ax1.plot(connection_stats_vmaf[\"time_group\"], connection_stats_vmaf[\"qoe\"], label=\"VMAF\", color=\"blue\")\n",
    "    ax1.plot(connection_stats_visqol[\"time_group\"], connection_stats_visqol[\"qoe\"], label=\"VISQOL\", color=\"red\")\n",
    "    ax1.set_xlabel(\"Time (s)\")\n",
    "    ax1.set_ylabel(\"QoE\")\n",
    "    ax1.set_title(f\"QoE progression - {index_list_full_names[idx]}\")\n",
    "    ax1.grid()\n",
    "    # y ranges from 0 to 1\n",
    "    ax1.set_ylim(0, 1.01)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_cpu[\"@timestamp\"], df_cpu[\"cpu\"], color=\"black\", marker=\"o\", linewidth=1, markersize=3, label=\"CPU\")\n",
    "    ax2.set_ylabel(\"CPU (%)\")\n",
    "    # y axis should go from 0 to 1\n",
    "    ax2.set_ylim(0, 1.01)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines[\"right\"].set_position((\"axes\", 1.05))\n",
    "    ax3.spines[\"right\"].set_visible(True)\n",
    "    ax3.yaxis.set_label_position(\"right\")\n",
    "    ax3.yaxis.tick_right()\n",
    "    ax3.plot(df_mem[\"@timestamp\"], df_mem[\"memory\"], color=\"green\", marker=\"o\", linewidth=1, markersize=3, label=\"Memory\")\n",
    "    ax3.set_ylabel(\"Memory (%)\")\n",
    "    # y axis should go from 0 to 1\n",
    "    ax3.set_ylim(0, 1.01)\n",
    "\n",
    "    fig.legend(loc=\"upper left\")\n",
    "    fig.savefig(f\"plots/qoe/simple/qoe_progression_{index_list_name[21:]}.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare QoE values between different CPU thresholds\n",
    "import warnings\n",
    "# display(\"All cases together\")\n",
    "# cases = get_cases(connection_stats_global)\n",
    "# comparisons = make_comparisons(cases)\n",
    "# display(comparisons)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for index_list_name in index_list_names:\n",
    "    connection_stats = connection_stats_global[\n",
    "        connection_stats_global[\"index\"] == index_list_name\n",
    "    ].copy()\n",
    "    display(index_list_name)\n",
    "    cases = get_cases(connection_stats)\n",
    "    basics, comparisons, basics_clean, comparisons_clean = make_comparisons(cases, index_list_name)\n",
    "    display(basics, basics_clean, comparisons, comparisons_clean)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "for index_list_name in index_list_names:\n",
    "    connection_stats = connection_stats_global[\n",
    "        connection_stats_global[\"index\"] == index_list_name\n",
    "    ].copy()\n",
    "    display(index_list_name)\n",
    "    cases = get_cases(connection_stats, label=\"VISQOL\", qoe_metric=\"visqol\")\n",
    "    basics, comparisons, basics_clean, comparisons_clean = make_comparisons(cases, index_list_name)\n",
    "    display(basics, basics_clean, comparisons, comparisons_clean)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare connection start values when the connection start is followed by a connection success\n",
    "import warnings\n",
    "def remove_no_connections(connection_stats):\n",
    "    # Identify connection starts that are not followed by a connection success\n",
    "    mask = connection_stats[\"type\"] == \"connectionStart\"\n",
    "    \n",
    "    # Collect indices of connection starts to be removed\n",
    "    to_remove = []\n",
    "    for i, row in connection_stats[mask].iterrows():\n",
    "        # Check if there's a subsequent connectionSuccess for the same full_user and attempt\n",
    "        next_row = connection_stats[\n",
    "            (connection_stats[\"full_user\"] == row[\"full_user\"]) &\n",
    "            (connection_stats[\"attempt\"] == row[\"attempt\"]) &\n",
    "            (connection_stats[\"start_time\"] > row[\"start_time\"]) &\n",
    "            (connection_stats[\"type\"] == \"connectionSuccess\")\n",
    "        ]\n",
    "        \n",
    "        # If no connectionSuccess found, mark for removal\n",
    "        if next_row.empty:\n",
    "            to_remove.append(i)\n",
    "    \n",
    "    # Remove the identified rows\n",
    "    connection_stats.drop(to_remove, inplace=True)\n",
    "\n",
    "    return connection_stats\n",
    "# Connection start of successful connections\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for index in index_list_names:\n",
    "    connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index].copy()\n",
    "    connection_stats = remove_no_connections(connection_stats)\n",
    "\n",
    "    display(index)\n",
    "    cases = get_cases(connection_stats, con_type=\"connectionStart\", label=\"connection start duration (seconds)\")\n",
    "    basics, comparisons, basics_clean, comparisons_clean = make_comparisons(cases, index, plots=False)\n",
    "    display(basics, basics_clean, comparisons, comparisons_clean)\n",
    "\n",
    "warnings.filterwarnings(\"default\")\n",
    "# Connection start of successful connections\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for index in index_list_names:\n",
    "    connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index].copy()\n",
    "    connection_stats = remove_no_connections(connection_stats)\n",
    "\n",
    "    display(index)\n",
    "    cases = get_cases(connection_stats, con_type=\"connectionStart\", label=\"connection start duration (seconds)\")\n",
    "    basics, comparisons, basics_clean, comparisons_clean = make_comparisons(cases, index, plots=False)\n",
    "    display(basics, basics_clean, comparisons, comparisons_clean)\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For subscribers in tests with subscribers, compare connection start values when the connection start is followed by a connection success\n",
    "def remove_no_connections_subs(connection_stats):\n",
    "    con_stats = connection_stats.copy()\n",
    "    # remove all connectionStart that are not followed by a connectionSuccess for that full_user and attempt\n",
    "    for i, row in con_stats.iterrows():\n",
    "        if row[\"type\"] == \"connectionStart\":\n",
    "            if int(row[\"user\"]) > 3:\n",
    "                con_stats = con_stats.drop(i)\n",
    "            else:\n",
    "                next_row = con_stats[\n",
    "                    (con_stats[\"full_user\"] == row[\"full_user\"])\n",
    "                    & (con_stats[\"attempt\"] == row[\"attempt\"])\n",
    "                    & (con_stats[\"start_time\"] > row[\"start_time\"])\n",
    "                ]\n",
    "                if next_row.empty or next_row.iloc[0][\"type\"] != \"connectionSuccess\":\n",
    "                    con_stats = con_stats.drop(i)\n",
    "    return con_stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "connection_stats_subs = connection_stats_global[connection_stats_global[\"index\"].str.contains(\"0s\")].copy()\n",
    "for index in index_list_names:\n",
    "    if \"0s\" in index:\n",
    "        connection_stats = connection_stats_subs[connection_stats_subs[\"index\"] == index].copy()\n",
    "        connection_stats = remove_no_connections_subs(connection_stats)\n",
    "        display(index)\n",
    "        cases = get_cases(connection_stats, con_type=\"connectionStart\", label=\"connection start duration (seconds)\")\n",
    "        basics, comparisons, basics_clean, comparisons_clean = make_comparisons(cases, index, plots=False)\n",
    "        display(basics, basics_clean, comparisons, comparisons_clean)\n",
    "warnings.filterwarnings(\"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For subscribers in tests with subscribers, compare connection start values when the connection start is followed by a connection success\n",
    "def remove_no_connections_subs(connection_stats):\n",
    "    con_stats = connection_stats.copy()\n",
    "    # remove all connectionStart that are not followed by a connectionSuccess for that full_user and attempt\n",
    "    for i, row in con_stats.iterrows():\n",
    "        if row[\"type\"] == \"connectionStart\":\n",
    "            if int(row[\"user\"]) <= 3:\n",
    "                con_stats = con_stats.drop(i)\n",
    "            else:\n",
    "                next_row = con_stats[\n",
    "                    (con_stats[\"full_user\"] == row[\"full_user\"])\n",
    "                    & (con_stats[\"attempt\"] == row[\"attempt\"])\n",
    "                    & (con_stats[\"start_time\"] > row[\"start_time\"])\n",
    "                ]\n",
    "                if next_row.empty or next_row.iloc[0][\"type\"] != \"connectionSuccess\":\n",
    "                    con_stats = con_stats.drop(i)\n",
    "    return con_stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "connection_stats_subs = connection_stats_global[connection_stats_global[\"index\"].str.contains(\"0s\")].copy()\n",
    "for index in index_list_names:\n",
    "    if \"0s\" in index:\n",
    "        connection_stats = connection_stats_subs[connection_stats_subs[\"index\"] == index].copy()\n",
    "        connection_stats = remove_no_connections_subs(connection_stats)\n",
    "        display(index)\n",
    "        cases = get_cases(connection_stats, con_type=\"connectionStart\", label=\"connection start duration (seconds)\")\n",
    "        basics, comparisons, basics_clean, comparisons_clean = make_comparisons(cases, index, plots=False)\n",
    "        display(basics, basics_clean, comparisons, comparisons_clean)\n",
    "warnings.filterwarnings(\"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all stats data and save to file (only if not already done)\n",
    "# 3 types of stats\n",
    "# webrtcStats: jitter etc.\n",
    "# inbound: qoe calculated via inbound metrics\n",
    "# outbound: qoe calculated via outbound metrics\n",
    "# at the moment we only care about webrtcStats\n",
    "\n",
    "def process_stats(index_list_name):\n",
    "    stats = get_index_data(index_list_name, \"stats\")\n",
    "    for stat in stats:\n",
    "        if 'stats' in stat:\n",
    "            user = stat['user']\n",
    "            session = stat['session']\n",
    "            in_stats = stat['stats']\n",
    "            for s in in_stats:\n",
    "                if 'webrtcStats' in s:\n",
    "                    webrtc_stats = s['webrtcStats']\n",
    "                    for ws in webrtc_stats:\n",
    "                        if ('event' in ws) and (ws['event'] == 'stats'):\n",
    "                            yield {\n",
    "                                'user': user,\n",
    "                                'session': session,\n",
    "                                'data': ws\n",
    "                            }\n",
    "\n",
    "def add_user_data(data, user, session, remote_peer):\n",
    "    for d in data:\n",
    "        d['user'] = user\n",
    "        d['session'] = session\n",
    "        d['peerId'] = remote_peer\n",
    "        # if cummulative metrics packetsLost and packetsSent/packetsReceived are present, calculate packetLoss\n",
    "        if 'packetsLost' in d and ('packetsSent' in d or 'packetsReceived' in d):\n",
    "            d['packetLoss'] = d['packetsLost'] / (d['packetsSent'] + d['packetsReceived'])\n",
    "\n",
    "\n",
    "def separate_stats(index_list_name):\n",
    "\n",
    "    stats_dict = {\n",
    "        'inbound_audio': [],\n",
    "        'inbound_video': [],\n",
    "        'outbound_audio': [],\n",
    "        'outbound_video': [],\n",
    "        'remote_inbound_audio': [],\n",
    "        'remote_inbound_video': [],\n",
    "        'remote_outbound_audio': [],\n",
    "        'remote_outbound_video': []\n",
    "    }\n",
    "\n",
    "    def process_data(data, user, session, peer_id, direction, type):\n",
    "        if data:\n",
    "            add_user_data(data, user, session, peer_id)\n",
    "            stats_dict[f\"{direction}_{type}\"].extend(data)\n",
    "\n",
    "    for full_data in process_stats(index_list_name):\n",
    "        user = full_data['user']\n",
    "        session = full_data['session']\n",
    "        r = full_data['data']\n",
    "        remotePeer = r['peerId']\n",
    "        if 'data' in r:\n",
    "            d = r['data']\n",
    "            local_audio = d['audio']\n",
    "            local_video = d['video']\n",
    "            remote_audio = d['remote']['audio']\n",
    "            remote_video = d['remote']['video']\n",
    "\n",
    "            process_data(local_audio['inbound'], user, session, remotePeer, 'inbound', 'audio')\n",
    "            process_data(local_audio['outbound'], user, session, remotePeer, 'outbound', 'audio')\n",
    "            process_data(local_video['inbound'], user, session, remotePeer, 'inbound', 'video')\n",
    "            process_data(local_video['outbound'], user, session, remotePeer, 'outbound', 'video')\n",
    "            process_data(remote_audio['inbound'], user, session, remotePeer, 'remote_inbound', 'audio')\n",
    "            process_data(remote_audio['outbound'], user, session, remotePeer, 'remote_outbound', 'audio')\n",
    "            process_data(remote_video['inbound'], user, session, remotePeer, 'remote_inbound', 'video')\n",
    "            process_data(remote_video['outbound'], user, session, remotePeer, 'remote_outbound', 'video')\n",
    "\n",
    "    for key, value in stats_dict.items():\n",
    "        if value:\n",
    "            stats_dict[key] = pd.DataFrame(value)\n",
    "            stats_dict[key].to_csv(f\"dfs_stats_processed/{index_list_name}_{key}.csv\", index=False)\n",
    "\n",
    "for index_list_name in index_list_names:\n",
    "    separate_stats(index_list_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index_list_names[1]\n",
    "direction = \"remote_inbound\"\n",
    "type = \"video\"\n",
    "metric = \"bitrate\"\n",
    "strictness = 0\n",
    "\n",
    "\n",
    "stats_inbound_video = pd.read_csv(f\"dfs_stats_processed/{index}_{direction}_{type}.csv\")\n",
    "display(stats_inbound_video.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [45, 30]\n",
    "\n",
    "strictness = 0\n",
    "directions = [\"inbound\", \"outbound\", \"remote_inbound\", \"remote_outbound\"]\n",
    "types = [\"audio\", \"video\"]\n",
    "jitter_threshold = 0.03\n",
    "\n",
    "for type in types:\n",
    "    for direction in directions:\n",
    "        if direction == \"inbound\":\n",
    "            metrics = [\n",
    "                \"bitrate\",\n",
    "                \"packetRate\",\n",
    "                \"jitter\",\n",
    "            ]\n",
    "        elif direction == \"outbound\":\n",
    "            metrics = [\n",
    "                \"bitrate\",\n",
    "                \"packetRate\",\n",
    "            ]\n",
    "        elif direction == \"remote_inbound\":\n",
    "            metrics = [\n",
    "                \"jitter\",\n",
    "                \"roundTripTime\"\n",
    "            ]\n",
    "        elif direction == \"remote_outbound\":\n",
    "            metrics = []\n",
    "        for metric in metrics:\n",
    "            fig, axs = plt.subplots(len(index_list_names), 1)\n",
    "            for i, index in enumerate(index_list_names):\n",
    "                filepath = f\"dfs_stats_processed/{index}_{direction}_{type}.csv\"\n",
    "                if not os.path.exists(filepath):\n",
    "                    continue\n",
    "                stats_inbound_video = pd.read_csv(\n",
    "                    f\"dfs_stats_processed/{index}_{direction}_{type}.csv\"\n",
    "                )\n",
    "                if not metric in stats_inbound_video.columns:\n",
    "                    continue\n",
    "                start_time = stats_inbound_video[\"timestamp\"].min()\n",
    "                end_time = stats_inbound_video[\"timestamp\"].max()\n",
    "                stats_inbound_video[\"timestamp\"] = stats_inbound_video[\"timestamp\"].apply(\n",
    "                    lambda x: (x - start_time) / 1000\n",
    "                )\n",
    "                cpu = df_cpu_filtered_list[i]\n",
    "                ax = axs[i]\n",
    "                ax.set_title(f\"{index} - {direction} {type} {metric}\")\n",
    "                ax.set_ylabel(\"CPU (%)\")\n",
    "                ax.set_xlabel(\"Time (s)\")\n",
    "                ax.set_ylim(0, 1.01)\n",
    "                ax.plot(\n",
    "                    cpu[\"@timestamp\"],\n",
    "                    cpu[\"cpu\"],\n",
    "                    color=\"black\",\n",
    "                    marker=\"o\",\n",
    "                    linewidth=4,\n",
    "                    markersize=10,\n",
    "                )\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.set_ylabel(f\"{metric}\")\n",
    "                # ax2.set_ylim(0, 100)\n",
    "                # ax2.set_yticks(np.arange(0, 100, 5))\n",
    "                stats_inbound_video[\"timestamp\"] = stats_inbound_video[\"timestamp\"].apply(\n",
    "                    lambda x: round(x, strictness)\n",
    "                )\n",
    "                avg_jitter = (\n",
    "                    stats_inbound_video[metric]\n",
    "                    .groupby(stats_inbound_video[\"timestamp\"])\n",
    "                    .mean()\n",
    "                )\n",
    "                ax2.plot(\n",
    "                    avg_jitter.index,\n",
    "                    avg_jitter.values,\n",
    "                    color=\"blue\",\n",
    "                    marker=\"o\",\n",
    "                )\n",
    "                max_jitter = (\n",
    "                    stats_inbound_video[metric].groupby(stats_inbound_video[\"timestamp\"]).max()\n",
    "                )\n",
    "                min_jitter = (\n",
    "                    stats_inbound_video[metric].groupby(stats_inbound_video[\"timestamp\"]).min()\n",
    "                )\n",
    "                ax2.fill_between(\n",
    "                    avg_jitter.index,\n",
    "                    min_jitter.values,\n",
    "                    max_jitter.values,\n",
    "                    color=\"blue\",\n",
    "                    alpha=0.3,\n",
    "                )\n",
    "                # horizontal line on threshold\n",
    "                if metric == \"jitter\":\n",
    "                    ax2.axhline(y=jitter_threshold, color=\"red\", linestyle=\"--\")\n",
    "                elif metric == \"roundTripTime\":\n",
    "                    ax2.axhline(y=0.25, color=\"red\", linestyle=\"--\")\n",
    "                ax.grid(True)\n",
    "                # fig.savefig(f\"plots/{index}_{direction}_{type}.png\", bbox_inches=\"tight\")\n",
    "\n",
    "# TODO: Plot por usuario como las de QoE\n",
    "# TODO: medias barra para QoS y media para QoE\n",
    "# TODO: revisar secciones moradas de QoE -> son en los tests de pion 2p, cuando un solo usuario tiene una desconexin en la sesin el otro usuario ya no puede grabar (evento de trackunsubscribed)\n",
    "# TODO: Lanzar los tests con mediasoup -> con OV2\n",
    "# TODO: Bug a veces no se capturan las stats con una de las librerias -> Los payloads eran muy grandes en los tests de 8p con pion y mediasoup, arreglado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint the connection progression of all users in a test and save the relevant data to DataFrames\n",
    "plt.rcParams[\"figure.figsize\"] = [40, 35]\n",
    "from decimal import Decimal, getcontext\n",
    "getcontext().prec = 9\n",
    "\n",
    "def avg_qos_intervals(stats, full_user, metric):\n",
    "    time_interval = \"1s\"\n",
    "    user_data = full_user.split(\"-\")\n",
    "    user = user_data[0]\n",
    "    session = user_data[1]\n",
    "    user_stats = stats[(stats[\"user\"] == f\"User{user}\") & (stats[\"session\"] == f\"LoadTestSession{session}\")]\n",
    "    user_stats = user_stats.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "    user_stats[metric] = user_stats[metric].apply(Decimal)\n",
    "    if \"bitrate\" in metric or \"packetRate\" in metric:\n",
    "        user_stats = user_stats[user_stats[metric] > 0]\n",
    "    user_stats['timedelta'] = pd.to_timedelta(user_stats['timestamp'], unit='S')\n",
    "    user_stats.set_index('timedelta', inplace=True)\n",
    "    user_stats = user_stats.resample(time_interval).agg({metric: 'mean'}).dropna()\n",
    "    user_stats['timestamp'] = user_stats.index.total_seconds().astype(int)\n",
    "    user_stats['next_timestamp'] = user_stats['timestamp'].shift(-1)\n",
    "    user_stats = user_stats.dropna(subset=['next_timestamp'])\n",
    "    sub_intervals = list(zip(user_stats[['timestamp', 'next_timestamp']].values.tolist(), user_stats[metric]))\n",
    "    return sub_intervals\n",
    "\n",
    "def get_con_type(next_color):\n",
    "    if next_color == \"magenta\":\n",
    "        return \"connectionStart\"\n",
    "    elif next_color == \"indigo\":\n",
    "        return \"connectionSuccess\"\n",
    "    elif next_color == \"red\":\n",
    "        return \"connectionFail\"\n",
    "    else:\n",
    "        return \"QOE\"\n",
    "\n",
    "def paint_bar_qos(ax, prev_time, current_time, y_ticks, prev_state, next_color):\n",
    "    total_duration = current_time - prev_time\n",
    "    yrange = (y_ticks - 0.5, 0.8)\n",
    "    if prev_state != \"connectionSuccess\":\n",
    "        segment = [(prev_time, total_duration)]\n",
    "        ax.broken_barh(segment, yrange, facecolors=next_color)\n",
    "\n",
    "def paint_success_bar_qos(ax, segments, y_ticks, success_cmap, normalize):\n",
    "    yrange = (y_ticks - 0.5, 0.8)\n",
    "    batched_segments = []\n",
    "    batched_colors = []\n",
    "\n",
    "    for prev_time, current_time, qos in segments:\n",
    "        total_duration = current_time - prev_time\n",
    "        segment = (prev_time, total_duration)\n",
    "        color = success_cmap(normalize(qos))\n",
    "\n",
    "        batched_segments.append(segment)\n",
    "        batched_colors.append(color)\n",
    "    \n",
    "    ax.broken_barh(batched_segments, yrange, facecolors=batched_colors)\n",
    "\n",
    "def draw_cons_plot_qos(i, index_list_name, direction, track_type, metric, vmin, vmax, colors_list):\n",
    "    success_cmap = mcolors.LinearSegmentedColormap.from_list(\"success\", colors_list)\n",
    "    normalize = mcolors.Normalize(vmin, vmax)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    y_labels = []\n",
    "    y_ticks = 0\n",
    "    user_count = 0\n",
    "    cpu_filtered = df_cpu_filtered_list[i]\n",
    "    start_time, end_time = start_end_times[index_list_name]\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    stats = pd.read_csv(f\"dfs_stats_processed/{index_list_name}_{direction}_{track_type}.csv\")\n",
    "    stats[\"timestamp\"] = pd.to_datetime(stats[\"timestamp\"], unit=\"ms\") \\\n",
    "        .dt.tz_localize(\"UTC\") \\\n",
    "        .apply(lambda x: (x - start_time).total_seconds())\n",
    "    plt.ioff()\n",
    "    for full_user, full_user_obj in user_events_obj[index_list_name].items():\n",
    "        # display(f\"User {full_user} started at time {datetime.now()}\")\n",
    "        if ((\"remote_inbound\" in direction) or (\"outbound\" in direction)) and (not \"8p\" in index_list_name) and (int(full_user.split(\"-\")[0]) > 3):\n",
    "            continue\n",
    "        user_count += 1\n",
    "        y_labels.append(full_user)\n",
    "        y_ticks += 1\n",
    "        next_color = \"white\"\n",
    "        prev_time = None\n",
    "        prev_state = None\n",
    "        combined = full_user_obj[\"combined\"]\n",
    "        full_con_segments = connection_time_intervals[index_list_name][full_user]\n",
    "        con_segments = avg_qos_intervals(stats, full_user, metric)\n",
    "        # connection success\n",
    "        for full_segment in full_con_segments:\n",
    "            paint_bar_qos(ax, full_segment[0], full_segment[1], y_ticks, \"\", \"indigo\")\n",
    "        segments_to_plot = [(segment[0][0], segment[0][1], segment[1]) for segment in con_segments]\n",
    "        paint_success_bar_qos(ax, segments_to_plot, y_ticks, success_cmap, normalize)\n",
    "        # other connection events\n",
    "        for r, row in combined.iterrows():\n",
    "            current_time = (row[\"timestamp\"] - start_time).total_seconds()\n",
    "            if prev_time is not None:\n",
    "                paint_bar_qos(ax, prev_time, current_time, y_ticks, prev_state, next_color)\n",
    "            connectionSuccessful = (\n",
    "                row[\"event\"].startswith(\"streamCreated\")\n",
    "                or row[\"event\"].startswith(\"streamPlaying\")\n",
    "                or row[\"event\"].startswith(\"LocalTrackPublished\")\n",
    "                or row[\"event\"].startswith(\"TrackSubscribed\")\n",
    "            )\n",
    "            if row[\"event\"] == \"connectionStart\":\n",
    "                next_color = \"magenta\"\n",
    "                prev_state = \"connectionStart\"\n",
    "            elif connectionSuccessful:\n",
    "                next_color = \"indigo\"\n",
    "                prev_state = \"connectionSuccess\"\n",
    "            else:\n",
    "                next_color = \"red\"\n",
    "                prev_state = \"connectionFail\"\n",
    "            prev_time = current_time\n",
    "        if prev_time is not None:\n",
    "            paint_bar(ax, prev_time, test_duration, y_ticks, prev_state, next_color)\n",
    "        # display(f\"User {full_user} finished at time {datetime.now()}\")\n",
    "\n",
    "    ax.set_yticks(range(1, y_ticks + 1))\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_title(f\"Connection Progression ({direction} {track_type} {metric}) - {index_list_full_names[i]}\", pad=10)\n",
    "    # y axis label\n",
    "    ax.set_ylabel(\"User - Session\")\n",
    "    if user_count >= 150:\n",
    "        # remove y labels\n",
    "        ax.set_yticklabels([])\n",
    "    # x axis label\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xlim(-5, test_duration + 5)\n",
    "    # grid\n",
    "    ax.grid(True)\n",
    "    # x axis ticks every 10 seconds\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    # cpu in new axis\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel(\"CPU (%)\")\n",
    "    # ax2 y axis should go from 0 to 1\n",
    "    ax2.set_ylim(0, 1.01)\n",
    "    ax2.set_yticks(np.arange(0, 1.01, 0.05))\n",
    "    ax2.plot(cpu_filtered[\"@timestamp\"], cpu_filtered[\"cpu\"], color=\"black\", marker=\"o\", linewidth=4, markersize=10)\n",
    "    # create legend\n",
    "    legend_labels = [\n",
    "        plt.Line2D([0], [0], color=\"magenta\", lw=4, label=\"Connection Start\"),\n",
    "        plt.Line2D([0], [0], color=\"indigo\", lw=4, label=\"Stream Created/Playing, No stats recorded\"),\n",
    "        plt.Line2D([0], [0], color=\"darkblue\", lw=4, label=\"Excellent QoS\"),\n",
    "        plt.Line2D([0], [0], color=\"green\", lw=4, label=\"Good QoS\"),\n",
    "        plt.Line2D([0], [0], color=\"yellow\", lw=4, label=\"Fair QoS\"),\n",
    "        plt.Line2D([0], [0], color=\"orange\", lw=4, label=\"Poor QoS\"),\n",
    "        plt.Line2D([0], [0], color=\"black\", lw=4, label=\"Bad QoS\"),\n",
    "        plt.Line2D([0], [0], color=\"red\", lw=4, label=\"Failed Connection\"),\n",
    "        plt.Line2D([0], [0], color=\"black\", lw=4, label=\"CPU\", marker=\"o\", markersize=10)\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_labels, loc=\"upper left\")\n",
    "    plt.ion()\n",
    "    plt.draw()\n",
    "    os.makedirs(f\"plots/qos/{direction}/{track_type}/{metric}\", exist_ok=True)\n",
    "    fig.savefig(f\"plots/qos/{direction}/{track_type}/{metric}/connection_progression_{direction}_{track_type}_{metric}_{index_list_name[21:]}_{metric}.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(index_list_names):\n",
    "    #draw_cons_plot_qos(i, index, \"remote_inbound\", \"video\", \"roundTripTime\", [0, 0.1, 0.15, 0.3, 0.5, float('inf')], [\"darkblue\", \"green\", \"yellow\", \"orange\", \"black\"])\n",
    "        draw_cons_plot_qos(i, index, \"remote_inbound\", \"video\", \"roundTripTime\", 0, 0.5, [\"darkblue\", \"green\", \"yellow\", \"orange\", \"black\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(index_list_names):\n",
    "    # draw_cons_plot_qos(i, index, \"inbound\", \"video\", \"jitter\", [0, 0.02, 0.03, 0.05, 0.1, float(\"inf\")], [\"darkblue\", \"green\", \"yellow\", \"orange\", \"black\"])\n",
    "    # draw_cons_plot_qos(i, index, \"inbound\", \"audio\", \"jitter\", [0, 0.02, 0.03, 0.05, 0.1, float(\"inf\")], [\"darkblue\", \"green\", \"yellow\", \"orange\", \"black\"])\n",
    "    draw_cons_plot_qos(i, index, \"inbound\", \"video\", \"jitter\", 0, 0.1, [\"darkblue\", \"green\", \"yellow\", \"orange\", \"black\"])\n",
    "    draw_cons_plot_qos(i, index, \"inbound\", \"audio\", \"jitter\", 0, 0.1, [\"darkblue\", \"green\", \"yellow\", \"orange\", \"black\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(index_list_names):\n",
    "    # draw_cons_plot_qos(i, index, \"inbound\", \"video\", \"bitrate\", [0, 500000, 800000, 1000000, 1500000, float('inf')], [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    # draw_cons_plot_qos(i, index, \"inbound\", \"audio\", \"bitrate\", [0, 10000, 16000, 32000, 64000, float('inf')], [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    # draw_cons_plot_qos(i, index, \"outbound\", \"video\", \"bitrate\", [0, 500000, 800000, 1000000, 1500000, float('inf')], [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    # draw_cons_plot_qos(i, index, \"outbound\", \"audio\", \"bitrate\", [0, 10000, 16000, 32000, 64000, float('inf')], [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    draw_cons_plot_qos(i, index, \"inbound\", \"video\", \"bitrate\", 0, 1500000, [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    draw_cons_plot_qos(i, index, \"inbound\", \"audio\", \"bitrate\", 0, 64000, [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    draw_cons_plot_qos(i, index, \"outbound\", \"video\", \"bitrate\", 0, 1500000, [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])\n",
    "    draw_cons_plot_qos(i, index, \"outbound\", \"audio\", \"bitrate\", 0, 64000, [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
