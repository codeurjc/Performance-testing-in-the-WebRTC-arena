{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import orjson\n",
    "from collections import Counter\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [25, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_names = [\n",
    "    \"loadtest-webrtc-2024-kurento-2p\",\n",
    "    \"loadtest-webrtc-2024-kurento-8p\",\n",
    "    \"loadtest-webrtc-2024-kurento-3p-10s\",\n",
    "    \"loadtest-webrtc-2024-kurento-3p-40s\",\n",
    "    \"loadtest-webrtc-2024-pion-2p\",\n",
    "    \"loadtest-webrtc-2024-pion-8p\",\n",
    "    \"loadtest-webrtc-2024-pion-3p-10s\",\n",
    "    \"loadtest-webrtc-2024-pion-3p-40s\",\n",
    "]\n",
    "\n",
    "index_list_full_names = [\n",
    "    \"Kurento, 2 publishers per session\",\n",
    "    \"Kurento, 8 publishers per session\",\n",
    "    \"Kurento, 3 publishers and 10 subscribers per session\",\n",
    "    \"Kurento, 3 publishers and 40 subscribers per session\",\n",
    "    \"Pion, 2 publishers per session\",\n",
    "    \"Pion, 8 publishers per session\",\n",
    "    \"Pion, 3 publishers and 10 subscribers per session\",\n",
    "    \"Pion, 3 publishers and 40 subscribers per session\",\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            return orjson.loads(f.read())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_index_data(index_list_name, data_type):\n",
    "    full_user_data = []\n",
    "\n",
    "    for entry in os.scandir(f'stats/{index_list_name}'):\n",
    "        if entry.is_dir():\n",
    "            for sub_entry in os.scandir(entry.path):\n",
    "                if sub_entry.is_dir():\n",
    "                    data_path = os.path.join(sub_entry.path, data_type + '.json')\n",
    "                    if os.path.exists(data_path):\n",
    "                        data = read_file(data_path)\n",
    "                        full_user_data.append({\n",
    "                            'user': sub_entry.name,\n",
    "                            'session': entry.name,\n",
    "                            data_type: data\n",
    "                        })\n",
    "    return full_user_data\n",
    "\n",
    "\n",
    "def is_publisher(index, user):\n",
    "    return (\"8p\" in index) or (user <= 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_data(user_data):\n",
    "    data = []\n",
    "    for user in user_data:\n",
    "        for event in user['events']:\n",
    "            data.append({\n",
    "                'user': int(user['user'].replace(\"User\", \"\")),\n",
    "                'session': int(user['session'].replace(\"LoadTestSession\", \"\")),\n",
    "                'event': event['event'] + (f\"-{event['connection']}\" if 'connection' in event else \"\"),\n",
    "                'connection': event['connection'] if 'connection' in event else None,\n",
    "                'timestamp': event['timestamp'],\n",
    "                'publishers': 0,\n",
    "                'subscribers': 0,\n",
    "                'streams_in': 0,\n",
    "                'streams_out': 0\n",
    "            })\n",
    "    return data\n",
    "\n",
    "def process_index_list(index_list_name):\n",
    "    user_data = get_index_data(index_list_name, \"events\")\n",
    "    data = process_user_data(user_data)\n",
    "    events_df = pd.DataFrame(data)\n",
    "    events_df['timestamp'] = pd.to_datetime(events_df['timestamp'])\n",
    "    events_df = events_df.sort_values(by='timestamp')\n",
    "    return index_list_name, events_df\n",
    "\n",
    "events_dfs = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = {executor.submit(process_index_list, index_list_name): index_list_name for index_list_name in index_list_names}\n",
    "    for future in as_completed(futures):\n",
    "        index_list_name, events_df = future.result()\n",
    "        events_dfs[index_list_name] = events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to(array, number, user, session, timestamp):\n",
    "    array.append({\n",
    "        \"number\": number,\n",
    "        \"user\": user,\n",
    "        \"session\": session,\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "full_publishers_progression = {}\n",
    "full_subscribers_progression = {}\n",
    "full_streams_in_progression = {}\n",
    "full_streams_out_progression = {}\n",
    "def process_events_df(index_list_name, events_df):\n",
    "    sessions = events_df['session'].unique()\n",
    "    publishers_progression = []\n",
    "    subscribers_progression = []\n",
    "    streams_in_progression = []\n",
    "    streams_out_progression = []\n",
    "\n",
    "    previous_data = {'publishers': 0, 'subscribers': 0, 'streams_in': 0, 'streams_out': 0}\n",
    "    for session in sessions:\n",
    "        previous_data[session] = {'publishers': 0, 'subscribers': 0}\n",
    "\n",
    "    for i, row in events_df.iterrows():\n",
    "        session = row['session']\n",
    "        user = row['user']\n",
    "        event = row['event']\n",
    "        connection = row['connection']\n",
    "\n",
    "        if event == 'streamCreated' and connection == 'local':\n",
    "            if \"0s\" in index_list_name and user > 3:\n",
    "                subscribers = previous_data['subscribers'] + 1\n",
    "                publishers_in_session = previous_data[session]['publishers']\n",
    "                subscribers_in_session = previous_data[session]['subscribers'] + 1\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['subscribers'] = subscribers\n",
    "                previous_data[session]['subscribers'] = subscribers_in_session\n",
    "            else:\n",
    "                publishers = previous_data['publishers'] + 1\n",
    "                streams_in = previous_data['streams_in'] + 2\n",
    "                publishers_in_session = previous_data[session]['publishers'] + 1\n",
    "                subscribers_in_session = previous_data[session]['subscribers']\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['publishers'] = publishers\n",
    "                previous_data['streams_in'] = streams_in\n",
    "                previous_data[session]['publishers'] = publishers_in_session\n",
    "\n",
    "            previous_data['streams_out'] = streams_out\n",
    "\n",
    "            add_to(publishers_progression, previous_data['publishers'], user, session, row['timestamp'])\n",
    "            add_to(subscribers_progression, previous_data['subscribers'], user, session, row['timestamp'])\n",
    "            add_to(streams_in_progression, previous_data['streams_in'], user, session, row['timestamp'])\n",
    "            add_to(streams_out_progression, previous_data['streams_out'], user, session, row['timestamp'])\n",
    "\n",
    "        elif event == 'streamDestroyed' and connection == 'local':\n",
    "            if \"0s\" in index_list_name and user > 3:\n",
    "                subscribers = previous_data['subscribers'] - 1\n",
    "                publishers_in_session = previous_data[session]['publishers']\n",
    "                subscribers_in_session = previous_data[session]['subscribers'] - 1\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['subscribers'] = subscribers\n",
    "                previous_data[session]['subscribers'] = subscribers_in_session\n",
    "            else:\n",
    "                publishers = previous_data['publishers'] - 1\n",
    "                streams_in = previous_data['streams_in'] - 2\n",
    "                publishers_in_session = previous_data[session]['publishers'] - 1\n",
    "                subscribers_in_session = previous_data[session]['subscribers']\n",
    "                streams_out = 2 * publishers_in_session * (publishers_in_session - 1) + 2 * subscribers_in_session * publishers_in_session\n",
    "\n",
    "                previous_data['publishers'] = publishers\n",
    "                previous_data['streams_in'] = streams_in\n",
    "                previous_data[session]['publishers'] = publishers_in_session\n",
    "\n",
    "            previous_data['streams_out'] = streams_out\n",
    "\n",
    "            add_to(publishers_progression, previous_data['publishers'], user, session, row['timestamp'])\n",
    "            add_to(subscribers_progression, previous_data['subscribers'], user, session, row['timestamp'])\n",
    "            add_to(streams_in_progression, previous_data['streams_in'], user, session, row['timestamp'])\n",
    "            add_to(streams_out_progression, previous_data['streams_out'], user, session, row['timestamp'])\n",
    "        \n",
    "        else:\n",
    "            events_df.at[i, 'publishers'] = previous_data['publishers']\n",
    "            events_df.at[i, 'subscribers'] = previous_data['subscribers']\n",
    "            events_df.at[i, 'streams_in'] = previous_data['streams_in']\n",
    "            events_df.at[i, 'streams_out'] = previous_data['streams_out']\n",
    "\n",
    "    publishers_progression = pd.DataFrame(publishers_progression)\n",
    "    subscribers_progression = pd.DataFrame(subscribers_progression)\n",
    "    streams_in_progression = pd.DataFrame(streams_in_progression)\n",
    "    streams_out_progression = pd.DataFrame(streams_out_progression)\n",
    "\n",
    "    return (index_list_name, publishers_progression, subscribers_progression, streams_in_progression, streams_out_progression)\n",
    "\n",
    "# Use parallel processing to handle events dataframes\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = [executor.submit(process_events_df, index_list_name, events_dfs[index_list_name]) for index_list_name in index_list_names]\n",
    "    for future in as_completed(futures):\n",
    "        (index_list_name, publishers_progression, subscribers_progression, streams_in_progression, streams_out_progression) = future.result()\n",
    "        full_publishers_progression[index_list_name] = publishers_progression\n",
    "        full_subscribers_progression[index_list_name] = subscribers_progression\n",
    "        full_streams_in_progression[index_list_name] = streams_in_progression\n",
    "        full_streams_out_progression[index_list_name] = streams_out_progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log file and get specific lines with regex\n",
    "import re\n",
    "def process_log_file(log_file):\n",
    "    regex = re.compile(r'(.*) ERROR .* Participant (.*) in session (.*) failed (\\d+) times')\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S.%f %z\"\n",
    "    failed_attempts = []\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Participant' in line:\n",
    "                match = regex.search(line)\n",
    "                if match:\n",
    "                    failed_attempts.append({\n",
    "                        'user': match.group(2).replace('User', ''),\n",
    "                        'session': match.group(3).replace('LoadTestSession', ''),\n",
    "                        'attempts': int(match.group(4)),  # Ensure attempts is an integer\n",
    "                        'timestamp': datetime.strptime(match.group(1) + \" +0200\", date_format),\n",
    "                        'full_user': f\"{match.group(2).replace('User', '')}-{match.group(3).replace('LoadTestSession', '')}\",\n",
    "                    })\n",
    "    return log_file, pd.DataFrame(failed_attempts)\n",
    "\n",
    "failures = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = {executor.submit(process_log_file, f'logs/{index_list_name}.log'): index_list_name for index_list_name in index_list_names}\n",
    "    for future in as_completed(futures):\n",
    "        log_file, df = future.result()\n",
    "        index_list_name = log_file.split('/')[-1].replace('.log', '')\n",
    "        failures[index_list_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test, events_df in events_dfs.items():\n",
    "#     events_df = events_df.sort_values(by=[\"session\", \"user\", \"timestamp\"])\n",
    "#     fig, ax = plt.subplots()\n",
    "#     event_types = events_df['event'].unique()\n",
    "#     event_types.sort()\n",
    "#     colors = mpl.colormaps.get_cmap('tab20')(np.linspace(0, 1, len(event_types)))\n",
    "#     event_color = dict(zip(event_types, colors))\n",
    "#     y_labels = []\n",
    "#     y_ticks = 0\n",
    "#     for session in events_dfs[test]['session'].unique():\n",
    "#         events_df_session = events_df[events_df['session'] == session]\n",
    "#         for user in events_df_session['user'].unique():\n",
    "#             events_df_user = events_df_session[events_df_session['user'] == user]\n",
    "#             y_ticks += 1\n",
    "#             y_pos = [y_ticks] * len(events_df_user)\n",
    "#             ax.scatter(events_df_user['timestamp'], y_pos, c=[event_color[event] for event in events_df_user['event']])\n",
    "#             y_labels.append(f'{user} - {session}')\n",
    "\n",
    "#     # add legend\n",
    "#     legend_labels = []\n",
    "#     for event, color in event_color.items():\n",
    "#         legend_labels.append(plt.Line2D([0], [0], marker='o', color='w', label=event, markerfacecolor=color))\n",
    "#     ax.legend(handles=legend_labels, loc='upper left')\n",
    "#     fig.suptitle(f'{test}')\n",
    "#     # y labels should be user - session\n",
    "#     ax.set_yticks(range(1, y_ticks + 1))\n",
    "#     ax.set_yticklabels(y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test in events_dfs.keys():\n",
    "#     events_df = events_dfs[test]\n",
    "#     events_df = events_df.sort_values(by=[\"session\", \"user\", \"timestamp\"])\n",
    "#     # events_df = events_df[\n",
    "#     #     (events_df['event'] == 'connectionStart') |\n",
    "#     #     # (events_df['event'] == 'signalConnected') |\n",
    "#     #     # (events_df['event'] == 'connectedPublisher') |\n",
    "#     #     ((events_df['event'] == 'streamCreated') & (events_df['connection'] == 'local')) |\n",
    "#     #     ((events_df['event'] == 'streamDestroyed') & (events_df['connection'] == 'local')) |\n",
    "#     #     (events_df['event'] == 'connectionEnd')\n",
    "#     # ]\n",
    "#     events_df = events_df[\n",
    "#         ~(events_df['event'] == 'publisherStartSpeaking-local') & ~(events_df['event'] == 'publisherStopSpeaking-local') &\n",
    "#         ~(events_df['event'] == 'accessAllowed-local')\n",
    "#     ]\n",
    "#     fig, ax = plt.subplots()\n",
    "#     event_types = events_df['event'].unique()\n",
    "#     event_types.sort()\n",
    "#     colors = mpl.colormaps.get_cmap('tab20')(np.linspace(0, 1, len(event_types)))\n",
    "#     event_color = dict(zip(event_types, colors))\n",
    "#     y_labels = []\n",
    "#     y_ticks = 0\n",
    "#     for session in events_df['session'].unique():\n",
    "#         events_df_session = events_df[events_df['session'] == session]\n",
    "#         for user in events_df_session['user'].unique():\n",
    "#             events_df_user = events_df_session[events_df_session['user'] == user]\n",
    "#             y_ticks += 1\n",
    "#             y_pos = [y_ticks] * len(events_df_user)\n",
    "#             ax.scatter(events_df_user['timestamp'], y_pos, c=[event_color[event] for event in events_df_user['event']])\n",
    "#             y_labels.append(f'{user} - {session}')\n",
    "\n",
    "\n",
    "#     # add legend\n",
    "#     legend_labels = []\n",
    "#     for event, color in event_color.items():\n",
    "#         legend_labels.append(plt.Line2D([0], [0], marker='o', color='w', label=event, markerfacecolor=color))\n",
    "#     ax.legend(handles=legend_labels, loc='upper left')\n",
    "#     fig.suptitle(f'{test}')\n",
    "#     # y labels should be user - session\n",
    "#     ax.set_yticks(range(1, y_ticks + 1))\n",
    "#     ax.set_yticklabels(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpu_list = []\n",
    "for index in index_list_names:\n",
    "    df_cpu = pd.read_csv(f\"dfs_final/{index}-medianode.csv\")\n",
    "    df_cpu = df_cpu.drop(columns=[\"memory\"]).dropna()\n",
    "    df_cpu[\"@timestamp\"] = pd.to_datetime(df_cpu[\"@timestamp\"], format=\"ISO8601\")\n",
    "    df_cpu_list.append(df_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_qoe_df_list = []\n",
    "for index in index_list_names:\n",
    "    index_qoe_df = pd.DataFrame()\n",
    "    for entry in os.scandir(f\"qoe/{index}\"):\n",
    "        if entry.is_file():\n",
    "            with open(f\"qoe/{index}/{entry.name}\", 'r') as f:\n",
    "                qoe = orjson.loads(f.read())\n",
    "            qoe_df = pd.DataFrame.from_dict(qoe)\n",
    "            info = entry.name.split(\"_\")[0].split(\"-\")\n",
    "            session = info[1]\n",
    "            userRecording = info[2]\n",
    "            userBeingRecorded = info[3]\n",
    "            qoe_df[\"userRecording\"] = userRecording\n",
    "            qoe_df[\"userBeingRecorded\"] = userBeingRecorded\n",
    "            qoe_df[\"session\"] = session\n",
    "            if index_qoe_df.empty:\n",
    "                index_qoe_df = qoe_df\n",
    "            else:\n",
    "                index_qoe_df = pd.concat([index_qoe_df, qoe_df])\n",
    "    index_qoe_df_list.append(index_qoe_df)\n",
    "\n",
    "\n",
    "seconds_per_fragment = 17\n",
    "seconds_per_padding = 2\n",
    "seconds_per_video = seconds_per_fragment - seconds_per_padding\n",
    "def set_qoe_times(df, user_1_start_connection_time, user_2_start_connection_time):\n",
    "    cut_index_0_time = 5 # recording stats 5 seconds after connection is successful\n",
    "    max_time = max(user_1_start_connection_time, user_2_start_connection_time)\n",
    "    time_fn = lambda cut_index: max_time + timedelta(seconds=(cut_index * seconds_per_fragment) + cut_index_0_time).total_seconds()\n",
    "    df.loc[:, \"timestamp\"] = df[\"cut_index\"].apply(time_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"dfs_final/batches.json\", 'r') as f:\n",
    "    batches = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_obj = {}\n",
    "\n",
    "def get_connection_times(events):\n",
    "    connection_times_recorded = []\n",
    "    recorded_media_events = events[\n",
    "        (events[\"event\"] != \"connectionStart\") &\n",
    "        (events[\"event\"] != \"connectionFail\")\n",
    "    ]\n",
    "    for m, row in recorded_media_events.iterrows():\n",
    "        timestamp = row[\"timestamp\"]\n",
    "        con_time = (timestamp - start_time).total_seconds()\n",
    "        connection_times_recorded.append(con_time)\n",
    "    return connection_times_recorded\n",
    "\n",
    "\n",
    "def get_failure_times(events):\n",
    "    connection_times_recorded = []\n",
    "    recorded_media_events = events[\n",
    "        events[\"event\"] == \"connectionFail\"\n",
    "    ]\n",
    "    for m, row in recorded_media_events.iterrows():\n",
    "        timestamp = row[\"timestamp\"]\n",
    "        con_time = (timestamp - start_time).total_seconds()\n",
    "        connection_times_recorded.append(con_time)\n",
    "    return connection_times_recorded\n",
    "\n",
    "\n",
    "connection_stats_global = pd.DataFrame()\n",
    "df_cpu_filtered_list = []\n",
    "start_end_times = {}\n",
    "for i, index_list_name in enumerate(index_list_names):\n",
    "    #stats_df = get_index_data(index_list_name, \"stats\")\n",
    "    events_df = events_dfs[index_list_name]\n",
    "    events_df = events_df.sort_values(by=[\"session\", \"user\", \"timestamp\"])\n",
    "    start_time = events_df[\"timestamp\"].min()\n",
    "    end_time = events_df[\"timestamp\"].max()\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    start_end_times[index_list_name] = (start_time, end_time)\n",
    "    cpu = df_cpu_list[i]\n",
    "    cpu_filtered = cpu[(cpu[\"@timestamp\"] >= start_time) & (cpu[\"@timestamp\"] <= end_time)]\n",
    "    cpu_filtered.loc[:, \"@timestamp\"] = cpu_filtered[\"@timestamp\"].apply(lambda x: (x - start_time).total_seconds())\n",
    "    cpu_filtered = cpu_filtered.sort_values(by=\"@timestamp\")\n",
    "    df_cpu_filtered_list.append(cpu_filtered)\n",
    "    index_qoe_df = index_qoe_df_list[i]\n",
    "    sessions = events_df[\"session\"].unique()\n",
    "    user_events_obj[index_list_name] = {}\n",
    "    for session in sessions:\n",
    "        users = events_df[events_df[\"session\"] == session][\"user\"].unique()\n",
    "        for user in users:\n",
    "            full_user = f\"{user}-{session}\"\n",
    "            user_events = events_df[\n",
    "                (events_df[\"session\"] == session) & (events_df[\"user\"] == user)\n",
    "            ]\n",
    "            connection_starts = user_events[user_events[\"event\"] == \"connectionStart\"]\n",
    "            failures_user = failures[index_list_name][\n",
    "                (failures[index_list_name][\"full_user\"] == full_user) &\n",
    "                (failures[index_list_name][\"timestamp\"] >= start_time) &\n",
    "                (failures[index_list_name][\"timestamp\"] <= end_time)\n",
    "            ].copy()\n",
    "            failures_user[\"event\"] = \"connectionFail\"\n",
    "            failures_user[\"recoverable\"] = False\n",
    "            if \"kurento\" in index_list_name:\n",
    "                if is_publisher(index_list_name, user):\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"] == \"streamCreated-local\"\n",
    "                    ]\n",
    "                    stream_playing_locals = user_events[\n",
    "                        user_events[\"event\"] == \"streamPlaying-local\"\n",
    "                    ]\n",
    "                else:\n",
    "                    # we choose the first stream created as confirmation of media connection if the user is subscriber\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"].str.startswith(\"streamCreated\")\n",
    "                    ]\n",
    "                    stream_playing_locals = user_events[\n",
    "                        user_events[\"event\"].str.startswith(\"streamPlaying\")\n",
    "                    ]\n",
    "                # combine connection starts and stream created locals and failures_user\n",
    "                combined = pd.concat(\n",
    "                    [\n",
    "                        connection_starts,\n",
    "                        stream_created_locals,\n",
    "                        stream_playing_locals,\n",
    "                        failures_user,\n",
    "                    ]\n",
    "                )\n",
    "                # if there is more than one stream created and a stream playing, we only keep the first one (marks connection success)\n",
    "                combined = combined.sort_values(by=\"timestamp\")\n",
    "                prev_row_event = \"\"\n",
    "                for r, row in combined.iterrows():\n",
    "                    is_prev_row_connected = prev_row_event.startswith(\"streamCreated\") or prev_row_event.startswith(\"streamPlaying\")\n",
    "                    is_current_row_connected = row[\"event\"].startswith(\"streamCreated\") or row[\"event\"].startswith(\"streamPlaying\")\n",
    "                    if is_prev_row_connected and is_current_row_connected:\n",
    "                        combined = combined.drop(r)\n",
    "                    prev_row_event = row[\"event\"]\n",
    "            else:\n",
    "                reconnecting_events = user_events[user_events[\"event\"].str.contains(\"Reconnecting\")].copy()\n",
    "                if not reconnecting_events.empty:\n",
    "                    reconnecting_events[\"event\"] = \"connectionFail\"\n",
    "                    reconnecting_events[\"recoverable\"] = True\n",
    "                    failures_user = pd.concat([failures_user, reconnecting_events])\n",
    "                # combine connection starts and stream created locals and failures_user\n",
    "                if is_publisher(index_list_name, user):\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"] == \"LocalTrackPublished-local\"\n",
    "                    ]\n",
    "                    stream_created_locals = stream_created_locals.sort_values(by=\"timestamp\")\n",
    "                    # The second one means the publisher has fully connected\n",
    "                    stream_created_locals = stream_created_locals.iloc[::2, :]\n",
    "                else:\n",
    "                    # we choose the first stream created as confirmation of media connection if the user is subscriber\n",
    "                    stream_created_locals = user_events[\n",
    "                        user_events[\"event\"].str.startswith(\"TrackSubscribed\")\n",
    "                    ]\n",
    "                # combine connection starts and stream created locals and failures_user\n",
    "                combined = pd.concat(\n",
    "                    [connection_starts, stream_created_locals, failures_user]\n",
    "                )\n",
    "                combined = combined.sort_values(by=\"timestamp\")\n",
    "                for r, row in combined.iterrows():\n",
    "                    is_prev_row_connected = prev_row_event.startswith(\"TrackSubscribed\")\n",
    "                    is_current_row_connected = row[\"event\"].startswith(\"TrackSubscribed\")\n",
    "                    if is_prev_row_connected and is_current_row_connected:\n",
    "                        combined = combined.drop(r)\n",
    "                    prev_row_event = row[\"event\"]\n",
    "            index_qoe_df_filtered = index_qoe_df[\n",
    "                (index_qoe_df[\"session\"] == \"LoadTestSession\" + str(session)) &\n",
    "                (index_qoe_df[\"userRecording\"] == \"User\" + str(user))\n",
    "            ].copy()\n",
    "            index_qoe_df_filtered = index_qoe_df_filtered.sort_values(by=\"cut_index\")\n",
    "            user_events_obj[index_list_name][full_user] = {}\n",
    "            user_events_obj[index_list_name][full_user][\"combined\"] = combined\n",
    "            user_events_obj[index_list_name][full_user][\"qoe\"] = index_qoe_df_filtered\n",
    "            user_events_obj[index_list_name][full_user][\"user\"] = user\n",
    "            user_events_obj[index_list_name][full_user][\"session\"] = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection_time_intervals(user_events, start_time, test_duration):\n",
    "    connection_time_intervals = {}\n",
    "    users = user_events.keys()\n",
    "    for username in users:\n",
    "        group = user_events[username]['combined'].copy()\n",
    "        group[\"timestamp\"] = group[\"timestamp\"].apply(lambda x: (x - start_time).total_seconds())\n",
    "        connection_success_times = group.loc[~((group['event'] == 'connectionFail') | (group['event'] == 'connectionStart'))]['timestamp']\n",
    "        connection_fail_times = group.loc[group['event'] == 'connectionFail']['timestamp']\n",
    "\n",
    "        intervals = []\n",
    "        for success_time in connection_success_times:\n",
    "            fail_time = connection_fail_times[connection_fail_times > success_time].min()\n",
    "            if pd.isnull(fail_time):\n",
    "                intervals.append((success_time, test_duration))\n",
    "            else:\n",
    "                intervals.append((success_time, min(fail_time, test_duration)))\n",
    "\n",
    "        connection_time_intervals[username] = intervals\n",
    "\n",
    "    return connection_time_intervals\n",
    "\n",
    "def intersect_intervals(intervals1, intervals2):\n",
    "    intersection = []\n",
    "    for interval1 in intervals1:\n",
    "        for interval2 in intervals2:\n",
    "            start = max(interval1[0], interval2[0])\n",
    "            end = min(interval1[1], interval2[1])\n",
    "            if start < end:\n",
    "                intersection.append((start, end))\n",
    "    return intersection\n",
    "\n",
    "intersections = {}\n",
    "connection_time_intervals = {}\n",
    "for index_list_name in index_list_names:\n",
    "    intersections[index_list_name] = {}\n",
    "    start_time, end_time = start_end_times[index_list_name]\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    connection_time_intervals_index = get_connection_time_intervals(user_events_obj[index_list_name], start_time, test_duration)\n",
    "    connection_time_intervals[index_list_name] = connection_time_intervals_index\n",
    "    usernames = list(connection_time_intervals_index.keys())\n",
    "    for i in range(len(usernames)):\n",
    "        for j in range(i + 1, len(usernames)):\n",
    "            _, session1 = usernames[i].split(\"-\")\n",
    "            _, session2 = usernames[j].split(\"-\")\n",
    "            if session1 == session2:\n",
    "                user1_intervals = connection_time_intervals_index[usernames[i]]\n",
    "                user2_intervals = connection_time_intervals_index[usernames[j]]\n",
    "                intersection = intersect_intervals(user1_intervals, user2_intervals)\n",
    "                if not usernames[i] in intersections[index_list_name]:\n",
    "                    intersections[index_list_name][usernames[i]] = {}\n",
    "                intersections[index_list_name][usernames[i]][usernames[j]] = intersection\n",
    "                if not usernames[j] in intersections[index_list_name]:\n",
    "                    intersections[index_list_name][usernames[j]] = {}\n",
    "                intersections[index_list_name][usernames[j]][usernames[i]] = intersection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [40, 35]\n",
    "\n",
    "colors_list = [\"black\", \"orange\", \"yellow\", \"green\", \"darkblue\"]\n",
    "bounds = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "norm_bounds = [(b - min(bounds)) / (max(bounds) - min(bounds) ) for b in bounds]\n",
    "success_cmap = mcolors.LinearSegmentedColormap.from_list(\"success\", list(zip(norm_bounds, colors_list)))\n",
    "normalize = mcolors.Normalize(vmin=min(bounds), vmax=max(bounds))\n",
    "\n",
    "def get_con_type(next_color):\n",
    "    if next_color == \"magenta\":\n",
    "        return \"connectionStart\"\n",
    "    elif next_color == \"green\":\n",
    "        return \"connectionSuccess\"\n",
    "    elif next_color == \"red\":\n",
    "        return \"connectionFail\"\n",
    "\n",
    "def calc_stats(cpu_filtered, prev_time, current_time, prev_attempt, n_batch, user, session, full_user, next_color, connection_stats):\n",
    "    total_duration = current_time - prev_time\n",
    "    cpu_subset = cpu_filtered[(cpu_filtered['@timestamp'] >= prev_time) & (cpu_filtered['@timestamp'] <= current_time)]\n",
    "    con_type = get_con_type(next_color)\n",
    "    stats = {\n",
    "        \"type\": con_type,\n",
    "        \"start_time\": prev_time,\n",
    "        \"end_time\": current_time,\n",
    "        \"total_duration\": total_duration,\n",
    "        \"attempt\": prev_attempt,\n",
    "        \"batch\": n_batch,\n",
    "        \"user\": user,\n",
    "        \"session\": session,\n",
    "        \"full_user\": full_user\n",
    "    }\n",
    "    # if there is no cpu data, add the next cpu data available\n",
    "    if cpu_subset.empty:\n",
    "        # seek closest cpu data to start_time or end_time\n",
    "        cpu_prev_df = cpu_filtered[cpu_filtered['@timestamp'] < prev_time]\n",
    "        cpu_next_df = cpu_filtered[cpu_filtered['@timestamp'] > current_time]\n",
    "        # if one of the dfs is empty, we take the other\n",
    "        if cpu_prev_df.empty:\n",
    "            cpu_next = cpu_next_df.iloc[0]\n",
    "            cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_next['@timestamp']]\n",
    "        elif cpu_next_df.empty:\n",
    "            cpu_prev = cpu_prev_df.iloc[-1]\n",
    "            cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_prev['@timestamp']]\n",
    "        else:\n",
    "            cpu_next = cpu_next_df.iloc[0]\n",
    "            cpu_prev = cpu_prev_df.iloc[-1]\n",
    "            cpu_prev_diff = prev_time - cpu_prev['@timestamp']\n",
    "            cpu_next_diff = cpu_next['@timestamp'] - current_time\n",
    "            if cpu_prev_diff < cpu_next_diff:\n",
    "                cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_prev['@timestamp']]\n",
    "            else:\n",
    "                cpu_subset = cpu_filtered[cpu_filtered['@timestamp'] == cpu_next['@timestamp']]\n",
    "    stats[\"cpu_mean\"] = cpu_subset['cpu'].mean()\n",
    "    stats[\"cpu_start\"] = cpu_subset['cpu'].iloc[0]\n",
    "    stats[\"cpu_end\"] = cpu_subset['cpu'].iloc[-1]\n",
    "    stats[\"cpu_median\"] = cpu_subset['cpu'].median()\n",
    "    stats[\"cpu_max\"] = cpu_subset['cpu'].max()\n",
    "    stats[\"cpu_min\"] = cpu_subset['cpu'].min()\n",
    "    connection_stats.append(stats)\n",
    "    return connection_stats\n",
    "\n",
    "def paint_bar(prev_time, current_time, y_ticks, prev_state, next_color):\n",
    "    total_duration = current_time - prev_time\n",
    "    yrange = (y_ticks - 0.5, 0.8)\n",
    "    if prev_state != \"connectionSuccess\":\n",
    "        segment = [(prev_time, total_duration)]\n",
    "        ax.broken_barh(segment, yrange, facecolors=next_color)\n",
    "\n",
    "def paint_success_bar(prev_time, current_time, y_ticks):\n",
    "    total_duration = current_time - prev_time\n",
    "    yrange = (y_ticks - 0.5, 0.8)\n",
    "    segment = [(prev_time, total_duration)]\n",
    "    ax.broken_barh(segment, yrange, facecolors=success_cmap(norm_bounds[0]))\n",
    "\n",
    "for i, index_list_name in enumerate(index_list_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    y_labels = []\n",
    "    y_ticks = 0\n",
    "    user_count = 0\n",
    "    connection_stats = []\n",
    "    batch_info = batches[index_list_name]\n",
    "    cpu_filtered = df_cpu_filtered_list[i]\n",
    "    start_time, end_time = start_end_times[index_list_name]\n",
    "    test_duration = (end_time - start_time).total_seconds()\n",
    "    for full_user, full_user_obj in user_events_obj[index_list_name].items():\n",
    "        user_count += 1\n",
    "        if user_count <= batch_info[\"startingParticipants\"]:\n",
    "            n_batch = 0\n",
    "        else:\n",
    "            n_batch = math.floor((user_count - batch_info[\"startingParticipants\"] - 1) / batch_info[\"batchSize\"]) + 1\n",
    "        user_split = full_user.split(\"-\")\n",
    "        user = user_split[0]\n",
    "        session = user_split[1]\n",
    "        y_labels.append(full_user)\n",
    "        y_ticks += 1\n",
    "\n",
    "        next_color = \"white\"\n",
    "        prev_time = None\n",
    "        prev_state = None\n",
    "        prev_attempt = 0\n",
    "        combined = full_user_obj[\"combined\"]\n",
    "        qoe = full_user_obj[\"qoe\"]\n",
    "        con_segments = connection_time_intervals[index_list_name][full_user]\n",
    "        user_intersections = intersections[index_list_name][full_user]\n",
    "        # connection success\n",
    "        for segment in con_segments:\n",
    "            paint_success_bar(segment[0], segment[1], y_ticks)\n",
    "        # other connection events\n",
    "        for r, row in combined.iterrows():\n",
    "            current_time = (row[\"timestamp\"] - start_time).total_seconds()\n",
    "            if prev_time is not None:\n",
    "                paint_bar(prev_time, current_time, y_ticks, prev_state, next_color)\n",
    "                calc_stats(cpu_filtered, prev_time, current_time, prev_attempt, n_batch, user, session, full_user, next_color, connection_stats)\n",
    "            connectionSuccessful = (\n",
    "                row[\"event\"].startswith(\"streamCreated\")\n",
    "                or row[\"event\"].startswith(\"streamPlaying\")\n",
    "                or row[\"event\"].startswith(\"LocalTrackPublished\")\n",
    "                or row[\"event\"].startswith(\"TrackSubscribed\")\n",
    "            )\n",
    "            if row[\"event\"] == \"connectionStart\":\n",
    "                next_color = \"magenta\"\n",
    "                prev_state = \"connectionStart\"\n",
    "            elif connectionSuccessful:\n",
    "                next_color = \"green\"\n",
    "                prev_state = \"connectionSuccess\"\n",
    "            else:\n",
    "                next_color = \"red\"\n",
    "                prev_attempt = row[\"attempts\"]\n",
    "                prev_state = \"connectionFail\"\n",
    "            prev_time = current_time\n",
    "        if prev_time is not None:\n",
    "            paint_bar(prev_time, test_duration, y_ticks, prev_state, next_color)\n",
    "            calc_stats(cpu_filtered, prev_time, test_duration, prev_attempt, n_batch, user, session, full_user, next_color, connection_stats)\n",
    "    connection_stats_df = pd.DataFrame(connection_stats)\n",
    "    # add all rows to connection_stats_global\n",
    "    if connection_stats_global.empty:\n",
    "        connection_stats_global = connection_stats_df\n",
    "        connection_stats_global[\"index\"] = index_list_name\n",
    "    else:\n",
    "        connection_stats_df[\"index\"] = index_list_name\n",
    "        connection_stats_global = pd.concat([connection_stats_global, connection_stats_df])\n",
    "\n",
    "    ax.set_yticks(range(1, y_ticks + 1))\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_title(f\"Connection Progression - {index_list_full_names[i]}\", pad=10)\n",
    "    # y axis label\n",
    "    ax.set_ylabel(\"User - Session\")\n",
    "    if user_count >= 150:\n",
    "        # remove y labels\n",
    "        ax.set_yticklabels([])\n",
    "    # x axis label\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xlim(-5, test_duration + 5)\n",
    "    # grid\n",
    "    ax.grid(True)\n",
    "    # x axis ticks every 10 seconds\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    # cpu in new axis\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel(\"CPU (%)\")\n",
    "    # ax2 y axis should go from 0 to 1\n",
    "    ax2.set_ylim(0, 1.01)\n",
    "    ax2.set_yticks(np.arange(0, 1.01, 0.05))\n",
    "    ax2.plot(cpu_filtered[\"@timestamp\"], cpu_filtered[\"cpu\"], color=\"black\", marker=\"o\", linewidth=4, markersize=10)\n",
    "    # create legend\n",
    "    legend_labels = [\n",
    "        plt.Line2D([0], [0], color=\"magenta\", lw=4, label=\"Connection Start\"),\n",
    "        plt.Line2D([0], [0], color=\"indigo\", lw=4, label=\"Stream Created/Playing, No Video\"),\n",
    "        plt.Line2D([0], [0], color=\"darkblue\", lw=4, label=\"Excellent QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"green\", lw=4, label=\"Good QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"yellow\", lw=4, label=\"Fair QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"orange\", lw=4, label=\"Poor QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"black\", lw=4, label=\"Bad QoE\"),\n",
    "        plt.Line2D([0], [0], color=\"red\", lw=4, label=\"Failed Connection\"),\n",
    "        plt.Line2D([0], [0], color=\"black\", lw=4, label=\"CPU\", marker=\"o\", markersize=10)\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_labels, loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_start_durations = []\n",
    "def calculate_connection_start_data(connection_stats):\n",
    "    connection_stats = connection_stats[connection_stats[\"type\"] == \"connectionStart\"].drop(columns=[\"type\", \"index\", \"user\", \"session\"])\n",
    "    # connection_stats = connection_stats.sort_values(by=\"start_time\").reset_index().drop(columns=[\"index\"])\n",
    "    connection_stats = connection_stats.sort_values(by=\"cpu_median\").reset_index().drop(columns=[\"index\"])\n",
    "    connection_stats = connection_stats[[\"full_user\", \"attempt\", \"batch\", \"total_duration\", \"cpu_median\", \"cpu_mean\", \"cpu_max\", \"cpu_min\"]]\n",
    "\n",
    "    # average, median, min, max total duration when cpu_median is below 0.8\n",
    "    connection_stats_filtered = connection_stats[connection_stats[\"cpu_median\"] < 0.8]\n",
    "    mean80 = connection_stats_filtered[\"total_duration\"].mean()\n",
    "    median80 = connection_stats_filtered[\"total_duration\"].median()\n",
    "    min80 = connection_stats_filtered[\"total_duration\"].min()\n",
    "    max80 = connection_stats_filtered[\"total_duration\"].max()\n",
    "    cpu_start_durations.append({\n",
    "        \"test case\": index,\n",
    "        \"threshold\": \"Median CPU < 0.8\",\n",
    "        \"median connection start duration (seconds)\": median80,\n",
    "        \"mean connection start duration (seconds)\": mean80,\n",
    "        \"min connection start duration (seconds)\": min80,\n",
    "        \"max connection start duration (seconds)\": max80\n",
    "    })\n",
    "    # average, median, min, max total duration when cpu_median is between 0.8 and 0.9\n",
    "    connection_stats_filtered = connection_stats[(connection_stats[\"cpu_median\"] >= 0.8) & (connection_stats[\"cpu_median\"] < 0.9)]\n",
    "    mean89 = connection_stats_filtered[\"total_duration\"].mean()\n",
    "    median89 = connection_stats_filtered[\"total_duration\"].median()\n",
    "    min89 = connection_stats_filtered[\"total_duration\"].min()\n",
    "    max89 = connection_stats_filtered[\"total_duration\"].max()\n",
    "    cpu_start_durations.append({\n",
    "        \"test case\": index,\n",
    "        \"threshold\": \"0.8 <= Median CPU < 0.9\",\n",
    "        \"median connection start duration (seconds)\": median89,\n",
    "        \"mean connection start duration (seconds)\": mean89,\n",
    "        \"min connection start duration (seconds)\": min89,\n",
    "        \"max connection start duration (seconds)\": max89\n",
    "    })\n",
    "    # average, median, min, max total duration when cpu_median is between 0.9 and 1\n",
    "    connection_stats_filtered = connection_stats[(connection_stats[\"cpu_median\"] >= 0.9) & (connection_stats[\"cpu_median\"] <= 1)]\n",
    "    mean90 = connection_stats_filtered[\"total_duration\"].mean()\n",
    "    median90 = connection_stats_filtered[\"total_duration\"].median()\n",
    "    min90 = connection_stats_filtered[\"total_duration\"].min()\n",
    "    max90 = connection_stats_filtered[\"total_duration\"].max()\n",
    "    cpu_start_durations.append({\n",
    "        \"test case\": index,\n",
    "        \"threshold\": \"0.9 <= Median CPU <= 1\",\n",
    "        \"median connection start duration (seconds)\": median90,\n",
    "        \"mean connection start duration (seconds)\": mean90,\n",
    "        \"min connection start duration (seconds)\": min90,\n",
    "        \"max connection start duration (seconds)\": max90\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test estadÃ­sticos (Wilconxon)\n",
    "# Connection starts\n",
    "for index in index_list_names:\n",
    "        connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index]\n",
    "        calculate_connection_start_data(connection_stats)\n",
    "\n",
    "cpu_start_durations_df = pd.DataFrame(cpu_start_durations)\n",
    "display(cpu_start_durations_df)\n",
    "cpu_start_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection start of successful connections\n",
    "for index in index_list_names:\n",
    "    connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index]\n",
    "    # remove all connectionStart that are not followed by a connectionSuccess for that full_user and attempt\n",
    "    for i, row in connection_stats.iterrows():\n",
    "        if row[\"type\"] == \"connectionStart\":\n",
    "            next_row = connection_stats[\n",
    "                (connection_stats[\"full_user\"] == row[\"full_user\"])\n",
    "                & (connection_stats[\"attempt\"] == row[\"attempt\"])\n",
    "                & (connection_stats[\"start_time\"] > row[\"start_time\"])\n",
    "            ]\n",
    "            if next_row.empty or next_row.iloc[0][\"type\"] != \"connectionSuccess\":\n",
    "                connection_stats = connection_stats.drop(i)\n",
    "\n",
    "    calculate_connection_start_data(connection_stats)\n",
    "\n",
    "cpu_start_durations_df = pd.DataFrame(cpu_start_durations)\n",
    "display(cpu_start_durations_df)\n",
    "cpu_start_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_start_durations = []\n",
    "for index in index_list_names:\n",
    "    if \"10s\" in index or \"40s\" in index:\n",
    "        connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index]\n",
    "        # remove all connectionStart that are not followed by a connectionSuccess for that full_user and attempt\n",
    "        for i, row in connection_stats.iterrows():\n",
    "            if row[\"type\"] == \"connectionStart\":\n",
    "                if int(row[\"user\"]) > 3:\n",
    "                    connection_stats = connection_stats.drop(i)\n",
    "                else:\n",
    "                    next_row = connection_stats[\n",
    "                        (connection_stats[\"full_user\"] == row[\"full_user\"])\n",
    "                        & (connection_stats[\"attempt\"] == row[\"attempt\"])\n",
    "                        & (connection_stats[\"start_time\"] > row[\"start_time\"])\n",
    "                    ]\n",
    "                    if next_row.empty or next_row.iloc[0][\"type\"] != \"connectionSuccess\":\n",
    "                        connection_stats = connection_stats.drop(i)\n",
    "        calculate_connection_start_data(connection_stats)\n",
    "\n",
    "cpu_start_durations_df = pd.DataFrame(cpu_start_durations)\n",
    "display(cpu_start_durations_df)\n",
    "cpu_start_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_start_durations = []\n",
    "for index in index_list_names:\n",
    "    if \"10s\" in index or \"40s\" in index:\n",
    "        connection_stats = connection_stats_global[connection_stats_global[\"index\"] == index]\n",
    "        # remove all connectionStart that are not followed by a connectionSuccess for that full_user and attempt\n",
    "        for i, row in connection_stats.iterrows():\n",
    "            if row[\"type\"] == \"connectionStart\":\n",
    "                if int(row[\"user\"]) <= 3:\n",
    "                    connection_stats = connection_stats.drop(i)\n",
    "                else:\n",
    "                    next_row = connection_stats[\n",
    "                        (connection_stats[\"full_user\"] == row[\"full_user\"])\n",
    "                        & (connection_stats[\"attempt\"] == row[\"attempt\"])\n",
    "                        & (connection_stats[\"start_time\"] > row[\"start_time\"])\n",
    "                    ]\n",
    "                    if next_row.empty or next_row.iloc[0][\"type\"] != \"connectionSuccess\":\n",
    "                        connection_stats = connection_stats.drop(i)\n",
    "        calculate_connection_start_data(connection_stats)\n",
    "\n",
    "cpu_start_durations_df = pd.DataFrame(cpu_start_durations)\n",
    "display(cpu_start_durations_df)\n",
    "cpu_start_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 types of stats\n",
    "# webrtcStats: jitter etc.\n",
    "# inbound: qoe calculated via inbound metrics\n",
    "# outbound: qoe calculated via outbound metrics\n",
    "# at the moment we only care about webrtcStats\n",
    "\n",
    "stats_dict = {\n",
    "    \"inbound\": {},\n",
    "    \"outbound\": {},\n",
    "    \"remote_inbound\": {},\n",
    "    \"remote_outbound\": {}\n",
    "}\n",
    "\n",
    "def process_stats(index_list_name):\n",
    "    stats = get_index_data(index_list_name, \"stats\")\n",
    "    for stat in stats:\n",
    "        if 'stats' in stat:\n",
    "            in_stats = stat['stats']\n",
    "            for s in in_stats:\n",
    "                # localUser = s['user']\n",
    "                # localSession = s['session']\n",
    "                if 'webrtcStats' in s:\n",
    "                    webrtc_stats = s['webrtcStats']\n",
    "                    for ws in webrtc_stats:\n",
    "                        if ('event' in ws) and (ws['event'] == 'stats'):\n",
    "                            yield ws\n",
    "\n",
    "def add_remote_peer(data, remote_peer):\n",
    "    for d in data:\n",
    "        d['peerId'] = remote_peer\n",
    "\n",
    "\n",
    "def separate_stats(index_list_name):\n",
    "    def process_data(data, peer_id, direction):\n",
    "        if data:\n",
    "            add_remote_peer(data, peer_id)\n",
    "            if index_list_name not in stats_dict[direction]:\n",
    "                stats_dict[direction][index_list_name] = []\n",
    "            stats_dict[direction][index_list_name].extend(data)\n",
    "\n",
    "    for r in process_stats(index_list_name):\n",
    "        remotePeer = r['peerId']\n",
    "        if 'data' in r:\n",
    "            d = r['data']\n",
    "            local_audio = d['audio']\n",
    "            local_video = d['video']\n",
    "            remote_audio = d['remote']['audio']\n",
    "            remote_video = d['remote']['video']\n",
    "\n",
    "            process_data(local_audio['inbound'], remotePeer, 'inbound')\n",
    "            process_data(local_audio['outbound'], remotePeer, 'outbound')\n",
    "            process_data(local_video['inbound'], remotePeer, 'inbound')\n",
    "            process_data(local_video['outbound'], remotePeer, 'outbound')\n",
    "            process_data(remote_audio['inbound'], remotePeer, 'remote_inbound')\n",
    "            process_data(remote_audio['outbound'], remotePeer, 'remote_outbound')\n",
    "            process_data(remote_video['inbound'], remotePeer, 'remote_inbound')\n",
    "            process_data(remote_video['outbound'], remotePeer, 'remote_outbound')\n",
    "\n",
    "    for key in ['inbound', 'outbound', 'remote_inbound', 'remote_outbound']:\n",
    "        stats_dict[key][index_list_name] = pd.DataFrame(stats_dict[key][index_list_name])\n",
    "        stats_dict[key][index_list_name]['peerId'] = remotePeer\n",
    "\n",
    "# Using ThreadPoolExecutor for concurrent processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = {executor.submit(separate_stats, index_list_name): index_list_name for index_list_name in index_list_names}\n",
    "    for future in as_completed(futures):\n",
    "        future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all entries where audio.inbound is an empty list\n",
    "def not_empty_list(x):\n",
    "    return x != []\n",
    "normalized_inbound = normalized_qos[normalized_qos[\"audio.inbound\"].apply(not_empty_list)]\n",
    "\n",
    "# keep only audio.inbound, video.inbound, remote.audio.outbound, remote.video.outbound and peerID\n",
    "normalized_inbound = normalized_inbound[[\"audio.inbound\", \"video.inbound\", \"remote.audio.outbound\", \"remote.video.outbound\", \"peerId\"]]\n",
    "\n",
    "def expand_column(df, column_name):\n",
    "    expanded = df[column_name].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "    expanded = pd.json_normalize(expanded)\n",
    "    return expanded\n",
    "\n",
    "audio_inbound = expand_column(normalized_inbound, \"audio.inbound\")\n",
    "video_inbound = expand_column(normalized_inbound, \"video.inbound\")\n",
    "remote_audio_outbound = expand_column(normalized_inbound, \"remote.audio.outbound\")\n",
    "remote_video_outbound = expand_column(normalized_inbound, \"remote.video.outbound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_audio_inbound = audio_inbound.loc[:, [\"timestamp\", \"packetRate\", \"bitrate\"]]\n",
    "plt_audio_inbound[\"timestamp\"] = pd.to_datetime(plt_audio_inbound[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "cpu = df_cpu_list[0]\n",
    "cpu = cpu.drop(columns=[\"memory\"]).dropna()\n",
    "cpu[\"@timestamp\"] = pd.to_datetime(cpu[\"@timestamp\"], format=\"ISO8601\").dt.tz_localize(None)\n",
    "\n",
    "publishers_progression[\"timestamp\"] = publishers_progression[\"timestamp\"].dt.tz_localize(None)\n",
    "\n",
    "# here min_ts is calculated as the minimum datetime of the 3 dataframes\n",
    "# cpu are time aware while plt_audio_inbound is not\n",
    "min_ts = pd.concat([cpu[\"@timestamp\"], publishers_progression[\"timestamp\"], plt_audio_inbound[\"timestamp\"]]).min()\n",
    "\n",
    "cpu_td = pd.DataFrame(columns=[\"timestamp\", \"cpu\"])\n",
    "\n",
    "cpu_td[\"timestamp\"] = (cpu[\"@timestamp\"] - min_ts).dt.total_seconds()\n",
    "cpu_td[\"cpu\"] = cpu[\"cpu\"]\n",
    "\n",
    "user_td = pd.DataFrame(columns=[\"timestamp\", \"publishers\"])\n",
    "user_td[\"timestamp\"] = (publishers_progression[\"timestamp\"] - min_ts).dt.total_seconds()\n",
    "user_td[\"publishers\"] = publishers_progression[\"number\"]\n",
    "user_td[\"label\"] = publishers_progression[\"user\"] + \"-\" + publishers_progression[\"session\"]\n",
    "\n",
    "plt_audio_inbound[\"timestamp\"] = (plt_audio_inbound[\"timestamp\"] - min_ts).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cpu_td[\"timestamp\"], cpu_td[\"cpu\"], label=\"CPU usage\", color=\"black\", marker=\"o\", linestyle=\"--\", alpha=0.2)\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ylabel(\"CPU usage (%)\")\n",
    "ax.grid()\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "#ax.set_xlim(100, 110)\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"users\")\n",
    "ax2.spines[\"right\"].set_position((\"axes\", 1.025))\n",
    "ax2.plot(user_td[\"timestamp\"], user_td[\"publishers\"], label=\"users\", color=\"red\", marker=\"o\", linestyle=\"-\")\n",
    "ax2.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# Annotate users plot with offset to prevent overlap\n",
    "texts = [ax2.text(x, y, label) for x, y, label in zip(user_td[\"timestamp\"], user_td[\"publishers\"], user_td[\"label\"])]\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='black'))\n",
    "\n",
    "# Commented out third y-axis for bitrate\n",
    "# ax3 = ax.twinx()\n",
    "# ax3.set_ylabel(\"bitrate\")\n",
    "# ax3.spines[\"right\"].set_position((\"axes\", 1.075))\n",
    "# ax3.plot(plt_audio_inbound[\"timestamp\"], plt_audio_inbound[\"bitrate\"], label=\"bitrate\", linestyle=\"-\")\n",
    "\n",
    "fig.legend(loc='upper right')\n",
    "fig.suptitle(\"Users over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cpu_td[\"timestamp\"], cpu_td[\"cpu\"], label=\"CPU usage\", color=\"black\", marker=\"o\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ylabel(\"CPU usage (%)\")\n",
    "ax.grid()\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"users\")\n",
    "ax2.spines[\"right\"].set_position((\"axes\", 1.025))\n",
    "ax2.plot(user_td[\"timestamp\"], user_td[\"publishers\"], label=\"users\", color=\"red\", marker=\"o\", linestyle=\"-\")\n",
    "ax3 = ax.twinx()\n",
    "ax3.set_ylabel(\"bitrate\")\n",
    "ax3.spines[\"right\"].set_position((\"axes\", 1.075))\n",
    "ax3.plot(plt_audio_inbound[\"timestamp\"], plt_audio_inbound[\"packetRate\"], label=\"packet rate\", linestyle=\"-\")\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle(\"Inbound audio rate over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all entries where audio.inbound is an empty list\n",
    "def not_empty_list(x):\n",
    "    return x != []\n",
    "normalized_outbound = normalized_qos[normalized_qos[\"audio.outbound\"].apply(not_empty_list)]\n",
    "\n",
    "# keep only audio.inbound, video.inbound, remote.audio.outbound, remote.video.outbound and peerID\n",
    "normalized_outbound = normalized_outbound[[\"audio.outbound\", \"video.outbound\", \"remote.audio.inbound\", \"remote.video.inbound\", \"peerId\"]]\n",
    "\n",
    "def expand_column(df, column_name):\n",
    "    expanded = df[column_name].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "    expanded = pd.json_normalize(expanded)\n",
    "    return expanded\n",
    "\n",
    "audio_outbound = expand_column(normalized_outbound, \"audio.outbound\")\n",
    "video_outbound = expand_column(normalized_outbound, \"video.outbound\")\n",
    "remote_audio_inbound = expand_column(normalized_outbound, \"remote.audio.inbound\")\n",
    "remote_video_inbound = expand_column(normalized_outbound, \"remote.video.inbound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_remote_audio_inbound = remote_audio_inbound.loc[:, [\"timestamp\", \"roundTripTime\"]]\n",
    "plt_remote_audio_inbound[\"timestamp\"] = pd.to_datetime(plt_remote_audio_inbound[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "min_ts = pd.concat([cpu[\"@timestamp\"], user_join[\"@timestamp\"], plt_remote_audio_inbound[\"timestamp\"]]).min()\n",
    "\n",
    "cpu_td[\"timestamp\"] = (cpu[\"@timestamp\"] - min_ts).dt.total_seconds()\n",
    "\n",
    "user_td[\"timestamp\"] = (user_join[\"@timestamp\"] - min_ts).dt.total_seconds()\n",
    "\n",
    "plt_remote_audio_inbound[\"timestamp\"] = (plt_remote_audio_inbound[\"timestamp\"] - min_ts).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cpu_td[\"timestamp\"], cpu_td[\"cpu\"], label=\"CPU usage\", color=\"black\", marker=\"o\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_ylabel(\"CPU usage (%)\")\n",
    "ax.grid()\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"users\")\n",
    "ax2.spines[\"right\"].set_position((\"axes\", 1.025))\n",
    "ax2.plot(user_td[\"timestamp\"], user_td[\"publishers\"], label=\"users\", color=\"red\", marker=\"o\", linestyle=\"-\")\n",
    "ax3 = ax.twinx()\n",
    "ax3.set_ylabel(\"bitrate\")\n",
    "ax3.spines[\"right\"].set_position((\"axes\", 1.075))\n",
    "ax3.plot(plt_remote_audio_inbound[\"timestamp\"], plt_remote_audio_inbound[\"roundTripTime\"], label=\"roundTripTime\", linestyle=\"-\")\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle(\"Inbound audio RTT (s) over time\")\n",
    "\n",
    "ax3.axhline(y=0.1, color=\"green\", linestyle=\"-\", label=\"good rtt\")\n",
    "ax3.axhline(y=0.2, color=\"orange\", linestyle=\"-\", label=\"mediocre rtt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of directories in stats/\n",
    "dirs = os.listdir(\"stats/\")\n",
    "# read all events.json in each subdir\n",
    "events = []\n",
    "for i, dir in enumerate(dirs):\n",
    "    subdirs = os.listdir(f\"stats/{dir}\")\n",
    "    for subdir in subdirs:\n",
    "        with open(f\"stats/{dir}/{subdir}/events.json\") as f:\n",
    "            events.extend(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_start_fn = lambda x: (x['event'] == 'connectionStart')\n",
    "connection = list(filter(connection_start_fn, events))\n",
    "\n",
    "connection_df = pd.DataFrame(columns=[\"user\", \"session\", \"time to connect (s)\", \"connection start time\", \"stream created time\"])\n",
    "for i, conn in enumerate(connection):\n",
    "    participant = conn['participant']\n",
    "    session = conn['session']\n",
    "    connection_start = conn['timestamp']\n",
    "    stream_created = list(filter(lambda x: (x['event'] == 'streamCreated') and (x['connection'] == 'local') and (x['participant'] == participant) and (x['session'] == session), events))\n",
    "    if len(stream_created) > 1:\n",
    "        print(f\"More than one stream created for connection {participant}-{session}\")\n",
    "    if len(stream_created) == 0:\n",
    "        print(f\"No stream created for connection {participant}-{session}\")\n",
    "        continue\n",
    "    created_time = stream_created[0]['timestamp']\n",
    "    # convert to datetime\n",
    "    connection_start = datetime.strptime(connection_start, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    created_time = datetime.strptime(created_time, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    time_diff = (created_time - connection_start).total_seconds()\n",
    "    connection_df.loc[i] = [participant, session, time_diff, connection_start, created_time]\n",
    "\n",
    "#display(connection_df)\n",
    "\n",
    "#bar chart of time to connect\n",
    "plt.rcParams[\"figure.figsize\"] = [25, 10]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(connection_df[\"user\"] + \"-\"+ connection_df[\"session\"], connection_df[\"time to connect (s)\"])\n",
    "ax.set_xlabel(\"user-session\")\n",
    "ax.set_ylabel(\"time to connect (s)\")\n",
    "# rotate x labels\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.grid()\n",
    "fig.suptitle(\"Time to publish for each connection (s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
